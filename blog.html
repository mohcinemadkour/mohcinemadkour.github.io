<!DOCTYPE html>
<html lang="english">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="">
        <meta name="keywords" content="">
        <link rel="icon" href="/favicon.ico">

        <title>Mohcine Madkour Blog - Blog</title>

        <!-- Stylesheets -->
        <link href="/theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->



    </head>

    <body>

        <!-- Header -->

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="/"><img class="mr20" src="images/logo.svg" alt="logo">Mohcine Madkour Blog</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="pdfs/mohcine_madkour_cv.pdf">CV-Resume</a>
                                <a href="/">Homepage</a>
                                <a href="/categories.html">Categories</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="blogItem">
    <h1><a href="/Decison Tres and Random Forrest in R.html">Decison Tres and Random Forrest in R</a></h1>
    <h1>rpart</h1>
<p>This package includes several example sets of data that can be used for recursive partitioning and regression trees.  Categorical or continuous variables can be used depending on whether one wants classification trees or <em>regression trees</em>. This package as well at the <em>tree package</em> are probably the two go-to packages for trees.  However, care should be taken as the tree package and the rpart package can produce very different results.</p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine Madkour</a><br>
    Date: Thu 13 April 2017<br>
        Tags:
            <a href="/tag/decison-tres.html">
            Decison Tres</a>,             <a href="/tag/random-forrest.html">
            Random Forrest</a>,             <a href="/tag/r.html">
            R</a><br />
    <a href="/Decison Tres and Random Forrest in R.html#disqus_thread">Comments</a> -
    <a href="/Decison Tres and Random Forrest in R.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/Cross Validation.html">The caret Package</a></h1>
    <p>The caret package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:</p>
<ul>
<li>data splitting</li>
<li>pre-processing</li>
<li>feature selection</li>
<li>model tuning using resampling</li>
<li>variable importance estimation</li>
</ul>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine Madkour</a><br>
    Date: Sun 09 April 2017<br>
        Tags:
            <a href="/tag/data-valuation.html">
            data valuation</a><br />
    <a href="/Cross Validation.html#disqus_thread">Comments</a> -
    <a href="/Cross Validation.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/data valuation.html">Data Valuation</a></h1>
    <p>Despite the volume of money being invested in data and data technology, methods for answering this question are severely lacking. I am exploring the concept of data valuation, the characteristics of data that make it unique from other economic goods, and practical considerations for how enterprises can begin thinking about the general value of data.</p>
<p>This though is still in its infancy, but check back here for updates as we make progress!</p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine Madkour</a><br>
    Date: Sun 09 April 2017<br>
        Tags:
            <a href="/tag/data-valuation.html">
            data valuation</a><br />
    <a href="/data valuation.html#disqus_thread">Comments</a> -
    <a href="/data valuation.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/sparkim.html">Alternating Least Square example with SPARK</a></h1>
    <h1>The data</h1>
<p>This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.
FullFile.png</p>
<h1>ALS</h1>
<p><img alt="image" src="/images/ALS.png"></p>
<h1>Step 1 - Create an RDD from the CSV File</h1>
<h2>1.1 - Download the data</h2>
<div class="highlight"><pre><span></span>    #Download the data from github to the local directory
    !rm &#39;OnlineRetail.csv.gz&#39; -f
    !wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz
</pre></div>


<h2>1.2 - Put the csv into an RDD (at first, each row in the RDD is a string which correlates to a line in the csv) and show the first three lines.</h2>
<ul>
<li>Use the Spark context (sc) to get the list of possible methods. sc.<TAB></li>
<li>Use the textFile() method<div class="highlight"><pre><span></span>loadRetailData = sc.textFile(&quot;OnlineRetail.csv.gz&quot;)
loadRetailData.take(3)
</pre></div>


</li>
</ul>
<h1>Step 2 - Prepare and shape the data: "80% of a Data Scientists job"</h1>
<h2>2.1 - Remove the header from the RDD and split the remaining lines by comma.</h2>
<p>The header is the first line in the RDD -- use first() to obtain it.
Use the filter() method to filter out all lines which are not equal to the header line.
Map the split() method to the remaining lines to split on ","</p>
<div class="highlight"><pre><span></span>header = loadRetailData.first()
splitColumns = loadRetailData.filter(lambda line: line != header).map(lambda l: l.split(&quot;,&quot;))
</pre></div>


<h2>2.2 - Filter the remaining lines using <a href="https://docs.python.org/2.6/howto/regex.html">regular expressions</a></h2>
<p>The original file at UCI's Machine Learning Repository has commas in the product description. Those have been removed to expediate the lab. Only keep rows that have a quantity greater than 0, a non-empty customerID, and a non-blank stock code after removing non-numeric characters.</p>
<p>-Examine the header to determine which fields need to be used to filter the data.
- Use the filter() method for the first two requirements. Note -- you may have to cast values.
- Look at the <a href="https://docs.python.org/2.6/howto/regex.html">re.sub()</a> method</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="n">filteredRetailData</span> <span class="o">=</span> <span class="n">splitColumns</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;\D&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">l</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>


<h2>2.3 - Map each line to a SQL Row and create a Dataframe from the result. Register the Dataframe as an SQL temp table.</h2>
<p>Use the following for the Row column names: inv, stockCode, description, quant, invDate, price, custId, country. inv, stockCode, quant and custId should be integers.
price is a float. description and country are strings (the default).</p>
<p>Hint: When you replaced non-digit characters using the regular expression above, you replaced them in the context of a test. You'll have to do it again when creating the stockCode Row value. </p>
<p>-We haven't used SQLContext or Row in this notebook, so you will have to import them from the pyspark.sql package and then create a SQLContext
-You can create a Row using a map(). For example:
example = myRDD.map(lambda x: Row(v1=x[1], v2=int(x[2]), v3=float(x[3]))
Note how we set the column names this way
-use createDataFrame() in your SQLContext. Then register the dataframe with registerTempTable()
from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc)</p>
<div class="highlight"><pre><span></span>    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span><span class="p">,</span> <span class="n">Row</span>
     <span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span><span class="p">,</span> <span class="n">Row</span>
<span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
<span class="n">retailRows</span> <span class="o">=</span> <span class="n">filteredRetailData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">inv</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">stockCode</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;\D&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">description</span><span class="o">=</span><span class="n">l</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">quant</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span> <span class="n">invDate</span><span class="o">=</span><span class="n">l</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">price</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">5</span><span class="p">]),</span> <span class="n">custId</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">6</span><span class="p">]),</span> <span class="n">country</span><span class="o">=</span><span class="n">l</span><span class="p">[</span><span class="mi">7</span><span class="p">]))</span>
<span class="n">retailDf</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">retailRows</span><span class="p">)</span>
<span class="n">retailDf</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s2">&quot;retailPurchases&quot;</span><span class="p">)</span>
</pre></div>


<h2>2.4 - Keep only the data we need (custId, stockCode, and rank)</h2>
<p>The Alternating Least Squares algorithm requires three values. In this case, we're going to use the Customer ID (custId), stock code (stockCode) and a ranking value. In this situation there is not a ranking value within the data, so we will create one. We will set a value of 1 to indicate a purchase since these are all actual orders. Set that value to "purch".</p>
<p>After doing the select, group by custId and stockCode. 
- To add a fixed value within a select statement, use something like select x,y,1 as purch from z
- - Use the group by statement to group results. To group by two values, separate them by commas (i.e. group by x,y)</p>
<div class="highlight"><pre><span></span>query = &quot; SELECT custId, stockCode, 1 as purch FROM retailPurchases group by custId, stockCode&quot;
uniqueCombDf = sqlContext.sql(query)
</pre></div>


<h2>2.5 - Randomly split the data into a testing set (10% of the data), a cross validation set (10% of the data) a training set (80% of the data)</h2>
<div class="highlight"><pre><span></span>testDf, cvDf, trainDf = uniqueCombDf.randomSplit([.1,.1,.8])
</pre></div>


<h1>Step 3 - Build recommendation models</h1>
<h2>3.1 - Use the training dataframe to train a model with Alternating Least Squares using the ALS class</h2>
<p>ALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called ‘factor’ matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix.</p>
<p>Latent Factors / rank
    The number of columns in the user-feature and product-feature matricies
Iterations / maxIter
    The number of factorization runs</p>
<p>To use the ALS class type:
from pyspark.ml.recommendation import ALS</p>
<p>When running ALS, we need to create two separate instances. For both instances userCol is custId, itemCol is stockCode and ratingCol is purch.</p>
<p>For the first instance, use a rank of 15 and set iterations to 5.
For the second instance, use a rank of 2 and set iterations to 10.
Run fit() on both instances using the training dataframe.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml.recommendation</span> <span class="kn">import</span> <span class="n">ALS</span>
<span class="n">als1</span> <span class="o">=</span> <span class="n">ALS</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">maxIter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">userCol</span><span class="o">=</span><span class="s2">&quot;custId&quot;</span><span class="p">,</span> <span class="n">itemCol</span><span class="o">=</span><span class="s2">&quot;stockCode&quot;</span><span class="p">,</span> <span class="n">ratingCol</span><span class="o">=</span><span class="s2">&quot;purch&quot;</span><span class="p">)</span>
<span class="n">model1</span> <span class="o">=</span> <span class="n">als1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainDf</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.recommendation</span> <span class="kn">import</span> <span class="n">ALS</span>
<span class="n">als1</span> <span class="o">=</span> <span class="n">ALS</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">maxIter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">userCol</span><span class="o">=</span><span class="s2">&quot;custId&quot;</span><span class="p">,</span> <span class="n">itemCol</span><span class="o">=</span><span class="s2">&quot;stockCode&quot;</span><span class="p">,</span> <span class="n">ratingCol</span><span class="o">=</span><span class="s2">&quot;purch&quot;</span><span class="p">)</span>
<span class="n">model1</span> <span class="o">=</span> <span class="n">als1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainDf</span><span class="p">)</span>
<span class="n">als2</span> <span class="o">=</span> <span class="n">ALS</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">maxIter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">userCol</span><span class="o">=</span><span class="s2">&quot;custId&quot;</span><span class="p">,</span> <span class="n">itemCol</span><span class="o">=</span><span class="s2">&quot;stockCode&quot;</span><span class="p">,</span> <span class="n">ratingCol</span><span class="o">=</span><span class="s2">&quot;purch&quot;</span><span class="p">)</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">als2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainDf</span><span class="p">)</span>
</pre></div>


<h1>Step 4 - Test the models</h1>
<p>Use the models to predict what the user will rate a certain item. The closer our model is to 1 for an item a user has already purchased, the better.</p>
<h2>4.1 - Evaluate the model with the cross validation dataframe by using the transform function.</h2>
<p>Some of the users or purchases in the cross validation data may not have been in the training data. Let's remove the ones that aren't. To do this obtain all the the custId and stockCode values from the training data and filter out any lines with those values from the cross-validation data.</p>
<p>-At the end, print out how many cross-validation lines we had at the start -- and the new number afterwords.
-Use map() to return a specific value (i.e. foo = foo.map(lambda x: x.value)) and put them all in a set (i.e. foo1 = set(foo))
-You need all the returned values (remember they might be spread all across the cluster!) so run collect() on the results of the map(). (i.e. foo1 = set(foo.collect()))
- Use the filter() to filter out any values in the cross-validation dataframe which are in the stockCode or custId sets. Use toDF() to change the results to a dataframe.</p>
<div class="highlight"><pre><span></span>customers = set(trainDf.rdd.map(lambda line: line.custId).collect())
stock = set(trainDf.rdd.map(lambda line: line.stockCode).collect())
filteredCvDf = cvDf.rdd.filter(lambda line: line.stockCode in stock and line.custId in customers).toDF()
print cvDf.count()
print filteredCvDf.count()
</pre></div>


<h2>Step 4.2 - Make Predictions using transform()</h2>
<div class="highlight"><pre><span></span>predictions1 = model1.transform(filteredCvDf)
predictions2 = model2.transform(filteredCvDf)
</pre></div>


<h2>4.3 - Calculate and print the Mean Squared Error. For all ratings, subtract the prediction from the actual purchase (1), square the result, and take the mean of all of the squared differences.</h2>
<p>The lower the result number, the better the model.</p>
<div class="highlight"><pre><span></span>meanSquaredError1 = predictions1.map(lambda line: (line.purch - line.prediction)**2).mean()
meanSquaredError2 = predictions2.map(lambda line: (line.purch - line.prediction)**2).mean()
print &#39;Mean squared error = %.4f for our first model&#39; % meanSquaredError1
print &#39;Mean squared error = %.4f for our second model&#39; % meanSquaredError2
</pre></div>


<h2>4.4 - Confirm the model by testing it with the test data and the best hyperparameters found during cross-validation</h2>
<p>Filter the test dataframe (testDf) the same way as the cross-validation dataframe. Then run the transform() and calculate the mean squared error. It should be the same as the value calcuated above.</p>
<div class="highlight"><pre><span></span>filteredTestDf = testDf.rdd.filter(lambda line: line.stockCode in stock and line.custId in customers).toDF()
predictions3 = model2.transform(filteredTestDf)
meanSquaredError3 = predictions3.map(lambda line: (line.purch - line.prediction)**2).mean()
print &#39;Mean squared error = %.4f for our best model&#39; % meanSquaredError3
</pre></div>


<h1>Step 5 - Implement the model</h1>
<h2>5.1 - First, create a dataframe in which each row has the user id and an item id.</h2>
<p>Use the Dataframe methods to create a Dataframe with a specific user and that user's purchased products.
    First, use the Dataframe filter() to filter out all custId's but 15544.
    Then use the select() to only return the custId column.
    Now use distinct() to ensure we only have the single custId.
    Do a join() with the distinct values from the stockCode column. </p>
<div class="highlight"><pre><span></span>user = trainDf.filter(trainDf.custId == 15544)
userCustId = user.select(&quot;custId&quot;)
userCustIdDistinct = userCustId.distinct()
stockCode = trainDf.select(&quot;stockCode&quot;)
stockCodeDistinct = stockCode.distinct()
userItems = userCustIdDistinct.join(stockCodeDistinct)
</pre></div>


<h2>5.2 - Use 'transform' to rate each item.</h2>
<div class="highlight"><pre><span></span>bestRecsDf = model2.transform(userItems)
bestRecsDf.first()
</pre></div>


<p>In order to print the top five recommendations, we need to sort() them in descending orde</p>
<p><img alt="image" src="images/user.png"></p>
<p>This user seems to have purchased a lot of childrens gifts and some holiday items. The recommendation engine we created suggested some items along these lines</p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine Madkour</a><br>
    Date: Sun 09 April 2017<br>
        Tags:
            <a href="/tag/spark.html">
            Spark</a>,             <a href="/tag/machine-learning.html">
            machine learning</a><br />
    <a href="/sparkim.html#disqus_thread">Comments</a> -
    <a href="/sparkim.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/data science Curriculum.html">Data Science Curriculum</a></h1>
    <h1>Earning you way to a career in data science</h1>
<p>This page is intended to showcase a detailed curriculum that one could follow over the course of 18 weeks to acquire the necessary skills to become a very useful and practical data scientist. This curriculum is not meant as a replacement to machine learning, statistics, and computer science university Masters and Ph.D programs that take a more theoretical and abstract approach to their respective disciplines. You will not be magically transformed into one of the world's premier data scientists nor be able to do rigorous proofs of different machine learning concepts. </p>
<p>This curriculum is focused on transforming an individual to become the most practical, useful, and hireable to businesses that have use cases for advanced analytic intelligence. After course completion, you should be highly valuable to most any business that values data-driven decision making and have a good foundation for exploring in greater depth many other data science topics. </p>
<h1>Approach to Learning</h1>
<p>The main component of knowledge acquisition will be a time-tested source, the book. Reading textbooks can give you a very thorough, detailed and organized aggregation of the current knowledge you are trying to acquire. The many dozens of online data science courses that have come online in the last few years from coursera, edx, udacity, udemy, etc... have greatly expanded the ability of any individual to quickly acquire data science skills. Many of these courses do suggest books as companion sources but most don't enforce the reading of the material. Generally, a video lecture can only capture a small fraction of what is written in a textbook. </p>
<p>This is not to disparage online courses, as I have taken over a dozen of them myself and have learned a great deal but a good textbook can really widen the base of your knowledge quite a bit more if read in conjuction with most online video-lecture-based courses. There will be a big focus on reading textbooks and answering questions at the end of the chapter.</p>
<p>Of course, with data science being a very hands-on discipline, each topic in the curriculum will have programming assignments designed to implement theory and application taught in the textbooks.</p>
<h1>A Generalized Order of Learning</h1>
<p>Below is a list of verbs used to synthesize the above section in order of how the data science skills of the curriculum are suggested to be acquired. This is just my suggestion but this is how I would teach the material.  </p>
<ol>
<li><strong>Reading</strong> - Reading of new material is done first before any lecture. For most material, a good book exists that can cover the material.  </li>
<li><strong>Listening</strong> - Listening can refer to the more traditional lecture where only the most important/difficult concepts of the book are reahashed. This time could also be used as a question and answering session instead of any lecture. Online videos can be watched in replacement of a lecture. Blog posts and online forum discussions can also be good.  </li>
<li><strong>Doing</strong> - Doing refers to attempting problem sets and programming assignments on your own with limited help. Struggling without help is a great way to learn.  </li>
<li><strong>Collaborating</strong> - Working with others on problem sets or programming assignments is invaluable to the learning process. Seeing solutions from instructors or peers can be very helpful.  </li>
<li><strong>Remembering</strong> - Covering a large amount of topics in a short amount of time is a recipe for quickly forgetting. Techniques such as spaced repetition can help you remember and reinforce previous concepts.  </li>
</ol>
<h1>Week 0: Prerequisite Knowledge</h1>
<h3>Books</h3>
<ol>
<li><a href="http://greenteapress.com/wp/think-python-2e/">Think Python</a>  </li>
<li><a href="https://www.amazon.com/Statistics-4th-David-Freedman/dp/0393929728/">Statistics by David Freedman</a>  </li>
</ol>
<p>Before getting started on the main curriculum, some minimum assumptions are made: That you have some background in programming and some background in statistics and at the very least can do basic algebra. </p>
<h3>Python</h3>
<p>Good data science requires the knowledge of at least one programming language and it is better to know one programming language very well than many only marginally. This is similar to natural language where learning one language fluently is better than knowing several at the same level that a five year old does. After mastering one programming language it's usually fairly easy to translate code to another language as the concepts of programming do not change drastically from one to another.</p>
<p>The Python programming language is an excellent choice for learning data science. It is general purpose (can handle nearly any task), high level (for dummies), open source (free to see source code and usually free to use), has an excellent community (help is just a google search away) and has many friendly data science libraries already built (batteries included).</p>
<p>There are dozens of books and online courses available to begin learning Python. The short book <a href="http://greenteapress.com/wp/think-python-2e/">Think Python</a> (freely available) is a solid introduction to the language and will be supplemented by an abundance (50+) of short exercises and some smaller data inspecting/cleansing assignments using just the standard libraries (not the third party data science ones)</p>
<h3>Statistics</h3>
<p>The complement to computer skills for data scientists are math/statistics skills. To ease the student into statistics (for those that have forgotten or never taken a formal class), a nearly formula-less book called <a href="https://www.amazon.com/Statistics-4th-David-Freedman/dp/0393929728/">Statistics by David Freedman</a>. This book is good for getting an intuition about how statisticians think about problems and is read almost like a novel in that there is very little math to be done. The first five parts (18 chapters) cover the core proability and statistics material that forms the foundation of entry level stat books.</p>
<h1>Week 1: Software Development and Advanced Python</h1>
<h3>Books</h3>
<ol>
<li><a href="https://www.amazon.com/Fluent-Python-Ramalho/dp/1491946008">Fluent Python</a>  </li>
<li><a href="https://www.amazon.com/Head-First-Software-Development-Pilone/dp/0596527357">Head First Software Development</a></li>
</ol>
<p>After covering the basics of Python in the <a href="http://greenteapress.com/wp/think-python-2e/">Think Python</a> the student should be ready for a more advanced understanding of the language to help develop</p>
<ul>
<li>Day 1: Review of introductory python with focus on data structures </li>
<li>Day 2: Software development lifecycle with focus on debugging and testing  </li>
<li>Day 3: Overview of Classes and Objet Oriented Programming </li>
<li>Day 4: Python Data Model, special methods and the Standard Library    </li>
<li>Day 5: Multithreading and Multiprocessing   </li>
</ul>
<h1>Week 2: Data Wrangling</h1>
<p>After gaining a firm understanding of the core concepts of Python from Week 0 and 1, a deep dive into Python's data exploration libraries will be undertaken. The Pandas library is phenomenal for nearly all kinds of data wrangling tasks. In addition to Pandas a thorough look at Python's excellent visualization libraries - matplotlib and seaborn will be covered.</p>
<h3>Books</h3>
<p><a href="https://www.amazon.com/Learning-Pandas-Python-Discovery-Analysis/dp/1783985127">Learning Pandas</a>  </p>
<ul>
<li>Day 1: Introduction to Series and DataFrames  </li>
<li>Day 2: Split-Apply-Combine and Tidy Data  </li>
<li>Day 3: Matplotlib and Seaborn  </li>
<li>Day 4: Time Series and miscellaneous Pandas functionality  </li>
<li>Day 5: Data Science mock interview Assignment  </li>
</ul>
<h1>Week 3: Probability</h1>
<h3>Books</h3>
<p><a href="https://www.amazon.com/Introduction-Probability-Statistics-Random-Processes/dp/0990637204">Introduction to Probability</a> - Also free online at probabilitycourse.com.  </p>
<p>Although many data science jobs don't involve calculating probabilities by hand, the subjet underlies nearly all of data science tasks. A good understanding of probability will provide for much greater comprehension of many machine learning techniques.  </p>
<ul>
<li>Day 1: Basic Discrete and Continuous Probability  </li>
<li>Day 2: Conditional Probability, Bayes Theorem and counting methods  </li>
<li>Day 3: Random Variables - Expected Value and Variance  </li>
<li>Day 4: Discrete and Continuous Distributions  </li>
<li>Day 5: Joint Distributions  </li>
</ul>
<h1>Week 4: Statistics</h1>
<h3>Books</h3>
<ol>
<li>
<p><a href="https://www.amazon.com/Statistics-Plain-English-Third-Timothy/dp/041587291X">Statistics in Plain English</a>  </p>
</li>
<li>
<p>Day 1: Sampling Distributions and the Centeral Limit Theorem  </p>
</li>
<li>Day 2: Hypothesis Testing, confidence intervals, p-values, types of errors  </li>
<li>Day 3: Hypothesis Testing, confidence intervals, p-values, types of errors  </li>
<li>Day 4: Experimental Design and ANOVA  </li>
<li>Day 5: Case Study  </li>
</ol>
<h1>Week 5: Databases and SQL</h1>
<p>Statistics courses are generally taught with numbers that masquerade as data. Data in the wild is something completely different. The world's data is held in databases and up until recently most of this data was held in relational databases. Designing and understanding the basics of relational databases is extremely important as a data scientist. Communicating with data modelers/engineers will be very important. And accessing data through the (mostly) simple structure query language, SQL, is an absolute necessity to become a data scientist.</p>
<h3>Books</h3>
<p>Either MySQL or PostgreSQL book  </p>
<ul>
<li>Day 1: Introduction to Databases, relational databases, ER Modeling  </li>
<li>Day 2: Introduction to SQL (with MySQL or PostgreSQL), the different subcomponents of SQL and basic SELECT statements  </li>
<li>Day 3: Advanced SQL  </li>
<li>Day 4: Even more advanced SQL  </li>
<li>Day 5: Building a data warehouse in the cloud  </li>
</ul>
<h1>Week 6: Linear Models</h1>
<h3>Books</h3>
<p><a href="https://www.amazon.com/Introduction-Regression-Analysis-Douglas-Montgomery/dp/0470542810">Introduction to Linear Regression Analysis</a></p>
<ul>
<li>Day 1: Linear Regresion and correlation  </li>
<li>Day 2: Multiple Linear Regression, variable transformation and model building  </li>
<li>Day 3: Regression Diagnostics, Residual Analysis, Regularization  </li>
<li>Day 4: Classificaition with Logistic Regression  </li>
<li>Day 5: Generalized Linear Models  </li>
</ul>
<h1>Break Week</h1>
<h1>Week 7: Nonlinear Models</h1>
<h3>Books</h3>
<ol>
<li><a href="http://www-bcf.usc.edu/~gareth/ISL/">Introduction to Statistical Learning</a>  </li>
<li>
<p><a href="https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485">Applied Predictive Modeling</a>  </p>
</li>
<li>
<p>Day 1: Linear and nonlinear Discriminant Analysis  </p>
</li>
<li>Day 2: K-Nearest Neighbors and model validation  </li>
<li>Day 3: Support Vector Machines  </li>
<li>Day 4: Decision Trees  </li>
<li>Day 5: Random Forests and Gradient Boosted Trees  </li>
</ol>
<h1>Week 8: Dimensionality Reduction and Unsupervised Learning</h1>
<ul>
<li>Day 1: Curse of Dimensionality and PCA  </li>
<li>Day 2: K-means and hierarchical clustering  </li>
<li>Day 3: One-class SVM  </li>
<li>Day 4: Expectation Maximization   </li>
<li>Day 5: Graph-based learning  </li>
</ul>
<h1>Week 9: Specialty Topics</h1>
<ul>
<li>Day 1: Bag of Words model and Naive Bayes for Text Classification  </li>
<li>Day 2: Matrix Decomposition Methods for Topic Discovery  </li>
<li>Day 3: NLP Project  </li>
<li>Day 4: Recommendation Systems   </li>
<li>Day 5: Recommendation Systems  </li>
</ul>
<h1>Week 10: Hadoop Ecosystem</h1>
<ul>
<li>Day 1: Linux  </li>
<li>Day 2: Hadoop and Map Reduce  </li>
<li>Day 3: Cloud computing - AWS or Google Cloud  </li>
<li>Day 4: Spark  </li>
<li>Day 5: Hbase  </li>
</ul>
<h1>Week 11: Neural Networks</h1>
<ul>
<li>Day 1: Neural Networks  </li>
<li>Day 2: Convolutional and Recurrent Nets  </li>
<li>Day 3: Tensor Flow  </li>
<li>Day 4: Deep Learning </li>
<li>Day 5: Autoencoders and Restricted Boltzman Machines  </li>
</ul>
<h1>Week 12: Web Development</h1>
<ul>
<li>Day 1: Basic html/css    </li>
<li>Day 2: Javascript  </li>
<li>Day 3: JQuery  </li>
<li>Day 4: D3  </li>
<li>Day 5: App building  </li>
</ul>
<h1>Break Week</h1>
<h1>Weeks 13 - 15: Capstone Project</h1>
<h1>Week 16</h1>
<p>Review  </p>
<h1>Week 17</h1>
<p>Hundreds of Interview questions and beginning of job search  </p>
<h1>Week 18</h1>
<p>Interview Feedback  </p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine madkour</a><br>
    Date: Sun 09 April 2017<br>
        Tags:
            <a href="/tag/data-science.html">
            Data science</a><br />
    <a href="/data science Curriculum.html#disqus_thread">Comments</a> -
    <a href="/data science Curriculum.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/data science in 2 minutes.html">Data science in 2 minutes</a></h1>
    <h1>Data-Science-in-Two-Minutes</h1>
<p>This repository contains short, concise descriptions and explanations on numerous data science topics. The main purpose is to aggregate the huge multi-verse of data science into something digestible that can be read and understood in under two minutes. These summarizations are not meant to replace more thorough rigorous material but to act more as an index for someone wanting to examine the vast array of possibilities under the data science umbrella.</p>
<p>Data Science Topics:</p>
<h1>Probability</h1>
<h1>Statistics</h1>
<h3>P-value</h3>
<p>Value determined before a test that determines the probability that the null hypothesis will be rejected. If the test statistic produces a p-value in the rejection range and the null hypothesis is indeed correct then a type I error has been committed.</p>
<h3>MoM vs MLE vs MAP</h3>
<p>MoM - Method of moments is a simple ways to estimate population parameters by using the moments of the observations. You set up a system of equations and solve for the parameters</p>
<p>One Form of the Method
The basic idea behind this form of the method is to:
(1) Equate the first sample moment about the origin M1=1/n∑Xi=Xbar to the first theoretical moment E(X).
(2) Equate the second sample moment about the origin M2=1n∑i=1nX2i to the second theoretical moment E(X2).
(3) Continue equating sample moments about the origin, Mk, with the corresponding theoretical moments E(Xk), k = 3, 4, ... until you have as many equations as you have parameters.
(4) Solve for the parameters.</p>
<p>Links
https://onlinecourses.science.psu.edu/stat414/node/193</p>
<h3>Moment Generating Function</h3>
<p>Method to easily find moments of a probability distribution. M(t) = . It doesn’t always exist for all probability functions, though characteristic function always exists 
Taking the nth derivative of M evaluated at 0 yields the nth moment. 
The mgf uniquely characterizes a distribution so if two mgfs are equal then the pdfs are equivalent</p>
<h3>Unbalanced Classes SVM</h3>
<p>You can assign weights to each class to more heavily weigh the unbalanced class, but even without weighting SVM’s do well.</p>
<h3>Covariance Matrix</h3>
<p>Cov(X, Y) = Σ ( Xi - Xbar ) ( Yi - Ybar ) / N = Σ xiyi / N</p>
<p>To calculate this with a feature matrix.
Step 1: set x = X - Xbar
Step 2: Cov = x * x’/n</p>
<p>Diagonal elements will be variance</p>
<h1>Machine Learning</h1>
<p>When presented a set of inputs, a machine can learn some thing about those inputs for some purpose.</p>
<h2>Types of Machine Learning</h2>
<p>Machine learning can be divided into three broad learning types<br>
<em> Supervised - All inputs correspond with an output. The machine can be trained to predict future outputs.<br>
</em> Unsupervised - Only inputs are given. Machine can learn different structures within the input data.<br>
* Reinforcement - After a certain set of actions are performed some feedback on performance is returned which is used by the machine to learn.  </p>
<h3>Supervised Machine Learning Output</h3>
<ul>
<li>Regression - Continuous real valued response.</li>
<li>Classification - Each output is a particular class. <strong>Nominal</strong> classes have no particular natural ordering. <strong>Ordinal</strong> classes have a particular order (for example: Good, Average, Bad)</li>
</ul>
<h1>Supervised Machine Learning</h1>
<h3>Model</h3>
<p>A simplistic representation of the world. Does not capture everything.</p>
<h3>Flexible vs Inflexible Models</h3>
<ul>
<li>Flexible - Model has more knobs to tune and fit more wiggly (non-linear data). More prone to overfitting.</li>
<li>Inflexible - Less knobs to tune. More rigid and usually more assumptions and easier to interpret.</li>
</ul>
<h3>Parametric vs Non-Parametric Model</h3>
<ul>
<li>Parametric - the form of the model is known before hand. Finite number of parameters. Machine learns the coefficients/parameters of the model. More rigid but simpler to learn and interpret. Examples are linear, logistic regression and linear support vector machines.</li>
<li>Non-Parametric - Does not mean no parameters. The functional form of the model is not set before hand. Potentially infinite number of parameters. K-nearest neighbors, decision trees, RBF kernel Support Vector Machines  </li>
</ul>
<p><a href="http://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/">Blog Post</a><br>
<a href="https://www.quora.com/Do-Support-Vector-Machines-come-under-parametric-or-non-parametric-models-and-why">Quora Thread</a>  </p>
<h3>The many synonymous names for input and output variables</h3>
<ul>
<li>Input variables - predictor, covariates, feature, independent, explanatory, controlled, regressor, X</li>
<li>Output variable - response, dependent, target, Y</li>
</ul>
<h2>Linear Regression</h2>
<p>A very simple regression model that models the response as a linear combination of the predictor variables.</p>
<h3>Assumptions</h3>
<p>Linear regression makes many assumptions that make for a more rigid model though there are other techniques that can add flexibility to the model.
<em> Predictors are fixed constants
</em> Parameters are linear. Highly non-linear fits can still be made by transforming predictor variables. Only parameters need remain linear.
<em> Error variance is constant (Homoskedasticity). Plots of predicted value vs error are good to inspect whether this assumption is true. Non-constant variance can be a major problem with linear regression that can sometimes be alleviated by transforming the response variable.
</em> Errors are independent of one another. Knowing one error does not give information about another error as is the case with time series data.
<em> Errors are normally distributed with mean 0.
</em> No linear dependence (multicollinearity). No predictor variable can be a linear combination of all other predictors. Full rank matrix. 
* Linear specification is correct. The linear model accurately describes the true relationship between predictors and response.</p>
<p><a href="https://economictheoryblog.com/2015/04/01/ols_assumptions/">Linear Regression Assumption Blog post</a></p>
<h3>Fitting Linear Regression</h3>
<p>Different algorithms and different metrics can be used to find the parameters of a linear regression model. Most popular is method of least squares which minimizes the squared error between the regression line and each point. Minimum absolute error and other loss functions can be used. </p>
<p>Least squares equation below. 
<img alt="equation" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/917759911692e98ba477c3d669356525a84aace6"></p>
<p>The model coefficients can also be estimated using <a href="https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/">gradient descent</a>. Which slowly moves in the direction of the gradient (found by taking the derivative of the least squares equation with respect to each parameter) to find the optimal parameters. The method of maximum likelihood can also be used whenever there is a parameterized probability distribution for the error terms. Procedure works by finding the likelihood function (multiplying all the probability of each point) and using calculus to maximize this product. </p>
<h3>Multiple Regression</h3>
<p>Simple linear regression is with one predictor variable. Multiple Linear regression is with two or more.</p>
<h3>Variable Transformations</h3>
<p>Predictor variables can be transformed in any way imaginable as long as the input (design) matrix remains full rank. </p>
<h3>Linear Regression Problems and how to Fix them</h3>
<h4><a href="https://onlinecourses.science.psu.edu/stat501/node/337">Outliers vs High Leverage Observations vs Influence</a></h4>
<ul>
<li>Outliers are responses that are atypical and don't fit the natural trend of the data. </li>
<li>High leverage observations are unusual combinations of predictor variable values. Those observations that are furthest away from the mean of X. In simple linear regression, the observations at the end have the most leverage simply by being further away from the mean of the predictor variable.</li>
<li>Influential Observations have drastic effects on model predictions, parameter estimates and hypothesis tests</li>
</ul>
<h4>Detecting Outliers and Influential Observations</h4>
<ul>
<li>How to discover outliers - plots of predicted value vs (studentized) residual. But this won't work when outlier pulls regression line close to it. Look at deleted studentized residuals</li>
<li><a href="http://onlinestatbook.com/2/regression/influential.html">How to discover influential observations</a> - Cooks distance and DFFITS are good metrics. Both focus on how much a predicted value changes when one observation is left out.   </li>
<li>Fix - Examine outliers/influential observations for data quality issues. Delete them only when you have a good reason. Use <a href="https://onlinecourses.science.psu.edu/stat501/node/353">Robust estimation</a> to downweight outliers.</li>
</ul>
<h4><a href="http://people.duke.edu/~rnau/testing.htm">Diagnostics and fixes for regression</a></h4>
<p>When the assumptions of the linear model are violated then the value of the model can decrease
<em> Non-linear fit: A plot of residuals vs predicted values. If there is any kind of trend that can be modeled with the residuals, the model is not correctly specified. Need to use interaction terms, polynomial features, a different model or a transformation to the responses to make linear.
</em> Correlated Errors - Diagnosed by plotting residuals over time (or by row). Some time-series analysis can be done to detect any dependence on previous residuals or previous response. Fix by adding lags of dependent and independent variable
<em> Non-constant Variance - Diagnosed by looking at studentized residuals. If they grow/shrink then there is a problem. Fix by taking log transform of response. Can use box-cox method to find more precise transformation.
</em> Non-normal errors - Normal errors are not a necessary assumption. Diagnose with qq-plot. Points should fall close to diagonal line. Can fix with methods for outliers above.
* Multicollinearity - When predictor variables are highly correlated with one another. Can lead to very unstable coefficients and poorly interpretted and fit models. Use correlation matrix and pair plot of all combinations of predictor variables to see most highly correlated predictors. Also can use Variance Inflation Factor which determines how predictable a predictor variable is when that predictor variable is used as the response variable. Can fix by selecting to keep just one of each highly correlated pair. Also by centering and scaling predictor values. Better to use penalized regression like ridge, lasso or elastic net.</p>
<h3>Linear Regression Interpretation</h3>
<p>If all the assumptions in the model hold and there are no interaction or polynomial terms then the coefficient of each predictor variable tells us the amount of increase in the response when that predictor variable is increased by one unit holding all other predictor variables constant.  </p>
<h3>Regression Output</h3>
<ul>
<li>Coefficients and t-statistics - Each coefficient and its standard error is estimated from the data and is modeled by student-t distribution. A t-test is conducted to produce a p-value - a level of significance</li>
<li>Confidence interval - Confidence intervals can exist for the coefficients, the value of the regression line and for the prediction of a single obervation. They are not probabilities. Given a significance level, say 95%, the statistic you are measuring will capture the true value 95% of the time. </li>
<li>F - test: Determines if at least one of the predictors is necessary for the model. Does not say which ones are significant. F-ratio = Explained variance / Unexplained Variance. F-Ratio equals 1 when the model explains nothing.</li>
</ul>
<h3>Model Selection</h3>
<p>Building a model with all predictor variables typically isn't best practice (unless using penalized regression). Normally we look for parsimonious models - the least number of predictors with the highest predictive power. There are several ways to do this.</p>
<h4>Stepwise Selection</h4>
<ul>
<li>Forward Selection - Start with null model and add one variable at a time until some stopping criteria is met. Stop when AIC, BIC, Mallows CP, Adjusted R-squared stop improving</li>
<li>Backward Selection - Start will full model and remove one variable at a time until stopping criteria is met.</li>
<li>Forward and Backward selection - At each step, chose one variable to either add or remove from model</li>
<li>Best subset - make all possible models (not possible if number of predictors is large) and choose best</li>
</ul>
<h4>Selection Criteria</h4>
<p>AIC, BIC, Mallows CP and adjusted R-squared are 'historical' metrics for penalizing linear models without splitting data and doing cross validation. These metrics are used because residual squared error will always improve when more variables are added to the model. These selection criteria penalize for more predictors. Cross validation is typically used in place of these criteria when there is enough data.</p>
<h2>Penalized Regression</h2>
<p>Linear regression can be fit with numerous features and combinations/transformations of features with many of these features can be highly correlated to one another. As flexibility in the model increases, so does overfitting, building a model that fits the training set well but does not generalize well to unseen data. One of the best methods to combat overfitting is to penalize least squares by adding a term proportional to the size of the parameter. Ridge and Lasso regression are the most popular. Penalized regression does an excellent job at controlling overfitting. Can be used when number of parameters is greater than number of predictors. It is important to standardize predictors by subtracting mean and dividing by standard deviation since the size of the coefficient is directly related to the scale.</p>
<h3>Ridge Regression</h3>
<p>A penalty proportional to the L2 norm is added to the least squares equation. A closed-form solution exists. As the penalty increases the coefficients in the model tend towards 0.</p>
<h3>Lasso Regression</h3>
<p>The penalty is proportional to the L1 norm. Coefficients will become exactly 0 as the penalty increases and acts as a model selector unlike Ridge. Must be solved iteratively.</p>
<p><img alt="Lasso and Ridge" src="http://gerardnico.com/wiki/_media/data_mining/lasso_vs_ridge_regression.png?w=800&amp;tok=f55022"></p>
<h3>Elastic Net</h3>
<p>Has both L1 and L2 penalty</p>
<h3>Principal Components Regression</h3>
<p>Instead of fitting the dataset to all p predictors, use principal component analysis to find the first m &lt; p principal components that explain most of the variance and use these m transformed predictors in least squares. Choose the number of principal components by cross-validation. Standardize variables first.</p>
<h3>Partial Least Squares</h3>
<p>PLS is a supervised alternative to PCR. Standardize predictors and compute first component, Z as linear combination of each predictor, X, times its correlation coefficient to Y. Now use simple linear regression for all predictors onto Z and get residuals. Use these residuals to again find correlation coefficient to Y. Keep iterating until desired number of components is reached.</p>
<h3>Regression Splines</h3>
<p>Linear regression in one variable does not work well when trying to fit through highly non-linear data. Polynomial features can be used but this can lead to unstable swings in the regression line. Piecewise regression can be used using to help avoid using high degree polynomial terms. Regression splines with knots can also work very well and are surprisingly easy to fit. A cubic regression spline is built with X, X^2, X^3 and a third degree term for each knot. Apply least squares and the magic spline will pop out that is continuous at each knot and have the same first and second derivatives and look very smooth to the human eye. A slight variation is a natural cubic spline which must be linear before and after the first and last knots.</p>
<h3>Smoothing Splines</h3>
<p>Find a function that minimizes the squared loss but also is penalized proportional to its second derivative. If the second derivative is too high then the function will be very wiggly and overfit the data. This penalty ensures a smooth fit. It can be shown that a smoothing spline is a natural cubic spline with knots at every unique x value.</p>
<h3>Locally Weighted Regression</h3>
<p>At each unique x, a new low degree polynomial is fit. Each point is weighted by how far it is away from the current point being estimated.</p>
<h3>Generalized Additive Models</h3>
<p>A method that can use many different linear models, such as splines or weighted regression or polynomial features as additive building blocks to build one big linear regression. Fit using backfitting.</p>
<h3>Multivariate Adaptive Splines (MARS)</h3>
<p>MARS is a linear combination of multiple hinge functions. An example hinge (hockey stick) function is max(0, x - c) where c is a constant and a knot.  MARS is fit in an iterative, greedy fashion that adds two mirrored hinge functions to its model that have the greatest affect on lowering the squared error. After the addition of the pair of terms, pruning takes place where a term can be excluded from the model. </p>
<h3>Prediction vs Inference</h3>
<ul>
<li>Prediction - When given a set of inputs <strong>X</strong> and we are not necessarily concerned about interpreting the underlying target function <em>f</em> (could say its a black box) to predict <strong>y</strong>.</li>
<li>Inference - We care about the meaning of the predictors, their relationships, and how are they related (linear, non-linear) </li>
</ul>
<h2>Logistic Regression</h2>
<p>Simple model used for classification. Models the probability of a bernoulli distribution given input data. The output of logistic regression is the probability that an observation is in one of two classes. So even though technically logistic regression outputs a number between 0 and 1, it is used for classification.</p>
<h3>Logistic Model Specification</h3>
<p>Logistic Regression uses the same (linear combination of predictors times a coefficient) as linear regression except that it takes the result of this combination and smashes it with the sigmoid function so that it's value is always between 0 and 1.</p>
<h3>Sigmoid Function</h3>
<p><img alt="sigmoid" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a26a3fa3cbb41a3abfe4c7ff88d47f0181489d13"></p>
<h3>Model Interpretation</h3>
<p>Some algebra with the sigmoid function will show that for every one unit increase in a predictor variable will result in a corresponding increase parameter value increase to the log odds. The log odds can be any real number.</p>
<h3>Generalized Linear Models</h3>
<p>Not to be confused with General Linear Models (abbreviated GLM) which is the name for ordinary linear regression. Generalized Linear Models abbreviated GLIM but the trend is to use GLM specifically for Generalized Linear Models and have no abbreviation for General Linear Models (just call them linear models, ordinary linear regression, or simple linear regression).</p>
<p>GLMs offer more flexibility than ordinary linear regression by allowing a non-linear relationship to hold between the response and the predictors. The right hand side is still a linear combination of coefficients and covariates (<strong>XB</strong> in matrix notation) but the response variable <strong>Y</strong> is transformed by a <em>link</em> function <em>g</em> which transformed values are then assumed to have a linear relationship with the covariates.</p>
<p>The response variable does not have the constraint that it is continuous, normally distributed with constant variance. The classic case is a binomial (0/1) response which clearly doesn't follow linear regression assumptions. The outcome (0/1) is not directly modeled in this case, just the log-odds using the logit link function. Poisson and negative binomial regression can be used to model discrete counts. The distribution of <strong>Y</strong> is different than the link function. For instance, with binomial data, Y is distributed as a binomial distribution and uses the logit link. In ordinary linear regression, <strong>Y</strong> is normally distributed with the identity link function.</p>
<p>The response variable must still be independent and the covariates can be transformed as in linear regression.</p>
<p>No closed form solution. Use maximum likelihood with newton rapson or gradient descent.</p>
<h2>Linear Discriminant Analysis</h2>
<p>LDA is a machine learning method that can be used to classify two or more classes. LDA works by first assuming all predictor variables follow a normal distribution.  A multivariate normal distribution is estimated for each predictor for each different class. So, if there are three classes, 3 separate multivariate normal (with common covariance matrix and class specific mean) distributions will arise. A prior distribution of the classes is created based directly on the proportion of each class in the training data. Using bayes theorem, prediction can be made given a new observation.</p>
<h2>Quadratic Disriminant Analysis</h2>
<p>Same as LDA except that there will be a separate covariance matrix for each class. This give it more flexibility than LDA.</p>
<h3>SVM vs Logistic Regression</h3>
<p>If there is a separating hyperplane there is no guarantee logistic regression will be able to find the best one. It just guarantees the probability will be 0 or 1. This is more so for unregularized LR. SVMs might not do as well if there are random points close to the hyperplane</p>
<p>links
http://www.quora.com/Support-Vector-Machines/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression</p>
<h3>Recurrent Neural Net</h3>
<p>The units form a directed cycle and thus can keep an internal state where they have different gates that determine whether
Best use case: unsegmented Hand-written digits
LSTM - long-short term memory doesn’t have vanishing gradient problem
BPTT - trained through backpropagation through time</p>
<p>There are different gates - input gates, forget gates of previous input, output gates
Essentially the current input and the previous input are passed to different gates. Each has different weights. They are aggregated then squashed via an activation function and finally passed to an output layer where process begins again.</p>
<p>links
https://s3.amazonaws.com/piazza-resources/i48o74a0lqu0/i6ys94c8na8i2/RNN.pdf?AWSAccessKeyId=AKIAJKOQYKAYOBKKVTKQ&amp;Expires=1438359044&amp;Signature=bks5t9RHMGBKnu2X15JWE75Hcio%3D</p>
<h3>Convolutional Neural Nets</h3>
<p>Neurons are tiled in such a manner to represent overlapping visual fields.
There can be pooling layers which combine outputs from previous layers.
Can be fully connected layers.
Drop out layers reduce overfitting. Individual neurons drop out with some predefined probability</p>
<p>Max-Pooling: After each convolutional layer, there may be a pooling layer. The pooling layer takes small rectangular blocks from the convolutional layer and subsamples it to produce a single output from that block. There are several ways to do this pooling, such as taking the average or the maximum, or a learned linear combination of the neurons in the block. Our pooling layers will always be max-pooling layers; that is, they take the maximum of the block they are pooling.</p>
<h3>Generative vs Discriminative Models</h3>
<p>Generative models give a way to generate data given a particular model. They model the joint probability distribution p(x,y) and use this to calculate the posterior probability p(y|x). They model the distribution of classes p(x|y). 
Can generate sample points. For example - first pick y (say a topic) and then pick x (say a word in that topic)</p>
<p>p(y|x) = p(x, y) / p(x) = p(x|y)p(y)/p(x)</p>
<p>Generative models make assumption about distribution - as for example in Naive Bayes we assume independence of all features which can over-count evidence such as the word “Hong Kong”
Better for outlier detection and non-stationary models (dynamic data where test set is different that training)
Can be interpreted as probabilistic graphical model with a more rich interpretation of the model.
Gives you a way to generate data with p(y) and p(x|y)
Generative models assumptions don’t allow it to capture all the dependencies that are possible.
Generative models can do very well if structure is applied correctly.
Can work better with less data. 
Tend not to overfit because of restrictive assumptions</p>
<p>Discriminative classifiers model the posterior p(y|x) directly. They model the boundary between classes. Provide classification splits though not necessarily in a probabilistic manner, so you don’t actually need the underlying distribution</p>
<p>Can capture dependencies better because doesn’t have strict assumptions on distribution.
No attempt to model underlying probability distribution.</p>
<p>Links
http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf
http://stats.stackexchange.com/questions/12421/generative-vs-discriminative
http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf</p>
<h3>Parametric vs Non-Parametric Modeling</h3>
<p><strong>Parametric</strong> - The shape of the target function <em>f</em> is assumed. The most common is linear model f(X) = β0 + β1X1 + β2X2 + ... + βpXp. With model chosen parameters are estimated from historical data. If model assumptions are wrong then a poor fit could occur. Can choose more flexible models but those are prone to overfitting.</p>
<p><strong>Non-Parametric</strong> - No explicit assumptions about <em>f</em>. Can fit a wide variety of shapes. Since problem is not reduced to estimating parameters, much more data is needed for better fit. Hyperparameters are used instead to instruct fit.</p>
<p>https://www.quora.com/What-is-the-difference-between-the-parametric-model-and-the-non-parametric-model</p>
<h3>PCA</h3>
<p>Many uses and abuses of PCA</p>
<p>When there are a large number of covariates and potentially many of them are correlated with each other, PCA can greatly reduce the number of covariates and the multicollinearity between them</p>
<p>A principal component is the direction of the data that explains the most variance. One that captures the most spread in the data. Take a look at the images below. If were to only examine the points where the arrows touch the line, it would be clear to see that the points on the line in the left image vary greater than those on the right. The line on the right is the line that produces the maximum variance and thus would be the first principal component. </p>
<p>Each line (or hyperplane) created has a direction and variance associated with it. In PCA the direction is the eigenvector and the variance is the eigenvalue. The eigenvector with the highest eigenvalue is the principal component. This line (hyperplane) also minimizes the squared distance from the points to the line. This is not to be confused with linear regression which minimizes the squared error (given an x).</p>
<p>The number of eigenvectors (principal components) is equivalent to the number of dimensions of the data. Each successive eigenvector is orthogonal to the previous one. Using eigenvectors transforms your data from one space to another. These new directions are more intuitive and show more information. The image below shows this transformation</p>
<p>We can go a step further and reduce dimensionality by choosing to keep those eigenvectors with eigenvalues above a certain threshold.</p>
<p>We want lots of spread between covariates - maximum variance
To get PCA
Step 1: Get covariance matrix
Step 2: get eigenvalues of covariance matrix (do Singular value decomposition)
Step 3: normalize eigenvalues to 0 - 1. These eigenvalues represent the amount of variation retained for each variable
Step 4: USV from singular value decomposition U is new space (eigenvectors), S contains eigenvalues</p>
<p>The first principal component has the largest variance of the combination of covariates. It finds the direction of maximum variance and projects it on a smaller subspace. Eigenvectors point in this direction and corresponding eigenvalue gives variance in that direction
Second PC is largest variance of combination of covariates that are orthogonal to first PC</p>
<p>PCA: PCA sidesteps the problem of M not being diagonalizable by working directly with the n×n "covariance matrix" MTM.  Because MTM is symmetric it is guaranteed to be diagonalizable.  So PCA works by finding the eigenvectors of the covariance matrix and ranking them by their respective eigenvalues.  The eigenvectors with the greatest eigenvalues are the Principal Components of the data matrix.</p>
<p>Now, a little bit of matrix algebra can be done to show that the Principal Components of a PCA diagonalization of the covariance matrix MTM are the same left-singular vectors that are found through SVD (i.e. the columns of matrix V) - the same as the principal components found through PCA:</p>
<p>When PCA is not useful:
When doing predictive modeling, you are trying to explain the variation in the response, not the variation in the features. There is no reason to believe that cramming as much of the feature variation into a single new feature will capture a large amount of the predictive power of the features as a whole</p>
<p>When PCA may not be useful - for example when using random forests. The splits may happen in the last features that explain the least amount of variance among the features
The first principal component is a linear combination of all your features. The fact that it explains almost all the variability just means that most of the coefficients of the variables in the first principal component are significant.
Now the classification trees you generate are a bit of a different animal too. They do binary splits on continuous variables that best separate the categories you want to classify. That is not exactly the same as finding orthogonal linear combinations of continuous variables that give the direction of greatest variance. In fact we have recently discussed a paper on CV where PCA was used for cluster analysis and the author(s) found that there are situations where best separation is found not in the 1st few principal components but rather in the last ones.</p>
<p>links
https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/
http://www.stats.uwo.ca/faculty/braun/ss3850/notes/sas10.pdf
http://www.councilofdata.com/algorithms/principal-component-analysis-pca-part-1/</p>
<h3>Latent Dirichlet Allocation</h3>
<p>Step 1 - Choose number of topics by eyeballing, using prior info or max likelihood
Step 2 - randomly assign each word a topic
This gives each document and word to a distribution of topics
Say Doc1 is (30, 32, 38) for the three topics and 
Word1 in Doc1 is (60, 30, 10) for the three topics
Step 3 - Iterate through each word and randomly assign it to the topic given two components.</p>
<p>Component 1 - prevalence of topics in that specific document p(topic|document)
Component 2 - prevalence of word across topic p(word | topic)</p>
<p>Say word ‘ted’ represents 20% of the words in cat1 and 40% in cat2 across all documents and we want to reassign ‘ted’ in the first document. The first document is 85% cat1 and 15% cat2. So we can weigh these probabilities and come up with a 17:6 ratio of cat1:cat2 and randomly choose how to assign the word ‘ted’</p>
<p>This uses bayes theorem to generate the “prior” probabilities of each word (component 1).  What is the current composition of the document. This probability will add up to 1 and then updated by the likelihood - probability word is generated from that prior.
p(topic) * p(word | topic) for each topic and then randomly select from those numbers</p>
<p>To initiate generative process
Initially choose topics via dirichlet distribution. This assumes a prior for the topics
And then choose words from another dirichlet distribution</p>
<p>Dirichlet is a continuous multivariate distribution</p>
<p>Latent variable is one that we infer and not directly observed</p>
<p>links
http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/</p>
<p>Go through each document, and randomly assign each word in the document to one of the K topics.
Notice that this random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).
So to improve on them, for each document d…
Go through each word w in d…
And for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where we choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word’s topic with this probability). (Also, I’m glossing over a couple of things here, in particular the use of priors/pseudocounts in these probabilities.)
In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.
After repeating the previous step a large number of times, you’ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall).</p>
<h3>NMF</h3>
<p>Brute force matrix decomposition method factoring matrix C (DxW) into A (DxT) and matrix B (TxW). AxB approximately equals C. The non-negative part is useful in applications where non-negativity is a must. It can also make it easier to inspect. Smaller dimensions make it easier to store</p>
<p>In topic modeling
D - number of Documents
W - number of words
T - number of topics</p>
<p>So matrix A can be interpreted as the mixture of topics that for each document (row)
and matrix B can be interpreted as the mixture of words in that topic.</p>
<p>These can be converted to probabilities to form a generative model where documents are formed</p>
<p>Used to discover latent features
Algorithm uses ||C - AB||^2 and iterates to minimize this
Gibbs Sampling
A way to randomly sample from a complex multivariate joint probability distribution.</p>
<p>Step 1: pick out random feasible values of each variable
Step 2: condition on all the random variables except one. 
Step 3: Now you can use a simple uniform random variable to get a random value using the marginal distribution.
Step 4: repeat for other random variables in the joint. Once all variables have a value you have your ‘random’ point.</p>
<p>There is a burn-in required to get more feasible random values. Once you have sampled enough random values you can then choose from these sampled values to get more truly random values</p>
<h3>Expectation Maximization</h3>
<p>Form of soft clustering where each point can be part of multiple clusters with different probabilities. We usually assume a gaussian or multinomial distribution and want to find the optimal parameters (mean, covariance) of the distribution</p>
<p>Begin algorithm by picking number of clusters
Pick random (smart) gaussians (multinomial) distributions that are feasible
Go through each point and do a soft clustering - assign a probability that it arose from each gaussian P(cluster 1 | x1) and so on. Use bayes rule here and just assume equal priors - all classes are equally likely
Once it has these assignments (probabilities of being in each cluster) it readjusts the parameters of the gaussians to fit points better
Find new mean by doing a weighted average of the points. So points that are 99% in one class will have more weight than points that are 20% of that class
Calculate new variance in the same way. Find a weighted average of the squared difference between the point and the old mean.
Repeat until distributions aren’t moving
Exellent Video: https://www.youtube.com/watch?v=REypj2sy_5U
http://stats.stackexchange.com/questions/72774/numerical-example-to-understand-expectation-maximization</p>
<h1>Evaluation</h1>
<h3>ROC Curve</h3>
<p>A graphical plot that plots False positive rate vs True positive rate by varying the threshold of a binary classifier. </p>
<p>False positive rate(x) vs True positive rate(y)
False positive rate - out of all the samples that were actually negative, the percentage that was actually guessed positive - FP / Real Negatives</p>
<p>True Positive - sensitivity - recall - out of all the samples that were actually positive how many were actually guessed positive. TP / Real Positives</p>
<p>For example, we want to minimize false positives on spam so we set the threshold at .95. The false positive rate might be very low (.02) but the true positive rate might also be very low (.15) since the model needs to be very sure to guess spam. If we move the threshold to .5 - FPR will increase to say and TPR might be .8</p>
<p>Area on the curve is a good measure</p>
<p>Why the diagonal line with slope of 1? 
Let’s say there is 100 observations and 20 are actually positive.
And now I am given the chance to have 70% false positive rate (thats 56 positives out of 80 negatives wrong). Meaning by random if I am given 56 positives, just by random chance I should get at least 70% TPR. Let’s say I actually guess 5 / 20 right for TP and guess 56 positive wrong out of the 80 negative. I could simply reverse my decision and get 15 right for .75 TPR and only 24 wrong for .3 FPR. This is opposite (.25 TPR and .7 FPR)</p>
<h3>L1 vs L2</h3>
<p>Penalizing extreme parameter values
L1 - L1 norm, diamond shaped. Easier to think of this regularization as a condition sum(abs(parameters)) &lt; C</p>
<p>L1 better at sparse data. Incorrectly used on non-sparse data could yield large error.
L2 better at prediction since both highly correlated variables stay in the model.
L2 is like diversifying your portfolio. If one variable is corrupted can use other variable. L1 is more aggressive.</p>
<h1>Databases</h1>
<h1>Data Science Interview Questions</h1>
<p>What is wrong with memorizing data?</p>
<p>How would you describe a clock (in ML lingo) that is always two hours behind?</p>
<h2>Statistics</h2>
<p>When is an F-test used and what are its limitation?</p>
<p>Why does the sum of squares regression (SSR) have only one degree of freedom?</p>
<h2>Generic Questions</h2>
<p>What do you think a data scientist does?</p>
<p>What do you think are the most important skills for a data scientist to have?</p>
<p>What is a project you would want to work on at our company?</p>
<p>What unique skills do you think you could bring to the team?</p>
<p>What is an example of your experience working with real, dirty data?</p>
<p>What makes data dirty?</p>
<h3>What is the difference between reducible and irreducible error?</h3>
<h3>What is the difference between a machine learning model and a machine learning algorithm?</h3>
<p>Many times these words are used interchangeably but generally they have different meanings
A model (or method or technique) can be a general idea of how to interpret 
An algorithm instructs the user of the model of precisely how to execute computational steps
Examples: Linear regression is a model and least squares (or maximum likelihood) is the algorithm used to find the parameters in the model.
Neural Networks are a model with backpropagation</p>
<h1>Resources</h1>
<h3>Good Data Science Blogs</h3>
<p>blog.kaggle.com (Very useful for reading how winners of kaggle competitions win. Its usually 1. Feature Selection. 2. Gradient Boosted Trees or Deep Learning)<br>
http://www.kdnuggets.com/websites/blogs.html  </p>
<p>All of data science
http://www.datascienceontology.com/</p>
<p>Great quora thread on how to learn machine learning - https://www.quora.com/How-do-I-learn-machine-learning-1  </p>
<h3>Books</h3>
<h2>Free Books</h2>
<ul>
<li><strong>Intro to Statistical Learning</strong> Excellent introduction to most common ml algos. Only need basic stat/prob as a prereq. http://www-bcf.usc.edu/~gareth/ISL/</li>
<li><strong>Elements of Statistical Learning</strong> Same authors as above but much more rigorous - http://statweb.stanford.edu/~tibs/ElemStatLearn/</li>
<li><strong>Bayesian Reasoning and Machine Learning</strong> http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage</li>
<li><strong>Information Theory, Pattern Recognition and Neural Networks</strong> http://www.inference.phy.cam.ac.uk/itprnn/</li>
<li><strong>Mining of Massive Datasets</strong> http://www.mmds.org/</li>
<li><strong>Deep Learning</strong> - Still in production but nearly finished. http://www.deeplearningbook.org/</li>
</ul>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine madkour</a><br>
    Date: Sun 09 April 2017<br>
        Tags:
            <a href="/tag/data-science.html">
            Data science</a><br />
    <a href="/data science in 2 minutes.html#disqus_thread">Comments</a> -
    <a href="/data science in 2 minutes.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/DataTydying.html">Data Tydying in R</a></h1>
    <h1>Data Frame Row Slice</h1>
<div class="highlight"><pre><span></span>airports<span class="o">&lt;-</span>airports<span class="p">[</span><span class="m">-1</span><span class="p">,]</span>
</pre></div>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine madkour</a><br>
    Date: Sat 08 April 2017<br>
        Tags:
            <a href="/tag/data-preparation.html">
            Data Preparation</a><br />
    <a href="/DataTydying.html#disqus_thread">Comments</a> -
    <a href="/DataTydying.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/RLeaflet.html">Leaflet for R</a></h1>
    <h1>Introduction</h1>
<p>Leaflet is one of the most popular open-source JavaScript libraries for interactive maps. It’s used by websites ranging from The New York Times and The Washington Post to GitHub and Flickr, as well as GIS specialists like OpenStreetMap, Mapbox, and CartoDB.
This R package makes it easy to integrate and control Leaflet maps in R.</p>
<p>This R package makes it easy to integrate and control Leaflet maps in R.</p>
<h1>Installation</h1>
<p>To install this R package, run this command at your R prompt:</p>
<div class="highlight"><pre><span></span>install.packages(&quot;leaflet&quot;)
# to install the development version from Github, run
# devtools::install_github(&quot;rstudio/leaflet&quot;)
</pre></div>


<p>Basic Usage</p>
<p>You create a Leaflet map with these basic steps:</p>
<p>1- Create a map widget by calling leaflet().
2- Add layers (i.e., features) to the map by using layer functions (e.g.  addTiles, addMarkers, addPolygons) to modify the map widget.
3- Repeat step 2 as desired.
4- Print the map widget to display it.</p>
<p>Here’s a basic example:</p>
<div class="highlight"><pre><span></span><span class="kn">library</span><span class="p">(</span>leaflet<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>dplyr<span class="p">)</span>
m <span class="o">&lt;-</span> leaflet<span class="p">()</span> <span class="o">%&gt;%</span>
  addTiles<span class="p">()</span> <span class="o">%&gt;%</span>  <span class="c1"># Add default OpenStreetMap map tiles</span>
  addMarkers<span class="p">(</span>lng<span class="o">=</span><span class="m">174.768</span><span class="p">,</span> lat<span class="o">=</span><span class="m">-36.852</span><span class="p">,</span> popup<span class="o">=</span><span class="s">&quot;The birthplace of R&quot;</span><span class="p">)</span>
m  <span class="c1"># Print the map</span>
</pre></div>


<h1>Leaflet Heat Maps</h1>
<p>Create Map</p>
<p>We start by creating a map of the location.</p>
<div class="highlight"><pre><span></span><span class="kn">library</span><span class="p">(</span>rMaps<span class="p">)</span>
L2 <span class="o">&lt;-</span> Leaflet<span class="o">$</span>new<span class="p">()</span>
L2<span class="o">$</span>setView<span class="p">(</span><span class="kt">c</span><span class="p">(</span><span class="m">29.7632836</span><span class="p">,</span>  <span class="m">-95.3632715</span><span class="p">),</span> <span class="m">10</span><span class="p">)</span>
L2<span class="o">$</span>tileLayer<span class="p">(</span>provider <span class="o">=</span> <span class="s">&quot;MapQuestOpen.OSM&quot;</span><span class="p">)</span>
L2
</pre></div>


<h1>Get Data</h1>
<p>We will use the crime dataset from the ggmap package that contains a tidied up version of Houston crime data from January 2010 to August 2010.</p>
<div class="highlight"><pre><span></span>data(crime, package = &#39;ggmap&#39;)
library(plyr)
crime_dat = ddply(crime, .(lat, lon), summarise, count = length(address))
crime_dat = toJSONArray2(na.omit(crime_dat), json = F, names = F)
cat(rjson::toJSON(crime_dat[1:2]))

[[27.5071143,-99.5055471,1],[29.4836146,-95.0618715,10]

Add HeatMap
</pre></div>


<p>Now that we have the map and the data, the next step is to add the data to the map as a heatmap layer. Thanks to the Leaflet.heat plugin written by the Vladimir Agafonkin, the author of LeafletJS, this is really easy to do, with a little bit of custom javascript.</p>
<h1>Add leaflet-heat plugin. Thanks to Vladimir Agafonkin</h1>
<p>L2$addAssets(jshead = c(
  "http://leaflet.github.io/Leaflet.heat/dist/leaflet-heat.js"
))</p>
<h1>Add javascript to modify underlying chart</h1>
<p>L2$setTemplate(afterScript = sprintf("
<script>
  var addressPoints = %s
  var heat = L.heatLayer(addressPoints).addTo(map)         <br>
</script>
", rjson::toJSON(crime_dat)
))</p>
<p>L2</p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine madkour</a><br>
    Date: Sat 08 April 2017<br>
        Tags:
            <a href="/tag/data-visualization.html">
            Data Visualization</a>,             <a href="/tag/r.html">
            R</a><br />
    <a href="/RLeaflet.html#disqus_thread">Comments</a> -
    <a href="/RLeaflet.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/Vis311R.html">An interactive visual of Houston 311 calls</a></h1>
    <h1>An interactive visual of Houston 311 calls</h1>
<p><strong>Background</strong></p>
<p>Houston receives 311 calls for non-emergency services from it's residents, businesses and visitors. The response time for these calls is longer than those for emergency (911) calls. Accordingly the resources allocated to these services may not be as highly funded, resourced and/or prioritized as the emergency services. It is why the Houston authorities need to optimize the use and allocation of the resources available to service these calls.</p>
<p>Help Houston city to optimize the use and allocation of the resources available to service 311 calls.</p>
<p>Objective</p>
<p>The goal of this visual analysis is to aid the NYC authorities in optimizing the use of the limited resources available to service 311 calls/requests. There are 3 dimensions chosen along which this optimization can be done, time, location and skill type.</p>
<h1>barplot()</h1>
<div class="highlight"><pre><span></span>    pol = read.csv(&quot;http://www.calvin.edu/~stob/data/csbv.csv&quot;)
    barplot(table(pol$Political04), main=&quot;Political Leanings, Calvin Freshman 2004&quot;)
    barplot(table(pol$Political04), horiz=T)
    barplot(table(pol$Political04),col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;orange&quot;))
    barplot(table(pol$Political04),col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;orange&quot;),names=c(&quot;Conservative&quot;,&quot;Far Right&quot;,&quot;Liberal&quot;,&quot;Centrist))
</pre></div>


<p><img alt="image" src="/images/barplot.png"></p>
<div class="highlight"><pre><span></span>   barplot(xtabs(~sex + Political04, data=pol), legend=c(&quot;Female&quot;,&quot;Male&quot;), beside=T)
</pre></div>


<p><img alt="image" src="/images/barplotNB.png"></p>
<h1>boxplot()</h1>
<div class="highlight"><pre><span></span>data(iris)
boxplot(iris$Sepal.Length)
boxplot(iris$Sepal.Length, col=&quot;yellow&quot;)
boxplot(Sepal.Length ~ Species, data=iris)
boxplot(Sepal.Length ~ Species, data=iris, col=&quot;yellow&quot;, ylab=&quot;Sepal length&quot;,main=&quot;Iris Sepal Length by Species&quot;)
</pre></div>


<p><img alt="image" src="/images/boxplot.png"></p>
<h1>plot()</h1>
<div class="highlight"><pre><span></span>data(faithful)
plot(waiting~eruptions,data=faithful)
plot(waiting~eruptions,data=faithful,cex=.5)
plot(waiting~eruptions,data=faithful,pch=6)
plot(waiting~eruptions,data=faithful,pch=19)
plot(waiting~eruptions,data=faithful,cex=.5,pch=19,col=&quot;blue&quot;)
plot(waiting~eruptions, data=faithful, cex=.5, pch=19, col=&quot;blue&quot;, main=&quot;Old    Faithful Eruptions&quot;,
ylab=&quot;Wait time between eruptions&quot;, xlab=&quot;Duration of eruption&quot;)
</pre></div>


<p><img alt="image" src="/images/plot.PNG"></p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine madkour</a><br>
    Date: Sat 08 April 2017<br>
        Tags:
            <a href="/tag/data-visualization.html">
            Data Visualization</a>,             <a href="/tag/r.html">
            R</a>,             <a href="/tag/houston.html">
            Houston</a>,             <a href="/tag/311-calls.html">
            311 calls</a><br />
    <a href="/Vis311R.html#disqus_thread">Comments</a> -
    <a href="/Vis311R.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/VisR.html">Basic Viusalization in R</a></h1>
    <h1>barplot()</h1>
<div class="highlight"><pre><span></span>    pol = read.csv(&quot;http://www.calvin.edu/~stob/data/csbv.csv&quot;)
    barplot(table(pol$Political04), main=&quot;Political Leanings, Calvin Freshman 2004&quot;)
    barplot(table(pol$Political04), horiz=T)
    barplot(table(pol$Political04),col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;orange&quot;))
    barplot(table(pol$Political04),col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;orange&quot;),names=c(&quot;Conservative&quot;,&quot;Far Right&quot;,&quot;Liberal&quot;,&quot;Centrist))
</pre></div>


<p><img alt="image" src="/images/barplot.PNG"></p>
<div class="highlight"><pre><span></span>   barplot(xtabs(~sex + Political04, data=pol), legend=c(&quot;Female&quot;,&quot;Male&quot;), beside=T)
</pre></div>


<p><img alt="image" src="/images/barplotNB.PNG"></p>
<h1>boxplot()</h1>
<div class="highlight"><pre><span></span>data(iris)
boxplot(iris$Sepal.Length)
boxplot(iris$Sepal.Length, col=&quot;yellow&quot;)
boxplot(Sepal.Length ~ Species, data=iris)
boxplot(Sepal.Length ~ Species, data=iris, col=&quot;yellow&quot;, ylab=&quot;Sepal length&quot;,main=&quot;Iris Sepal Length by Species&quot;)
</pre></div>


<p><img alt="image" src="/images/boxplot.PNG"></p>
<h1>plot()</h1>
<div class="highlight"><pre><span></span>data(faithful)
plot(waiting~eruptions,data=faithful)
plot(waiting~eruptions,data=faithful,cex=.5)
plot(waiting~eruptions,data=faithful,pch=6)
plot(waiting~eruptions,data=faithful,pch=19)
plot(waiting~eruptions,data=faithful,cex=.5,pch=19,col=&quot;blue&quot;)
plot(waiting~eruptions, data=faithful, cex=.5, pch=19, col=&quot;blue&quot;, main=&quot;Old    Faithful Eruptions&quot;,
ylab=&quot;Wait time between eruptions&quot;, xlab=&quot;Duration of eruption&quot;)
</pre></div>


<p><img alt="image" src="/images/plot.PNG"></p>
<h1>Heatmap</h1>
<h2>Installing and loading required packages</h2>
<p>At first glance, this section might look a little bit more complicated then it need be, since executing library(packagename) is already sufficient to load required R packages if they are already installed.</p>
<div class="highlight"><pre><span></span>if (!require(&quot;gplots&quot;)) {
install.packages(&quot;gplots&quot;, dependencies = TRUE)
library(gplots)
}
if (!require(&quot;RColorBrewer&quot;)) {
install.packages(&quot;RColorBrewer&quot;, dependencies = TRUE)
library(RColorBrewer)
}
</pre></div>


<h2>Reading in data and transform it into matrix format</h2>
<p>We can feed in our data into R from many different data file formats, including ASCII formatted text files, Excel spreadsheets and so on. For this tutorial, we assume that our data is formatted as Comma-Separated Values (CSV); probably one of the most common data file formats.</p>
<p>When we open the CSV file in our favorite plain text editor instead of using a spread sheet program (Excel, Numbers, etc.), it looks like this:</p>
<div class="highlight"><pre><span></span>#heat map example data set,,,,
#12/08/13 sr,,,,
#
,var1,var2,var3,var4
measurement1,0.094,0.668,0.4153,0.4613
measurement2,0.1138,-0.3847,0.2671,0.1529
measurement3,0.1893,0.3303,0.5821,0.2632
measurement4,-0.0102,-0.4259,-0.5967,0.18
measurement5,0.1587,0.2948,0.153,-0.2208
measurement6,-0.4558,0.2244,0.6619,0.0457
measurement7,-0.6241,-0.3119,0.3642,0.2003
measurement8,-0.227,0.499,0.3067,0.3289
measurement9,0.7365,-0.0872,-0.069,-0.4252
measurement10,0.9761,0.4355,0.8663,0.8107
</pre></div>


<p>When we are reading the data from our CSV file into R and assign it to the variable data, note the two lines of comments preceding the main data in our CSV file, indicated by an octothorpe (#) character. Since we don’t need those lines to plot our heat map, we can ignore them by via the comment.char argument in the read.csv() function.</p>
<div class="highlight"><pre><span></span>data <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">&quot;../datasets/heatmaps_in_r.csv&quot;</span><span class="p">,</span> comment.char<span class="o">=</span><span class="s">&quot;#&quot;</span><span class="p">)</span>
</pre></div>


<p>One tricky part of the heatmap.2() function is that it requires the data in a numerical matrix format in order to plot it. By default, data that we read from files using R’s read.table() or read.csv() functions is stored in a data table format. The matrix format differs from the data table format by the fact that a matrix can only hold one type of data, e.g., numerical, strings, or logical. Fortunately, we don’t have to worry about the row that contains our column names (var1, var2, var3, var4) since the read.csv() function treats the first line of data as table header by default. But we would run into trouble if we want to include the row names (measurement1, measurment2, etc.) in our numerical matrix. For our own convenience, we store those row names in the first column as variable rnames, which we can use later to assign row names to our matrix after the conversion.</p>
<div class="highlight"><pre><span></span>rnames <span class="o">&lt;-</span> data<span class="p">[,</span><span class="m">1</span><span class="p">]</span>
</pre></div>


<p>Now, we transform the numerical data from the variable data (column 2 to 5) into a matrix and assign it to a new variable mat_data</p>
<div class="highlight"><pre><span></span>mat_data <span class="o">&lt;-</span> <span class="kp">data.matrix</span><span class="p">(</span>data<span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="kp">ncol</span><span class="p">(</span>data<span class="p">)])</span>
</pre></div>


<p>Instead of using the rather fiddly expression ncol(data)], which returns the total number of columns from the data table, we could also provide the integer 5 directly in order to specify the last column that we want to include. However, ncol(data)] is more convenient for larger data sets so that we don’t need to count all columns to get the index of the last column for specifying the upper boundary. Next, we assign the column names, which we have saved as rnames previously, to the matrix via</p>
<div class="highlight"><pre><span></span><span class="kp">rownames</span><span class="p">(</span>mat_data<span class="p">)</span> <span class="o">&lt;-</span> rnames
</pre></div>


<h2>Customizing and plotting the heat map</h2>
<p>Finally, we have our data in the “right” format in order to create our heat map, but before we get down to business, let us have a brief look at some options for customization.</p>
<h2>Optional: Choosing custom color palettes and color breaks</h2>
<p>Instead of using the default colors of the heatmap.2() function, I want to show you how to use the RColorBrewer package for creating our own color palettes. Here, we go with the most popular choice for heat maps: A color range from green over yellow to red.</p>
<div class="highlight"><pre><span></span>    my_palette <span class="o">&lt;-</span> colorRampPalette<span class="p">(</span><span class="kt">c</span><span class="p">(</span><span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="s">&quot;yellow&quot;</span><span class="p">,</span> <span class="s">&quot;green&quot;</span><span class="p">))(</span>n <span class="o">=</span> <span class="m">299</span><span class="p">)</span>
</pre></div>


<p>There are many different ways to specify colors in R. I find it most convenient to assign colors by their name. A nice overview of the different color names in R can be found at http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf</p>
<p>The argument (n = 299) lets us define how many individuals colors we want to have in our palette. Obviously, the higher the number of individual colors, the smoother the transition will be; the number 299 should be sufficiently large enough for a smooth transition. By default, RColorBrewer will divide the colors evenly so that every color in our palette will be an interval of individual colors of similar size. However, sometimes we want to have a little skewed color range depending on the data we are analyzing. Let’s assume that our example data set consists of Pearson correlation coefficients (i.e., R values) ranging from –1 to 1, and we are particularly interested in samples that have a (relatively) high correlation: R values in the range between 0.8 to 1.0. We want to highlight these samples in our heat map by only showing values from 0.8 to 1 in green. In this case, we can define our color breaks “unevenly” by using the following code:</p>
<div class="highlight"><pre><span></span>col_breaks = c(seq(-1,0,length=100), # for red
seq(0,0.8,length=100),  # for yellow
seq(0.81,1,length=100)) # for green
</pre></div>


<h2>Optional: Saving the heat map as PNG file</h2>
<p>R supports a variety of different vector graphics formats, such as SVG, PostScript, and PDFs, and raster graphics (bitmaps) like JPEG, PNG, TIFF, BMP, etc. Each format comes with its own advantages and disadvantages, and depending on the particular purposes (websites, journal articles, PowerPoint presentations, archiving … ) we chose one file format over the other. I don’t want to discuss all the details about when to use which particular file format in this tutorial but instead use a more common PNG format for our heat map. I picked PNG instead of JPEG, because PNG offers lossless compression (JPEG is a lossy image format) at the small cost of a slightly larger file size. However, you could completely omit the png() function in your script if you just want to show the heat map in an interactive screen in R.</p>
<div class="highlight"><pre><span></span>png(&quot;../images/heatmaps_in_r.png&quot;,    # create PNG for the heat map        
width = 5*300,        # 5 x 300 pixels
height = 5*300,
res = 300,            # 300 pixels per inch
pointsize = 8)        # smaller font size
</pre></div>


<p>The default parameters of the png() function would yield a relatively small PNG file at very low resolution, which is not really practical for heat maps. Thus we provide additional arguments for the image width, height and the resolution. The units of width and height are pixels, not inches. So if we want to create a 5x5 inch image with 300 pixels per inch, we have to do a little math here: [1500 pixels] / [300 pixels/inch] = 5 inches. Also, we choose a slightly smaller font size of 8 pt.</p>
<p>Be careful to not forget to close the png() plotting device at the end of you script via the function dev.off() otherwise you probably won’t be able to open the PNG file to view it.
Plotting the heat map</p>
<p>Now, let’s get down to business and take a look at the heatmap.2() function:</p>
<div class="highlight"><pre><span></span>    heatmap.2(mat_data,
      cellnote = mat_data,  # same data set for cell labels
      main = &quot;Correlation&quot;, # heat map title
      notecol=&quot;black&quot;,      # change font color of cell labels to black
      density.info=&quot;none&quot;,  # turns off density plot inside color legend
      trace=&quot;none&quot;,         # turns off trace lines inside the heat map
      margins =c(12,9),     # widens margins around plot
      col=my_palette,       # use on color palette defined earlier
      breaks=col_breaks,    # enable color transition at specified limits
      dendrogram=&quot;row&quot;,     # only draw a row dendrogram
      Colv=&quot;NA&quot;)            # turn off column clustering
</pre></div>


<p>Update Feb 19, 2014 - Clustering Methods</p>
<p>If you want to change the default clustering method (complete linkage method with Euclidean distance measure), this can be done as follows: For a square matrix, we can define the distance and cluster based on our matrix data by</p>
<div class="highlight"><pre><span></span>distance = dist(mat_data, method = &quot;manhattan&quot;)
cluster = hclust(distance, method = &quot;ward&quot;)
</pre></div>


<p>And eventually plug it into the heatmap.2() function</p>
<div class="highlight"><pre><span></span>heatmap.2(mat_data,
  ...
  Rowv = as.dendrogram(cluster), # apply default clustering method
  Colv = as.dendrogram(cluster)) # apply default clustering method
)
</pre></div>


<p>Update Mar 2, 2014 - Categorizing Measurements</p>
<p>I was just asked how to categorize the input variables by applying row or column labels. For example, if we want to group the “measurement” variables into 3 different categories: measurement 1-3 = category 1 measurement 4-6 = category 2 measurement 7-10 = category 3. My solution would be to simply provide RowSideColors as additional argument to the heatmap.2() function. E.g.,</p>
<div class="highlight"><pre><span></span>heatmap.2(mat_data,
  ...
  RowSideColors = c(    # grouping row-variables into different
     rep(&quot;gray&quot;, 3),   # categories, Measurement 1-3: green
     rep(&quot;blue&quot;, 3),    # Measurement 4-6: blue
     rep(&quot;black&quot;, 4)),    # Measurement 7-10: red
  ...
)
</pre></div>


<p>Note that we could also provide similar labels to the column variables via the ColSideColors argument. Another useful addition would be to add a color legend for our new category labels. The code for this particular example would be:</p>
<div class="highlight"><pre><span></span>par(lend = 1)           # square line ends for the color legend
legend(&quot;topright&quot;,      # location of the legend on the heatmap plot
    legend = c(&quot;category1&quot;, &quot;category2&quot;, &quot;category3&quot;), # category labels
    col = c(&quot;gray&quot;, &quot;blue&quot;, &quot;black&quot;),  # color key
    lty= 1,             # line style
    lwd = 10            # line width
)
</pre></div>


<p><img alt="The figure below shows how our modified heatmap would look like after we applied row categorization and provided a color legend:" src="/images/heatmaps_in_r_categorizing.png"></p>
<p><a href="https://github.com/mohcinemadkour/RPlots/tree/master/heatmaps">Check out the code source in githup</a></p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine madkour</a><br>
    Date: Sat 08 April 2017<br>
        Tags:
            <a href="/tag/data-visualization.html">
            Data Visualization</a>,             <a href="/tag/r.html">
            R</a><br />
    <a href="/VisR.html#disqus_thread">Comments</a> -
    <a href="/VisR.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/ggplot.html">"R Graphics with ggplot2"</a></h1>
    <h2>About ggplot2</h2>
<ul>
<li>
<p>Developed by Hadley Wickham in 2005.</p>
</li>
<li>
<p>Implements the graphics scheme described in the book <em>The Grammar of Graphics</em> by Leland Wilkinson.</p>
</li>
<li>
<p>Does not create interactive or 3D graphics.</p>
</li>
<li>
<p>~~As of February 2014, officially in maintenance mode, meaning no new features will be added.~~ ggplot2 2.0 released in December 2015 with over 100 fixes and improvements. </p>
</li>
</ul>
<h2>The Grammar of Graphics</h2>
<p>The <em>Grammar of Graphics</em> boiled down to 5 bullets, courtesy of Wickham (2016, p. 4):</p>
<ul>
<li>
<p>a statistical graphic is a mapping from data to <strong>aes</strong>thetic attributes (location, color, shape, size) of <strong>geom</strong>etric objects (points, lines, bars). </p>
</li>
<li>
<p>the geometric objects are drawn in a specific <strong>coord</strong>inate system.</p>
</li>
<li>
<p><strong>scale</strong>s control the mapping from data to aesthetics and provide tools to read the plot (ie, axes and legends).</p>
</li>
<li>
<p>the plot may also contain <strong>stat</strong>istical transformations of the data (means, medians, bins of data, trend lines).</p>
</li>
<li>
<p><strong>facet</strong>ing can be used to generate the same plot for different subsets of the data.</p>
</li>
</ul>
<h2>The Grammar of Graphics - illustration</h2>
<p><img alt="alt text" src="ggbasics.jpg"></p>
<p>www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf</p>
<h2>Basic ggplot2 syntax</h2>
<p><strong>Specify data, aesthetics and geometric shapes</strong> </p>
<p><code>ggplot(data, aes(x=, y=, color=, shape=, size=)) +</code> <br>
<code>geom_point()</code>, or <code>geom_histogram()</code>, or <code>geom_boxplot()</code>, etc.   </p>
<ul>
<li>
<p>This combination is very effective for exploratory graphs. </p>
</li>
<li>
<p>The data must be a data frame.</p>
</li>
<li>
<p>The <code>aes()</code> function maps columns of the data frame to aesthetic properties of geometric shapes to be plotted.</p>
</li>
<li>
<p><code>ggplot()</code> defines the plot; the <code>geoms</code> show the data; layers are added with <code>+</code> </p>
</li>
<li>
<p>Some examples should make this clear</p>
</li>
</ul>
<h2>The iris data set</h2>
<p>This is a famous data set from 1936, courtesy of Sir Ronald Fisher, that comes with R and is excellent for demonstrating ggplot2.</p>
<div class="highlight"><pre><span></span>str<span class="p">(</span>iris<span class="p">)</span>
</pre></div>


<h2>An iris</h2>
<p><img alt="alt text" src="iris_petal_sepal.jpg"></p>
<p>sebastianraschka.com/Images_old/2014_python_lda/</p>
<h2>ggplot2 example - scatter plot coded by species</h2>
<p>```{r, message=FALSE, fig.height=3, fig.width=6}
library(ggplot2) # once per session
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + geom_point() </p>
<div class="highlight"><pre><span></span>## ggplot2 example - scatter plot coded by species

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species, shape=Species)) + 
  geom_point() 
</pre></div>


<h2>ggplot2 example - scatter plot coded by species, size by Petal.Length</h2>
<p>```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color = Species, size = Petal.Length)) + 
  geom_point() </p>
<div class="highlight"><pre><span></span>## ggplot2 example - add multiple geoms (points and smooth line)

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + 
  geom_point() + geom_smooth(method=&quot;lm&quot;)
</pre></div>


<h2>ggplot2 example - boxplot (statistical transformation)</h2>
<p>```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Species, y = Sepal.Width)) + 
  geom_boxplot() </p>
<div class="highlight"><pre><span></span>## Moving beyond `ggplot` + `geoms`

* A natural next step in exploratory graphing is to create plots of subsets of data. These are called facets in ggplot2.

* Use `facet_wrap()` if you want to facet by one variable and have `ggplot2` control the layout. Example:   

     + `+ facet_wrap( ~ var)`

- Use `facet_grid()` if you want to facet by one and/or two variables and control layout yourself.     

Examples:    
+ `facet_grid(. ~ var1)` - facets in columns   
+ `facet_grid(var1 ~ .)` - facets in rows   
+ `facet_grid(var1 ~ var2)` - facets in rows and columns   



## ggplot2 example - `facet_wrap` (common scales)
```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width)) + 
  geom_point() + facet_wrap(~ Species)
</pre></div>


<h2>ggplot2 example - <code>facet_wrap</code> (free x scales)</h2>
<p>```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width)) + 
  geom_point() + facet_wrap(~ Species, scales = "free_x")</p>
<div class="highlight"><pre><span></span>## ggplot2 example - `facet_grid` (histograms)

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width)) +
  geom_histogram() + facet_grid(Species ~ .) 
</pre></div>


<h2>Customizing scales</h2>
<ul>
<li>
<p>Scales control the mapping from data to aesthetics and provide tools to read the plot (ie, axes and legends).</p>
</li>
<li>
<p>Every aesthetic has a default scale. To modify a scale, use a <code>scale</code> function. </p>
</li>
<li>
<p>All scale functions have a common naming scheme:
<code>scale</code> <code>_</code> name of aesthetic <code>_</code> name of scale</p>
</li>
<li>
<p>Examples: <code>scale_y_continuous</code>, <code>scale_color_discrete</code>, <code>scale_fill_manual</code></p>
</li>
<li>
<p>Heads up: The documentation for <code>ggplot2</code> scale functions will frequently use functions from the <code>scales</code> package (also by Wickham)!</p>
</li>
</ul>
<h2>ggplot2 example - update scale for y-axis</h2>
<p>```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + geom_point() +
  scale_y_continuous(limits=c(0,5), breaks=seq(0,5,0.5))</p>
<div class="highlight"><pre><span></span>## ggplot2 example - update scale for color

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + geom_point() +
  scale_color_manual(name=&quot;Iris Species&quot;, 
                     values=c(&quot;red&quot;,&quot;blue&quot;,&quot;black&quot;))
</pre></div>


<h2>stat functions</h2>
<ul>
<li>
<p>All <code>geoms</code> perform a default statistical transformation. </p>
</li>
<li>
<p>For example, <code>geom_histogram()</code> bins the data before plotting. <code>geom_smooth()</code> fits a line through the data according to a specified method.</p>
</li>
<li>
<p>In some cases the transformation is the "identity", which just means plot the raw data. For example, <code>geom_point()</code></p>
</li>
<li>
<p>These transformations are done by <code>stat</code> functions. The naming scheme is <code>stat_</code> followed by the name of the transformation. For example, <code>stat_bin</code>, <code>stat_smooth</code>, <code>stat_boxplot</code></p>
</li>
<li>
<p><strong>Every geom has a default stat, every stat has a default geom.</strong></p>
</li>
</ul>
<h2>ggplot2 example - geom using default stat</h2>
<p>```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) + 
  geom_point() + geom_smooth() </p>
<div class="highlight"><pre><span></span>## ggplot2 example - stat using default geom

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) + 
  geom_point() + stat_smooth() 
</pre></div>


<h2>Why should I care about <code>stat</code> versus <code>geom</code>?</h2>
<ul>
<li>
<p><strong>The stat and geom functions can use each other's arguments.</strong></p>
</li>
<li>
<p>When consulting the documentation for a particular <code>geom</code> you'll notice there is also documentation for an associated <code>stat</code>, and vice versa. (an exception is <code>geom_point</code> and <code>stat_identity</code>.)</p>
</li>
<li>
<p>Understanding how geoms and statistical transformations work together in ggplot2 can help you master the syntax faster.</p>
</li>
</ul>
<h2>Update themes and labels</h2>
<ul>
<li>
<p>The default ggplot2 theme is excellent. It follows the advice of several landmark papers regarding statistics and visual perception. (Wickham 2016, p. 176)</p>
</li>
<li>
<p>However you can change the theme using ggplot2's themeing system. To date, there are seven built-in themes: <code>theme_gray</code> (<em>default</em>), <code>theme_bw</code>, <code>theme_linedraw</code>, <code>theme_light</code>, <code>theme_dark</code>, <code>theme_minimal</code>, <code>theme_classic</code></p>
</li>
<li>
<p>You can also update axis labels and titles using the <code>labs</code> function.</p>
</li>
</ul>
<h2>ggplot2 example - update labels</h2>
<p>```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + geom_point() +
  labs(title="Sepal vs. Petal", 
       x="Petal Width (cm)", y="Sepal Width (cm)") </p>
<div class="highlight"><pre><span></span>## ggplot2 example - change theme

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 shape=Species)) + geom_point() +
  theme_bw()
</pre></div>


<h2>ggplot2 - some tips</h2>
<ul>
<li>Can do a lot with <code>ggplot(data, aes()) + geom</code>!</li>
<li>Data must be a data frame (not a matrix or collection of vectors)</li>
<li>The <code>ggplot2</code> documentation has many good examples</li>
<li>Prepare to invest some time if you want master ggplot2; <a href="https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf">the RStudio ggplot2 cheat sheet</a> can help.</li>
</ul>
<p>Let's go to R!</p>
<h2>References</h2>
<ul>
<li>
<p>Wickham, H. (2016), <em>ggplot2: Elegant Graphics for Data Analysis</em> (2nd ed), Springer.</p>
</li>
<li>
<p>Wickham, H. (2010), "A Layered Grammar of Graphics", <em>Journal of Computational and Graphical Statistics</em>, Volume 19, Number 1.</p>
</li>
<li>
<p>Chang, W. (2013), <em>R Graphics Cookbook</em>, O'Reilly.</p>
</li>
</ul>
<p><strong>ggplot2 cheat sheet</strong>  <br>
https://www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf</p>
<p><strong>Cookbook for R - Graphs</strong> <br>
http://www.cookbook-r.com/Graphs/</p>
<p><strong>Official ggplot2 web site</strong>  <br>
http://ggplot2.org/</p>
<h2>StatLab</h2>
<ul>
<li>
<p>Thanks for coming today!</p>
</li>
<li>
<p>For help and advice with your data analysis, contact StatLab to set up an appointment: statlab@virginia.edu</p>
</li>
<li>
<p>Sign up for more workshops or see past workshops:
http://data.library.virginia.edu/training/</p>
</li>
<li>
<p>Register for the Research Data Services newsletter to stay up-to-date on StatLab events and resources: http://data.library.virginia.edu/newsletters/</p>
</li>
</ul>

    <div class="blogMeta">
    Author: <a href="mailto: ">"Mohcine Madkour"</a><br>
    Date: Sat 08 April 2017<br>
        Tags:
            <a href="/tag/ggplot.html">
            ggplot</a><br />
    <a href="/ggplot.html#disqus_thread">Comments</a> -
    <a href="/ggplot.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/RDF Triple API.html">RDF Parser API</a></h1>
    <h1>RDF-Triple-API</h1>
<p>A simple API for extracting the RDF triple (subject, predicate, object) of any sentence. The parsed sentence is also returned in addition to the triple.</p>
<p>The algorithm implemented is taken from [this paper] (http://ailab.ijs.si/dunja/SiKDD2007/Papers/Rusu_Trippels.pdf) by Delia Rusu.</p>
<p>The sentence is parsed using the [stanford parser] (http://nlp.stanford.edu/software/lex-parser.shtml)</p>
<p>The endpoint for the api is http://www.newventify.com/rdf and has url parameter <code>sentence</code></p>
<p>A complete request would look like the following: <a href="http://www.newventify.com/rdf?sentence=The man stood next to the refrigerator">http://www.newventify.com/rdf?sentence=The man stood next to the refrigerator</a> and will return</p>
<p><code>{
  "object": {
    "POS": "NN", 
    "Tree Attributes": [], 
    "Word Attributes": [
      [
        "the", 
        "DT"
      ]
    ], 
    "word": "refrigerator"
  }, 
  "parse_tree": "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['The']), Tree('NN', ['man'])]), Tree('VP', [Tree('VBD', ['stood']), Tree('ADVP', [Tree('JJ', ['next'])]), Tree('PP', [Tree('TO', ['to']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['refrigerator'])])])])])])", 
  "predicate": {
    "POS": "VB", 
    "Tree Attributes": [
      "Tree('ADVP', [Tree('JJ', ['next'])])"
    ], 
    "Word Attributes": [], 
    "word": "stood"
  }, 
  "rdf": [
    "man", 
    "stood", 
    "refrigerator"
  ], 
  "sentence": "The man stood next to the refrigerator", 
  "subject": {
    "POS": "NN", 
    "Tree Attributes": [], 
    "Word Attributes": [
      [
        "The", 
        "DT"
      ]
    ], 
    "word": "man"
  }
}</code></p>
<p><a href="https://github.com/mohcinemadkour/RDF-Triple-API/blob/master/rdf_triple.py">Check out the source code here</a></p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine madkour</a><br>
    Date: Tue 09 February 2016<br>
        Tags:
            <a href="/tag/semantic-wev.html">
            Semantic Wev</a><br />
    <a href="/RDF Triple API.html#disqus_thread">Comments</a> -
    <a href="/RDF Triple API.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
    <div class="blogItem">
    <h1><a href="/Temporal clinical events clustering.html">Timeline of vaccine temporal events from VAERS reports</a></h1>
    <h1>Temporal Clustering of Vaccine events</h1>
<p><strong>Events in clinical narratives are naturally associated with medical trials, including surgery, vaccination, lab test, medication, medical procedure, and diagnosis. They are interrelated with many temporal relations. The grouping of medical events onto temporal clusters is a key to applications such as longitudinal studies, clinical question answering, and information retrieval. However, it is difficult to define clinical event quantitatively or consistently in coarse time-bins (e.g. before vaccination or after admission). In this article, I developed the K-means classifier to enable labeling a sequence of medical events with predefined time-bins. The features set is based solely on temporal distance similarity between boundaries of events. The result of the solution is integrated with the <a href="https://timeline.knightlab.com/">KnightLab timeline JS tool</a>.</strong></p>
<p>In this article, I investigate the task of tagging a sequence of events using a clustering algorithm. For this purpose I assume that each medical note can be associated with a predefined set of coarse of times that I refer to as time bins. For our example of VAERS note, the potential time-bins are: “before vaccination”, “soon after vaccination”, and “way after vaccination”. The time-bin “before vaccination” is intended to capture past medical history of the patient including the medical state of the patient on time of vaccination; “soon after vaccination” captures medical events that occurred immediately after the vaccination; and “way after vaccination” captures medical events that occurred after an extended duration from the vaccination. The issue in clustering events in predefined time-bins is that the time duration of each timebin varies based on the patient. For instance, the coarse of time “soon
after vaccination” could be the first few hours after or a few days
after depending on the general conditions. For that I consider that
related events happen in relatively close proximity of time. I use a
non-hierarchical clustering to classify the set of events. I consider
the temporal distance between events as the measure of similarity
between events of same clusters and dissimilarity between events of
different clusters.</p>
<h2>K-Means Clustering</h2>
<p>K-means is one of the simplest algorithms for solving the
clustering problem. Clustering is an unsupervised learning
problem whereby I aim to group subsets of entities with one
another based on a temporal distance similarity. The idea is to define
k centroids for the k assumed clusters and to associate each point
belonging to a given data set to the nearest center. A point represents
the time instant of the event or the center of interval if its time
interval event. When no point is pending, the first step is completed
and an early group age is done. At this point I re-calculate k new
centroids as barycenter of the clusters resulting from the previous
step. After I have these k new centroids, I re-bind the same data
set points to their nearest new center. A loop has been generated. As
a result of this loop the k centers change their location step by step
until no more changes are done or in other words centres do not
move any more.</p>
<h2>Timeline View</h2>
<p>For data that relates to temporal events, the Timeline Widget adds an interesting dimension to your exhibit.</p>
<p>The nobelists.js data file lists the years when the Nobelists won their prizes, so I can plot each one on a time line. To display timelines in Exhibit you need to include a separate utility, the Timeline widget. The Timeline widget is a bit bulky, so Exhibit doesn't include it by default. You have to include the time extension to Exhibit. Open the file nobelists.html, find the reference to exhibit-api.js and add the following script element after it:</p>
<div class="highlight"><pre><span></span><span class="nt">&lt;div</span> <span class="na">data-ex-role=</span><span class="s">&quot;view&quot;</span><span class="nt">&gt;</span>
    data-ex-view-class=&quot;Timeline&quot;  
    data-ex-start=&quot;.time&quot; 
    data-ex-end=&quot;.time2&quot; 
    data-ex-color-key=&quot;.cluster&quot; 
    data-ex-top-band-unit=&quot;month&quot; 
    data-ex-bottom-band-unit=&quot;year&quot; 
    data-ex-top-band-pixels-per-unit=&quot;90&quot; 
    data-ex-bottom-band-pixels-per-unit=&quot;400&quot;
        <span class="nt">&lt;div</span> <span class="na">data-ex-role=</span><span class="s">&quot;lens&quot;</span><span class="nt">&gt;</span>
                <span class="nt">&lt;span</span> <span class="na">data-ex-content=</span><span class="s">&quot;.hour&quot;</span><span class="nt">&gt;&lt;/span&gt;</span>: 
                <span class="nt">&lt;span</span> <span class="na">data-ex-content=</span><span class="s">&quot;.label&quot;</span><span class="nt">&gt;&lt;/span&gt;</span>
        <span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</pre></div>


<h2>Visualization</h2>
<p>I visualize the results using the Exhibit dashboard
solution. The timeline dashboard enables intuitive cluster analysis
by user interactions. Also our visualization allows summarizing by
the various types of events information. <img alt="Here is a screeshot of the visualization" src="/images/timeline.png"> Check out the <a href="http://htmlpreview.github.io/?https://github.com/mohcinemadkour/Event-Timeline/blob/master/index.html">visualization of clustered events</a></p>

    <div class="blogMeta">
    Author: <a href="mailto: ">Mohcine Madkour, Jingcheng Du, Hsing-Yi Song, Cui Tao</a><br>
    Date: Wed 18 February 2015<br>
        Tags:
            <a href="/tag/k-means.html">
            K-means</a>,             <a href="/tag/data-visualization.html">
            data visualization</a>,             <a href="/tag/vaers-reports.html">
            VAERS Reports</a><br />
    <a href="/Temporal clinical events clustering.html#disqus_thread">Comments</a> -
    <a href="/Temporal clinical events clustering.html">Permalink</a>
    </div>
    </div><!-- end #blogItem -->
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="/archives.html">Archives</a></li>
                            <li><a href="/tags.html">Tags</a></li>
                            <li><a href="/authors.html">Authors</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Social</div>
                        <ul class="list-unstyled">
                            <li><a href="#" target="_blank">You can add links in your config file</a></li>
                            <li><a href="#" target="_blank">Another social link</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Links</div>
                        <ul class="list-unstyled">
                            <li><a href="https://www.linkedin.com/in/mohcine-madkour-83a642b2/" target="_blank">Linkedin</a></li>
                            <li><a href="https://www.researchgate.net/profile/Mohcine_Madkour2" target="_blank">Researchgate</a></li>
                            <li><a href="https://scholar.google.com/citations?user=i8QA11IAAAAJ&hl=en" target="_blank">Google Scholar</a></li>
                        </ul>
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; blogname 2015</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    <script type="text/javascript">
        var disqus_shortname = 'http-mohcinemadkour-github-io';
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] ||
             document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    </body>
</html>