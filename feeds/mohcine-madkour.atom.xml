<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Mohcine Madkour - Mohcine Madkour</title><link href="https://mohcinemadkour.github.io/" rel="alternate"></link><link href="https://mohcinemadkour.github.io/feeds/mohcine-madkour.atom.xml" rel="self"></link><id>https://mohcinemadkour.github.io/</id><updated>2018-06-14T13:01:00-04:00</updated><entry><title>Deep Reinforcement Learning</title><link href="https://mohcinemadkour.github.io/posts/2018/06/Deep%20Reinforcement%20Learning/" rel="alternate"></link><published>2018-06-14T13:01:00-04:00</published><updated>2018-06-14T13:01:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-06-14:/posts/2018/06/Deep Reinforcement Learning/</id><summary type="html">&lt;h1&gt;Introduction to reinforcement learning&lt;/h1&gt;
&lt;h2&gt;The learning paradigm&lt;/h2&gt;
&lt;p&gt;Today, we will explore Reinforcement Learning – a goal-oriented learning based on interaction with environment. Reinforcement Learning is said to be the hope of true artificial intelligence. And it is rightly said so, because the potential that Reinforcement Learning possesses is immense.
The RL …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction to reinforcement learning&lt;/h1&gt;
&lt;h2&gt;The learning paradigm&lt;/h2&gt;
&lt;p&gt;Today, we will explore Reinforcement Learning – a goal-oriented learning based on interaction with environment. Reinforcement Learning is said to be the hope of true artificial intelligence. And it is rightly said so, because the potential that Reinforcement Learning possesses is immense.
The RL is kind of learning by doing, with no supervisor, but only a reward signal. The Feedback is  delayed and not instantaneous. In this kind of learning the time really matters and the agent’s actions affect the subsequent data it receives
A reward Rt is a scalar feedback signal, It indicates how well agent is doing at step t. The agent’s job is to maximise cumulative reward. Reinforcement learning is based on the reward hypothesis, which states that all goals can be described by the &lt;strong&gt;Maximisation of expected cumulative reward&lt;/strong&gt;. 
Examples of Rewards can be +ve reward for following desired trajectory and −ve reward for crashing.
The goal is to select actions to maximise total future reward. The actions may have long term consequences, and the reward may be delayed. Sometimes It may be better to sacrifice immediate reward to gain more long-term reward
Examples are numerous. For example a financial investment (may take months to mature), and Refuelling a helicopter (might prevent a crash in several hours)
Two fundamental problems in sequential decision making 
&lt;img alt="supervised vs rl" src="/images/dsrl.png"&gt;&lt;/p&gt;
&lt;h2&gt;Sequential Decision Making&lt;/h2&gt;
&lt;p&gt;The reinforcement learning id a Sequential Decision Making process. In general there is two types of environmnets: Fully Observable Environments which is recommended for Markov decision process in where the Agent state about the environmnet is identical with the environment state and with the information state; and the Partially Observable Environments in which the Partially Markov decision process can be applied. In this environment, the agent indirectly observes environment. The Agent must construct its own state representation whcih includes complete history, beliefs of environment state. The Recurrent neural network can be used in this case.&lt;/p&gt;
&lt;h2&gt;Components of an RL Agent&lt;/h2&gt;
&lt;p&gt;An RL agent may include one or more of these components: Policy: agent’s behaviour function, Value function: how good is each state and/or action, and Model: agent’s representation of the environment. The &lt;strong&gt;Policy&lt;/strong&gt; is the agent’s behaviour. It is a map from state to action, e.g. We have two types : Deterministic policy: a = π(s) and Stochastic policy: π(a|s) = P[At = a|St = s]. The &lt;strong&gt;Value Function&lt;/strong&gt; is a prediction of future reward. It is used to evaluate the goodness/badness of states And therefore to select between actions, e.g.]:&lt;strong&gt;vπ(s)&lt;/strong&gt; = Eπ [Rt+1 + γRt+2 + γ2 Rt+3 + ... | St = s]. &lt;strong&gt;The model&lt;/strong&gt; predicts what the environment will do next. The P predicts the next state, and the R predicts the next (immediate) reward.
Pss'= P[St+1 = s | St = s, At = a], Ras = E [Rt+1 |St = s, At = a]&lt;/p&gt;
&lt;h1&gt;Types of reinforcement learning algorithms&lt;/h1&gt;
&lt;p&gt;RL algorithms that satisfy the &lt;em&gt;Markov property&lt;/em&gt; are called the &lt;em&gt;Markov Decision Processes (MDP)&lt;/em&gt;. The Markov property assumes that the current state is independent of the path that leads to that particular state. 
&lt;img alt="Markovian environments and Non Markovian environments" src="/images/markov.png"&gt;
Hence, in Markovian problems a memoryless property of a stochastic process is assumed. In practice it means that the probability distribution of the future states depends only on the current state and not on the sequence of events that preceded. This is a useful property for stochastic processes as it allows for analysing the future by setting the present
&lt;img alt="State Transition from state s to state s'" src="/images/fig1_rl.png"&gt;
An MDPs consist of state (s), action (a) sets and given any state and action to be taken, a transition probability function of each possible next state (s’) illustrated in figure 1. In addition, each taken action to arrive to the next state is rewarded giving each of all possible actions a reward value
depending on the type of action. Each visited state is accredited by a value given to it according to a &lt;strong&gt;value function V(s)&lt;/strong&gt; which represents how good it is for an agent to be in a given state. The value of a state s under a policy π is then denoted as Vπ(s) which in theory denotes the expected return when starting in state s and following a sequence of states to be visited according to the order defined in π thereafter. When this theorem is applied to a model-free control problem, the &lt;strong&gt;state-value function&lt;/strong&gt; may
not suffice as it does not show what action was taken for the state value to be acquired. Therefore, a similar function has been introduced representing an estimation of the value of each possible action in a state. This is described as the &lt;strong&gt;action-value function&lt;/strong&gt; for policy π Qπ(s,a). Figure 2 illustrates an example of the relationship between the action-value function and the state-value function. In 2.a. the action-values are shown for each direction of the propagation, North, East, South, and West respectively. &lt;strong&gt;The state value function represents then the highest action-value possible in that state which is the action North in the example&lt;/strong&gt;.&lt;img alt="1a 1b" src="/images/rl_fig2.png"&gt;&lt;/p&gt;
&lt;p&gt;The optimal policy is denoted as the superscript asterisk to the action-value-function Q(s,a) and state value-function V(s). Formally, the optimal value function is then given by:
&lt;img alt="Eq1" src="/images/eq1.png"&gt;
Where Q*(s,a) is given by:
&lt;img alt="Eq2" src="/images/eq2.png"&gt;
Herein, T(s, a, s’) is the transition probability to the next state s’ given state s and action a. γ presents the discount factor which is usually smaller than 1 and is used to discount for earlier values in order to assign
higher values for sooner rewards. This is necessary to converge the algorithm.
Substituting equation 3 in 2 gives the Bellman equation:
&lt;img alt="Eq3" src="/images/eq3.png"&gt;
These updates will be appended to the states that were visited resulting (after a significant number of iterations) in state values showing how good to be in that state. In order to be able to choose between the states to select a policy, &lt;strong&gt;as many states as possible need to be visited&lt;/strong&gt; in order to converge to an accurate estimation of the state value. Acquiring the highest reward depends on these visited states and the reward accumulated. However, in order to discover more states and potentially higher rewards, the agent needs to take actions it has never taken before. This is referred to as the &lt;strong&gt;trade-off between exploitation and exploration&lt;/strong&gt;. This trade-off could be achieved by setting a variable denoted as Epsilon (ε) which gives the extent of exploration versus exploitation. A fully exploiting policy is referred to as an
epsilon-greedy policy and holds a value of 0 for ε. Correspondingly, a fully exploring policy gives a value of 1 to ε and is referred to as an epsilon-soft policy. The learning can therefore be tuned between these two extremes in order to allow for convergence towards an optimal value by occasionally exploring new states and actions.&lt;/p&gt;
&lt;h2&gt;Categorisies of RL agents&lt;/h2&gt;
&lt;p&gt;Reinforcement learning is like trial-and-error learning. The agent should discover a good policy from its experiences of the environment and Without losing too much reward along the way. The &lt;strong&gt;Exploration&lt;/strong&gt; finds more information about the environment. The &lt;strong&gt;Exploitation&lt;/strong&gt; exploits known information to maximise reward. It is usually important to explore as well as exploit.
An agent can evaluate the future Given a policy (&lt;strong&gt;Prediction&lt;/strong&gt;) or optimise the future and find the best policy (&lt;strong&gt;Control&lt;/strong&gt;)
There is five types of agents: &lt;strong&gt;Value Based&lt;/strong&gt; No Policy (Implicit)+ Value Function, &lt;strong&gt;Policy Based&lt;/strong&gt;: Policy + No Value Function, &lt;strong&gt;Actor Critic&lt;/strong&gt;: Policy+ Value Function, &lt;strong&gt;Model Free&lt;/strong&gt;: Policy and/or Value Function+ No Model
, &lt;strong&gt;Model Based&lt;/strong&gt;: Policy and/or Value Function+ Model
&lt;img alt="RL Agents" src="/images/RLAgents.png"&gt;&lt;/p&gt;
&lt;h2&gt;Classes of RL algorithms&lt;/h2&gt;
&lt;p&gt;RL knows three fundamental classes of methods for solving these learning problems: &lt;strong&gt;Dynamic Programming (DP)&lt;/strong&gt;, &lt;strong&gt;Monte Carlo methods&lt;/strong&gt;,  &lt;strong&gt;Temporal-difference learning&lt;/strong&gt;
Dependent on the problem at stake, each of these methods could be more suitable than the other. &lt;strong&gt;DP&lt;/strong&gt; methods are model-based and require therefore a complete and accurate model of the environment i.e. all the aforementioned functions of the environment need to be known to initiate learning. However,
the environment is not always defined prior to the learning process which poses a challenge to this method. This is where the two other &lt;strong&gt;model-free&lt;/strong&gt; learning methods come in handy. The &lt;strong&gt;Monte Carlo&lt;/strong&gt; algorithms only require an experience sample such as a data set in which the states, actions and rewards
of the (simulated) interaction with the environment. In comparison with DP methods, no model of the &lt;strong&gt;transition probability function&lt;/strong&gt; is required and neither the &lt;strong&gt;dynamics&lt;/strong&gt; of the environment. Monte Carlo algorithms solve the RL problem by &lt;strong&gt;averaging&lt;/strong&gt; sample return of each &lt;strong&gt;episode&lt;/strong&gt;. Only after the termination of an episode, that the value &lt;strong&gt;estimation&lt;/strong&gt; and &lt;strong&gt;policies&lt;/strong&gt; are updated. Hence, it is based on averages of complete returns of the value functions of each state. This class of algorithms does not exploit Markov property described before and is therefore more efficient in &lt;strong&gt;non-Markovian&lt;/strong&gt; environments. On the other hand, &lt;strong&gt;Temporal-Difference methods&lt;/strong&gt; do also not require a model of the environment but are like DP solving for incrementing &lt;strong&gt;step-by-step&lt;/strong&gt; rather than &lt;strong&gt;episode-by-episode&lt;/strong&gt;. Hence, TD methods exploit the &lt;strong&gt;Markovian property&lt;/strong&gt; and perform usually better in Markovian environments.
The choice between these two classes of model-free RL algorithms very much depends on the type of data set available. For continuous processes in which there are no fixed episodic transitions, &lt;strong&gt;Monte Carlo&lt;/strong&gt; methods may not be the optimal solution as they average the return only at the end of each episode. &lt;strong&gt;TD&lt;/strong&gt; algorithms might then be a better solution as they assign a reward incrementally over each state. This allows them to converge faster towards an optimal policy for large data sets with a large number state spaces.&lt;/p&gt;
&lt;h2&gt;Temporal-difference learning: On-policy and off-policy TD control&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TD&lt;/strong&gt; algorithms comprise two important RL classes of algorithms divided in &lt;strong&gt;Off-Policy&lt;/strong&gt; and &lt;strong&gt;On-Policy&lt;/strong&gt; TD control algorithm classes. The difference between the two lays in the policy that is learned from the simulation or set of experiences (data). &lt;strong&gt;On-Policy TD control&lt;/strong&gt; algorithms are often referred to as &lt;strong&gt;SARSA algorithms&lt;/strong&gt; in which the letters refer to the sequence of State, Action, Reward associated with the state transition, next State, next Action. This sequence is followed in each &lt;strong&gt;time-step&lt;/strong&gt; and is used to update the &lt;strong&gt;action-value&lt;/strong&gt; of these two states according:
&lt;img alt="Eq4" src="/images/eq4.png"&gt;
Here, α represents the step-size parameter which functions as the exponentially moving average parameter. It is especially useful for &lt;strong&gt;non-stationary&lt;/strong&gt; environments for weighting recent rewards more heavily than long-past ones. This could also be illustrated by rearranging the above equation to:
&lt;img alt="Eq5" src="/images/eq5.png"&gt;
If α is a number smaller than one for non-stationary environments which indicates that recent updates weight more than previous ones. This transition happens after every nonterminal state. The Q (st+1 ,at+1 ) components of every terminal state is defined as zero. Hence, every terminal state has an update value of 0. &lt;strong&gt;SARSA&lt;/strong&gt; is called an on-policy algorithm because it updates the &lt;strong&gt;action-value-function&lt;/strong&gt; according to the &lt;strong&gt;policy&lt;/strong&gt; it is taking in every &lt;strong&gt;step&lt;/strong&gt;. Therefore, it takes the epsilon-policy into account in order to arrive the optimal policy for a certain problem. &lt;strong&gt;Off-policy&lt;/strong&gt; algorithms approximate the best possible policy even when that policy is not taken by the agent. Hence, &lt;strong&gt;Off-Policy&lt;/strong&gt; algorithms base the update of the &lt;strong&gt;state action-value&lt;/strong&gt; function on the assumption of &lt;strong&gt;optimal behaviour&lt;/strong&gt; without taking into account the &lt;strong&gt;epsilon policy&lt;/strong&gt; (the chance to take a negative action). The cliff figure shows a suitable example given by Sutton and Barto (1998) and which illustrates the policy outcome differences between the two types of TD algorithms &lt;img alt="Cliff)" src="/images/cliff.png"&gt;. The cliff represents states with high negative reward. Since &lt;strong&gt;SARSA&lt;/strong&gt; takes the &lt;strong&gt;epsilon policy&lt;/strong&gt; into account, it learns that at some instances a non-optimal action will be taken which results in a high negative reward. Hence, it will learn to take the safe path rather than the optimal path. &lt;strong&gt;Q-learning algorithms&lt;/strong&gt; on the other hand, will take the optimal path by which the highest total reward could be achieved. This is because it does not take the &lt;strong&gt;epsilon probability&lt;/strong&gt; into account of taking an extremely negative action. This class of algorithms is denoted by the following equation:
&lt;img alt="Eq6" src="/images/eq6.png"&gt;
This difference will inevitably influence the suitability for the type of application. &lt;/p&gt;
&lt;h1&gt;Markov Decision Processes&lt;/h1&gt;
&lt;p&gt;Markov decision processes formally describe an environment for reinforcement learning Where the environment is fully observable, i.e. The current state completely characterises the process.
&lt;img alt="MDP" src="/images/MDP.png"&gt;
Almost all RL problems can be formalised as MDPs, e.g.Optimal control primarily deals with continuous MDPs, Partially observable problems can be converted into MDPs, Bandits are MDPs with one state
The Markov Property states that "The future is independent of the past given the present” in other ways a state St is Markov if and only if
P [S t+1 | S t ] = P [S t+1 | S 1 , ..., S t ]
The state captures all relevant information from the history and once the state is known, the history may be thrown away. i.e. The state is a sufficient statistic of the future.&lt;/p&gt;
&lt;p&gt;For a Markov state s and successor state s' , the state transition
probability is defined by Pss' = P[St+1 = s'| St = s]. The State transition matrix P defines transition probabilities from all states s to all successor states s' &lt;img alt="State Transition Matrix" src="/images/State_Transition_Matrix.png"&gt;where each row of the matrix sums to 1&lt;/p&gt;
&lt;p&gt;A Markov process is a memoryless random process, i.e. a sequence
of random states S1 , S2 , ... with the Markov property. Otherwise it is a tuple &lt;S,P&gt; with S is a (finite) set of states, P is a state transition probability matrix, Pss'= P [S t+1 = s'| St = s]
&lt;img alt="Example" src="/images/markov_process.png"&gt;&lt;/p&gt;
&lt;p&gt;A Markov reward process is a Markov chain with values.
Definition: A Markov Reward Process is a tuple &lt;S, P, R, γ&gt;
S is a finite set of states
P is a state transition probability matrix,
P ss'= P [St+1 = s'| St = s]
R is a reward function, Rs = E [Rt+1 | St = s]
γ is a discount factor, γ ∈ [0, 1]&lt;/p&gt;
&lt;p&gt;The return Gt is the total discounted reward from time-step t.
G t = Rt+1 + γRt+2 + ...
The discount γ ∈ [0, 1] is the present value of future rewards
The value of receiving reward R after k + 1 time-steps is γkR.
This values immediate reward above delayed reward.
γ close to 0 leads to ”myopic” evaluation
γ close to 1 leads to ”far-sighted” evaluation&lt;/p&gt;
&lt;p&gt;Most Markov reward and decision processes are discounted:
&lt;em&gt; Mathematically convenient to discount rewards
&lt;/em&gt; Avoids infinite returns in cyclic Markov processes
&lt;em&gt; Uncertainty about the future may not be fully represented
&lt;/em&gt; If the reward is financial, immediate rewards may earn more
interest than delayed rewards
&lt;em&gt; Animal/human behaviour shows preference for immediate
reward
&lt;/em&gt; It is sometimes possible to use undiscounted Markov reward
processes (i.e. γ = 1), e.g. if all sequences terminate.&lt;/p&gt;
&lt;p&gt;The value function v (s) gives the long-term value of state s
Definition : The state value function v (s) of an MRP is the expected return
starting from state s v (s) = E [G t | S t = s]
&lt;img alt="Value Function" src="/images/Value_Function.png"&gt;&lt;/p&gt;
&lt;h1&gt;Final Words&lt;/h1&gt;
&lt;p&gt;Reinforcement learning is extremely fun but hard topic. I am excited to learn more!&lt;/p&gt;</content><category term="Deep Reinforcement Learning"></category></entry><entry><title>Customizing a Docker image</title><link href="https://mohcinemadkour.github.io/posts/2018/06/Customizing%20a%20Docker%20image/" rel="alternate"></link><published>2018-06-12T13:01:00-04:00</published><updated>2018-06-12T13:01:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-06-12:/posts/2018/06/Customizing a Docker image/</id><summary type="html">&lt;h1&gt;Customizing a Docker image&lt;/h1&gt;
&lt;p&gt;Problem : Sometimes you have to use Docker images to troubleshoot R package issues. One of the tasks associated with this if for example the installation of system and R dependencies. This installation can be somewhat time consuming. In order to avoid this we can customize a …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Customizing a Docker image&lt;/h1&gt;
&lt;p&gt;Problem : Sometimes you have to use Docker images to troubleshoot R package issues. One of the tasks associated with this if for example the installation of system and R dependencies. This installation can be somewhat time consuming. In order to avoid this we can customize a container so that it has the dependencies we need. Here we’ll see how this is accomplished.&lt;/p&gt;
&lt;p&gt;We’ve been using the rocker r-devel-san-clang container. A first step is to fork this repository to your own account. The rocker repository can be found here.&lt;/p&gt;
&lt;p&gt;https://github.com/mohcinemadkour/r-devel-san-clang
You should be able to find a button in the top right of the page where you can fork the repository. We can now clone it to make a local copy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/mohcinemadkour/r-devel-san-clang.git
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can make a branch so that we keep our work seperate from master. Information on branching can be found at the &lt;a href="https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging"&gt;git documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git checkout -b bjk
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can now open the Dockerfile in our favorite text editor and customize it. In the Dockerfile we’ll see a couplpe of lines as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;RUN apt-get update -qq \
    &amp;amp;&amp;amp; apt-get install -t unstable -y --no-install-recommends \
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is where we’ll add our system dependencies. Docker’s best practices guide asks us to add our dependencies alphabetically to aid maintenance. We’ll add pandoc and qpdf.&lt;/p&gt;
&lt;p&gt;At the end of the file we’ll add a few lines to install our dependencies. For your package you will want to modify this list.&lt;/p&gt;
&lt;h2&gt;Install vcfR dependencies&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;RUN R --slave -e &amp;#39;install.packages(c(&amp;quot;ape&amp;quot;, &amp;quot;dplyr&amp;quot;, &amp;quot;knitr&amp;quot;, &amp;quot;poppr&amp;quot;, &amp;quot;Rcpp&amp;quot;, &amp;quot;memuse&amp;quot;, &amp;quot;pinfsc50&amp;quot;, &amp;quot;rmarkdown&amp;quot;, &amp;quot;testthat&amp;quot;, &amp;quot;tidyr&amp;quot;, &amp;quot;vegan&amp;quot;, &amp;quot;viridisLite&amp;quot;), dependencies = TRUE)&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Docker documentation includes directions on how to build our app. For our container it should look like this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo docker build -t r-devel-san-clang .
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here the -t flag allows us to add a tag. Note that it will take a bit for the dependencies to download and install. Okay, it will take quite a bit.&lt;/p&gt;
&lt;p&gt;After everything is installed we should be able to start the image&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo docker run --name=r-devel-san-clang -v ~/gits/vcfR:/RSource/vcfR --rm -ti r-devel-san-clang /bin/bash
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While building the container requires an investment in time, the result is that now when we run our image we have all of our dependencies installed and do not have to invest time in to install them. We should be ready to proceed to testing our docker now.&lt;/p&gt;</content><category term="Docker image. Dockerfile"></category></entry><entry><title>Recommender systems and knowledge graphs with Neo4j and Python</title><link href="https://mohcinemadkour.github.io/posts/2018/06/Getting%20started%20with%20Neo4j%20and%20Python/" rel="alternate"></link><published>2018-06-10T18:00:00-04:00</published><updated>2018-06-10T18:00:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-06-10:/posts/2018/06/Getting started with Neo4j and Python/</id><summary type="html">&lt;p&gt;https://www.meetup.com/Neo4j-Online-Meetup/events/251545325/
https://github.com/davidmeza1/doctopics
https://neo4j.com/blog/nasa-lesson-learned-database-using-neo4j-linkurious/
https://github.com/nicolewhite/neo4j-flask
https://neo4j.com/developer/python/
https://github.com/neo4j-examples/movies-python-bolt&lt;/p&gt;</summary><content type="html">&lt;p&gt;https://www.meetup.com/Neo4j-Online-Meetup/events/251545325/
https://github.com/davidmeza1/doctopics
https://neo4j.com/blog/nasa-lesson-learned-database-using-neo4j-linkurious/
https://github.com/nicolewhite/neo4j-flask
https://neo4j.com/developer/python/
https://github.com/neo4j-examples/movies-python-bolt&lt;/p&gt;</content><category term="Getting started with Neo4j and Python"></category></entry><entry><title>Hands on Risk Scores: knowing your data and methods</title><link href="https://mohcinemadkour.github.io/posts/2018/06/Risk%20Prediction/" rel="alternate"></link><published>2018-06-10T18:00:00-04:00</published><updated>2018-06-10T18:00:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-06-10:/posts/2018/06/Risk Prediction/</id><summary type="html">&lt;h1&gt;Difference between diagnostic and prognostic&lt;/h1&gt;
&lt;p&gt;&lt;img alt=" Diagnosis or Prognosis" src="/images/diagprog.png"&gt;&lt;/p&gt;
&lt;h1&gt;Objective&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Why" src="/images/checklist.png"&gt;&lt;/p&gt;
&lt;h1&gt;Method&lt;/h1&gt;
&lt;p&gt;&lt;img alt="How" src="/images/method.png"&gt;&lt;/p&gt;
&lt;h1&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img alt="WHAT you found" src="/images/result.png"&gt;&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;&lt;img alt="WHAT it means" src="/images/discussion.png"&gt;&lt;/p&gt;
&lt;h1&gt;Data Split&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Validation and test set" src="/images/analysistypes.png"&gt;&lt;/p&gt;
&lt;p&gt;Reference [(TRIPOD group (Collins et al), Annals, 2015.] http://annals.org/aim/article/2088549/transparent-reporting-multivariable-prediction-model-individual-prognosis-diagnosis-tripod-tripod )&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Difference between diagnostic and prognostic&lt;/h1&gt;
&lt;p&gt;&lt;img alt=" Diagnosis or Prognosis" src="/images/diagprog.png"&gt;&lt;/p&gt;
&lt;h1&gt;Objective&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Why" src="/images/checklist.png"&gt;&lt;/p&gt;
&lt;h1&gt;Method&lt;/h1&gt;
&lt;p&gt;&lt;img alt="How" src="/images/method.png"&gt;&lt;/p&gt;
&lt;h1&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img alt="WHAT you found" src="/images/result.png"&gt;&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;&lt;img alt="WHAT it means" src="/images/discussion.png"&gt;&lt;/p&gt;
&lt;h1&gt;Data Split&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Validation and test set" src="/images/analysistypes.png"&gt;&lt;/p&gt;
&lt;p&gt;Reference [(TRIPOD group (Collins et al), Annals, 2015.] http://annals.org/aim/article/2088549/transparent-reporting-multivariable-prediction-model-individual-prognosis-diagnosis-tripod-tripod )&lt;/p&gt;</content><category term="Risk Prediction"></category></entry><entry><title>Understanding AUC (of ROC), sensitivity and specificity values</title><link href="https://mohcinemadkour.github.io/posts/2018/06/Understanding%20AUC%20(of%20ROC),%20sensitivity%20and%20specificity%20values/" rel="alternate"></link><published>2018-06-09T16:00:00-04:00</published><updated>2018-06-09T16:00:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-06-09:/posts/2018/06/Understanding AUC (of ROC), sensitivity and specificity values/</id><summary type="html">&lt;h1&gt;Understanding AUC (of ROC), sensitivity and specificity values&lt;/h1&gt;
&lt;p&gt;The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Each point of the ROC curve (i.e. threshold) corresponds to specific values of sensitivity and specificity. The area under …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Understanding AUC (of ROC), sensitivity and specificity values&lt;/h1&gt;
&lt;p&gt;The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Each point of the ROC curve (i.e. threshold) corresponds to specific values of sensitivity and specificity. The area under the ROC curve (AUC) is a summary measure of performance, that indicates whether on average a true positive is ranked higher than a false positives. If model A has higher AUC than model B, model A is performing better on average, but there still could be specific areas of the ROC space where model B is better (i.e. thresholds for which sensitivity and specificity are higher for model B than A&lt;/p&gt;
&lt;h2&gt;Sensitivity (positive in disease)&lt;/h2&gt;
&lt;p&gt;Sensitivity is the ability of a test to correctly classify an individual as ′diseased′&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Sensitivity = a / a+c
= a (true positive) / a+c (true positive + false negative)
= Probability of being test positive when disease present.
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Specificity (negative in health)&lt;/h2&gt;
&lt;p&gt;The ability of a test to correctly classify an individual as disease- free is called the test′s specificity&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Specificity = d / b+d
= d (true negative) / b+d (true negative + false positive)
= Probability of being test negative when disease absent.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sensitivity and specificity are inversely proportional, meaning that as the sensitivity increases, the specificity decreases and vice versa.&lt;/p&gt;
&lt;h2&gt;Positive Predictive Value (PPV)&lt;/h2&gt;
&lt;p&gt;It is the percentage of patients with a positive test who actually have the disease. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;PPV: = a / a+b
= a (true positive) / a+b (true positive + false positive)
= Probability (patient having disease when test is positive)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Negative Predictive Value (NPV)&lt;/h2&gt;
&lt;p&gt;It is the percentage of patients with a negative test who do not have the disease.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;NPV:    =   d / c+d
=   d (true negative) / c+d (false negative + true negative)
=   Probability (patient not having disease when test is negative)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Positive and negative predictive values are directly related to the prevalence of the disease in the population. Assuming all other factors remain constant, the PPV will increase with increasing prevalence; and NPV decreases with increase in prevalence.
&lt;img alt=" Effect of disease prevalence on PPV and NPV" src="/images/PPV-NPV.jpeg"&gt;&lt;/p&gt;
&lt;h1&gt;Calculation of metric performance&lt;/h1&gt;
&lt;p&gt;Calculation of acc, ppv, npv, sen, spe, yod&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def ROC_parameters(obser,score,thr):
    #print obser,score,thr
    temp=np.zeros(len(score))
    #print thr;
    temp[[ i for i, x in enumerate(score) if x &amp;gt;= thr ]]= 1
    p_ind=[ i for i, x in enumerate(obser) if x == 1 ]
    n_ind = [ i for i, x in enumerate(obser) if x == 0 ]
    TP = sum(temp[p_ind]==1)
    FP = sum(temp[n_ind]==1)
    TN =sum(temp[n_ind]==0)
    FN = sum(temp[p_ind]==0)
    acc = (float)(TP+TN)/len(temp)
    #print TP,FP,TN,FN;
    if TP+FP&amp;gt;0:
        ppv = (float)(TP)/(TP+FP)
    else:
        ppv=np.NaN
    if TN+FN&amp;gt;0:
        npv = (float)(TN)/(TN+FN)
    else:
        npv=np.NaN
    if TP+FN&amp;gt;0:
        sen = (float)(TP)/(TP+FN)
    else:
        sen=np.NaN
    if TN+FP&amp;gt;0:
        spe = (float)(TN)/(TN+FP)
    else:
        spe=np.NaN
    yod = (float)(sen+spe-1)
    ls=list();
    ls.append(acc)
    ls.append(ppv)
    ls.append(npv)
    ls.append(sen)
    ls.append(spe)
    ls.append(yod)
    return ls
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Calculate Metrics&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def calculate_metric&lt;span class="p"&gt;(&lt;/span&gt;outcome&lt;span class="p"&gt;,&lt;/span&gt; score&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    obser &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;outcome&lt;span class="p"&gt;))&lt;/span&gt;
    obser&lt;span class="p"&gt;[[&lt;/span&gt;i &lt;span class="kr"&gt;for&lt;/span&gt; i&lt;span class="p"&gt;,&lt;/span&gt; x &lt;span class="kr"&gt;in&lt;/span&gt; enumerate&lt;span class="p"&gt;(&lt;/span&gt;outcome&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;if&lt;/span&gt; x &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt;
    obser &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;float&lt;span class="p"&gt;(&lt;/span&gt;i&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;for&lt;/span&gt; i &lt;span class="kr"&gt;in&lt;/span&gt; obser&lt;span class="p"&gt;]&lt;/span&gt;
    score &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;float&lt;span class="p"&gt;(&lt;/span&gt;i&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;for&lt;/span&gt; i &lt;span class="kr"&gt;in&lt;/span&gt; score&lt;span class="p"&gt;]&lt;/span&gt;
    prev &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    thres &lt;span class="o"&gt;=&lt;/span&gt; np.arange&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;1.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;#(0.01,0.98,0.01)&lt;/span&gt;
    xval &lt;span class="o"&gt;=&lt;/span&gt; thres
    acc &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    ppv &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    npv &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    sen &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    spe &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    yod &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    auc &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="kr"&gt;for&lt;/span&gt; l &lt;span class="kr"&gt;in&lt;/span&gt; &lt;span class="kp"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        plotdata &lt;span class="o"&gt;=&lt;/span&gt; ROC_parameters&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;,&lt;/span&gt;score&lt;span class="p"&gt;,&lt;/span&gt;thres&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;])&lt;/span&gt;
        acc&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        ppv&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        npv&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        sen&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        spe&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        yod&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        auc&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; roc_auc_score&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;,&lt;/span&gt; score&lt;span class="p"&gt;)&lt;/span&gt;
    prev &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;#roc_vals=np.zeros((length(spe),8))&lt;/span&gt;
    roc_vals&lt;span class="o"&gt;=&lt;/span&gt;pd.DataFrame&lt;span class="p"&gt;(&lt;/span&gt;index&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kp"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;101&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; columns&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;thres&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;acc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ppv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;npv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;specificity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;sensitivity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;yod_index&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;auc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="c1"&gt;#roc_vals &amp;lt;- dacolnames(roc_vals) &amp;lt;- c(&amp;quot;thres&amp;quot;,&amp;quot;acc&amp;quot;,&amp;quot;ppv&amp;quot;,&amp;quot;npv&amp;quot;,&amp;quot;specificity&amp;quot;,&amp;quot;sensitivity&amp;quot;,&amp;quot;yod_index&amp;quot;,&amp;quot;auc&amp;quot;)&lt;/span&gt;
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;thres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;thres
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;acc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; acc
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ppv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; ppv
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;npv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; npv
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;specificity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;spe
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sensitivity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; sen
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;yod_index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; yod&lt;span class="p"&gt;;&lt;/span&gt;
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;auc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; auc&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kr"&gt;return&lt;/span&gt; roc_vals
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Confidence_lower, Confidence_upper&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def confidence_interval(panel):
    vector = []
    confidence_lower=panel[1].copy()
    confidence_upper=panel[1].copy()
    nr=len(panel[1].axes[0])
    nc=len(panel[1].axes[1])
    for ix in  range(0,nr):
        for iy in range(0,nc):
            vector = []
            for k, df in panel.iteritems():
                vector.append(df.iloc[ix,iy])
            sorted_vector = np.array(vector)
            sorted_vector.sort()
            confidence_lower.iloc[ix,iy] = sorted_vector[int(0.05 * len(sorted_vector))]
            confidence_upper.iloc[ix,iy] = sorted_vector[int(0.95 * len(sorted_vector))]
    return confidence_lower, confidence_upper
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Boostraping: best_model_metric,panel_models&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def calculate_metric_boostrap(outcome, score):
    d = []
    for p in range(0,len(score)):
        d.append((score[p]))
    score=pd.Series(d)
    n_bootstraps = 100
    rng_seed = 42  # control reproducibility
    scores_table = {} 
    rng = np.random.RandomState(rng_seed)
    for i in range(n_bootstraps):
    # bootstrap by sampling with replacement on the prediction indices
        indices = rng.random_integers(0, len(outcome) - 1, len(outcome))
        if len(np.unique(outcome[indices])) &amp;lt; 2:
        # We need at least one positive and one negative sample for ROC AUC
        # to be defined: reject the sample
            continue
        scores_table[i]= calculate_metric(outcome[indices], score[indices])

    panel = pd.Panel(scores_table)
    df=panel.mean(axis=0)
    return df,panel
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Complete Example&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;testdata=pd.read_csv(main_folder+&amp;quot;/Val_Cohort/test_cohort_&amp;quot;+riskname+&amp;quot;.csv&amp;quot;)
devdata=pd.read_csv(main_folder+&amp;quot;/Dev_Cohort/development_cohort_&amp;quot;+riskname+&amp;quot;.csv&amp;quot;)
ro.globalenv[&amp;#39;r_df_cvcomp&amp;#39;] = pandas2ri.py2ri(testdata)
r_df_cvcomp=ro.r(&amp;#39;r_df_cvcomp&amp;#39;)
pred_score_cvcomp=statsf.predict(best_model_cvcomp,r_df_cvcomp, type=&amp;quot;response&amp;quot;)
pred_obser_cvcomp=testdata[&amp;#39;outcome&amp;#39;];
best_model_metric,panel_models=cm.calculate_metric_boostrap(pred_obser_cvcomp,pred_score_cvcomp)
confidence_lower, confidence_upper=cm.confidence_interval(panel_models)
best_model_metric.to_csv(main_folder+&amp;quot;/Prediction/Best_Model_Metrics/best_model_metric_&amp;quot;+riskname+&amp;quot;.csv&amp;quot;)
confidence_lower.to_csv(main_folder+&amp;quot;/Prediction/Best_Model_Metrics/confidence_lower_&amp;quot;+riskname+&amp;quot;.csv&amp;quot;)
confidence_upper.to_csv(main_folder+&amp;quot;/Prediction/Best_Model_Metrics/confidence_upper_&amp;quot;+riskname+&amp;quot;.csv&amp;quot;)
cutoff1_cvcomp=best_model_metric[&amp;#39;thres&amp;#39;].iloc[best_model_metric[&amp;#39;yod_index&amp;#39;].idxmax()-1]
cutoff2_cvcomp=cu.cal_cutoff2(best_model_metric)
with open(main_folder+&amp;quot;/Prediction/Cutoffs/cutoffs_&amp;quot;+riskname+&amp;quot;.txt&amp;quot;, &amp;quot;w&amp;quot;) as text_file: 
    text_file.write(&amp;quot;cutoff1: %s&amp;quot; % cutoff1_cvcomp.astype(float)+&amp;quot;\n&amp;quot;+&amp;quot;cutoff2: %s&amp;quot; % cutoff2_cvcomp.astype(float)) 
predicted_values_cvcomp =pd.DataFrame({&amp;quot;prediction&amp;quot;:pred_score_cvcomp}, index=range(0,len(pred_score_cvcomp)))
predicted_values_cvcomp.loc[predicted_values_cvcomp.prediction &amp;lt;= cutoff1_cvcomp, &amp;#39;category&amp;#39;] = &amp;#39;low&amp;#39; 
predicted_values_cvcomp.loc[predicted_values_cvcomp.prediction &amp;gt;cutoff2_cvcomp, &amp;#39;category&amp;#39;] = &amp;#39;high&amp;#39; 
predicted_values_cvcomp.loc[(predicted_values_cvcomp.prediction &amp;lt;=cutoff2_cvcomp) &amp;amp; (predicted_values_cvcomp.prediction&amp;gt;cutoff1_cvcomp), &amp;#39;category&amp;#39;] = &amp;#39;moderate&amp;#39; 
predicted_values_cvcomp.to_csv(main_folder+&amp;quot;/Val_Cohort/predicted_values_&amp;quot;+riskname+&amp;quot;.csv&amp;quot;)
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Plot&lt;/h1&gt;
&lt;h1&gt;Example of Scoring Learners and Cohort&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Cohort Definition&lt;/th&gt;
&lt;th&gt;Cohort Size&lt;/th&gt;
&lt;th&gt;CVD Percent in Cohort&lt;/th&gt;
&lt;th&gt;Covariates in Learner/Model&lt;/th&gt;
&lt;th&gt;Method Type&lt;/th&gt;
&lt;th&gt;Method Sensitivity&lt;/th&gt;
&lt;th&gt;Method PPV&lt;/th&gt;
&lt;th&gt;Balanced Accuracy&lt;/th&gt;
&lt;th&gt;Method Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ALL OF THEM (don't emulate&lt;/td&gt;
&lt;td&gt;369000&lt;/td&gt;
&lt;td&gt;0.80%&lt;/td&gt;
&lt;td&gt;"bmi&lt;/td&gt;
&lt;td&gt;numAge&lt;/td&gt;
&lt;td&gt;tchol&lt;/td&gt;
&lt;td&gt;sbp&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;t2d"&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age &amp;gt; 55&lt;/td&gt;
&lt;td&gt;122792&lt;/td&gt;
&lt;td&gt;22.60%&lt;/td&gt;
&lt;td&gt;"numAge&lt;/td&gt;
&lt;td&gt;tchol&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;gender"&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;0.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age 20-40&lt;/td&gt;
&lt;td&gt;121130&lt;/td&gt;
&lt;td&gt;0.02%&lt;/td&gt;
&lt;td&gt;"tchol&lt;/td&gt;
&lt;td&gt;t2d&lt;/td&gt;
&lt;td&gt;smoking&lt;/td&gt;
&lt;td&gt;race"&lt;/td&gt;
&lt;td&gt;LDA&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;"htn == ""Y"""&lt;/td&gt;
&lt;td&gt;108510&lt;/td&gt;
&lt;td&gt;18.85%&lt;/td&gt;
&lt;td&gt;smoking&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;"""NA"""&lt;/td&gt;
&lt;td&gt;"""NA"""&lt;/td&gt;
&lt;td&gt;"""NA"" (is this weird?)"&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;"gender == ""F"" &amp;amp; numAge &amp;gt; 60"&lt;/td&gt;
&lt;td&gt;53929&lt;/td&gt;
&lt;td&gt;14.30%&lt;/td&gt;
&lt;td&gt;"tchol&lt;/td&gt;
&lt;td&gt;htn"&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;NaN&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age 30-45&lt;/td&gt;
&lt;td&gt;99930&lt;/td&gt;
&lt;td&gt;"numAge&lt;/td&gt;
&lt;td&gt;race&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;gender&lt;/td&gt;
&lt;td&gt;smoking"&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age &amp;lt;= 40&lt;/td&gt;
&lt;td&gt;93980 train; 93981 test&lt;/td&gt;
&lt;td&gt;1.64%&lt;/td&gt;
&lt;td&gt;"numAge&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;smoking&lt;/td&gt;
&lt;td&gt;treat&lt;/td&gt;
&lt;td&gt;t2d&lt;/td&gt;
&lt;td&gt;gender&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;"gender == ""M"" &amp;amp; numAge &amp;gt; 60"&lt;/td&gt;
&lt;td&gt;36853&lt;/td&gt;
&lt;td&gt;30.75%&lt;/td&gt;
&lt;td&gt;"tchol&lt;/td&gt;
&lt;td&gt;htn"&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;0.28&lt;/td&gt;
&lt;td&gt;0.51&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GENETICS&lt;/td&gt;
&lt;td&gt;66100&lt;/td&gt;
&lt;td&gt;2.40%&lt;/td&gt;
&lt;td&gt;"tchol&lt;/td&gt;
&lt;td&gt;rs8055236&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;t2d&lt;/td&gt;
&lt;td&gt;smoking"&lt;/td&gt;
&lt;td&gt;lda&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;age &amp;lt; 55 &amp;amp; age &amp;gt; 35&lt;/td&gt;
&lt;td&gt;379272&lt;/td&gt;
&lt;td&gt;5.30%&lt;/td&gt;
&lt;td&gt;cvd ~ tchol + htn + t2d + bmi + rs8055236&lt;/td&gt;
&lt;td&gt;logit&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;nan&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;age&amp;gt;55&lt;/td&gt;
&lt;td&gt;logit&lt;/td&gt;
&lt;td&gt;0.22&lt;/td&gt;
&lt;td&gt;0.59&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GENETICS&lt;/td&gt;
&lt;td&gt;46217 train; 8157 test&lt;/td&gt;
&lt;td&gt;9.78%&lt;/td&gt;
&lt;td&gt;"cvd ~ numAge + htn + smoking&lt;/td&gt;
&lt;td&gt;+ treat + t2d + gender + bmi + tchol + sbp + rs10757278 + rs4665058 + rs8055236"&lt;/td&gt;
&lt;td&gt;SuperLearner&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;td&gt;0.369&lt;/td&gt;
&lt;td&gt;0.8394&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="AUC"></category><category term="Sensitivity"></category><category term="Specificity"></category></entry><entry><title>Text-mining exercises</title><link href="https://mohcinemadkour.github.io/posts/2018/06/text%20mining/" rel="alternate"></link><published>2018-06-05T02:08:00-04:00</published><updated>2018-06-05T02:08:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-06-05:/posts/2018/06/text mining/</id><summary type="html">&lt;p&gt;The original post is at &lt;a href="https://jensenlab.org/"&gt;JensenLab&lt;/a&gt;. By &lt;a href="https://jensenlab.org/people/larsjuhljensen/"&gt;Dr Lars Juhl Jensen&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Text-mining exercises&lt;/h1&gt;
&lt;h2&gt;Learning objectives&lt;/h2&gt;
&lt;p&gt;In these exercises, we will use a variety of text-mining tools and databases based on text mining, to interpret the results from microbiome studies. The exercises will teach you how to:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;automatically highlight named entities …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;The original post is at &lt;a href="https://jensenlab.org/"&gt;JensenLab&lt;/a&gt;. By &lt;a href="https://jensenlab.org/people/larsjuhljensen/"&gt;Dr Lars Juhl Jensen&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Text-mining exercises&lt;/h1&gt;
&lt;h2&gt;Learning objectives&lt;/h2&gt;
&lt;p&gt;In these exercises, we will use a variety of text-mining tools and databases based on text mining, to interpret the results from microbiome studies. The exercises will teach you how to:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;automatically highlight named entities in a web page
&lt;/em&gt;use named entity recognition for synonym-aware information retrieval
&lt;em&gt;extract associations based on cooccurrence of entities in the literature
&lt;/em&gt;discover novel, indirect associations between entities
*perform text-mining-based term enrichment analysis&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Whereas exercise 1 requires only a web browser, parts of exercise 2 takes place on the command line and involves running Python scripts. To follow exercise 2, you thus need access to a Mac or Linux computer with Python installed, and you need basic knowledge of how to use the command line.&lt;/p&gt;
&lt;h2&gt;Exercise 1&lt;/h2&gt;
&lt;p&gt;In this exercise we will first introduce the basics of text mining: 1) dictionary-based named entity recognition and 2) how this can used to help retrieve literature. Afterwards we will move on to how one can use the complete literature to 3) extract associations between entities and finally 4) how these associations can be used for knowledge discovery.&lt;/p&gt;
&lt;h3&gt;1.1. Named entity recognition&lt;/h3&gt;
&lt;p&gt;The goal of named entity recognition (NER) is to find names mentioned in text and resolve them to the underlying biomedical entities (document → A, B, C). To illustrate this, we will use the EXTRACT tool, which is designed to use NER to support manual database curation.&lt;/p&gt;
&lt;p&gt;Install the EXTRACT bookmarklet as described on the &lt;a href="https://extract.jensenlab.org/"&gt;EXTRACT website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Open the paper “Intestinal Microbiota and the Efficacy of Fecal Microbiota Transplantation in Gastrointestinal Disease” &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4073534/"&gt;(Aroniadis et al., 2014)&lt;/a&gt; and click the &lt;strong&gt;EXTRACT&lt;/strong&gt; bookmarklet. After a short time, terms should be highlighted in the text.&lt;/p&gt;
&lt;p&gt;What do the different colors mean?&lt;/p&gt;
&lt;p&gt;By clicking or hovering over a tagged term, you will get a popup that includes its standard name, entity type, database or ontology identifier, and a link to its reference record. Click or hover over &lt;strong&gt;Micrococcus pyogenes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What is the difference between Micrococcus pyogenes and Staphylococcus aureus?&lt;/p&gt;
&lt;p&gt;Select the &lt;strong&gt;Keywords&lt;/strong&gt; line and click the &lt;strong&gt;EXTRACT&lt;/strong&gt; bookmarklet. Hover over terms in the text or lines in the table.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What happens?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Use the buttons in the popup to copy the data into a spreadsheet/text file or save it in tabular format.&lt;/p&gt;
&lt;p&gt;Which information is then provided in addition to what is shown in the popup?&lt;/p&gt;
&lt;h3&gt;1.2. Information retrieval&lt;/h3&gt;
&lt;p&gt;The goal of information retrieval (IR) is to find the documents pertaining to a topic of interest. When the topic is a biological entity (A), NER can be used to index the literature and thereby support retrieval of relevant documents (A → documents).&lt;/p&gt;
&lt;p&gt;We run the same NER system used in EXTRACT on entire PubMed every week and make the results available through a suite of web resources. One such resource is &lt;a href="https://organisms.jensenlab.org/"&gt;ORGANISMS&lt;/a&gt;. It allows users to retrieve abstracts that mention any organism of interest (specified by an NCBI TaxID) based on the NER results.&lt;/p&gt;
&lt;p&gt;Go to https://organisms.jensenlab.org/ and query for &lt;strong&gt;S. aureus.&lt;/strong&gt; You are now presented with several options, since there are many genera starting with S that include an aureus species. Click on the &lt;strong&gt;Staphylococcus aureus&lt;/strong&gt; (taxid:1280) row to view the abstracts for this species.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Do the abstracts shown all mention Staphylococcus aureus?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Go back to the search page (e.g. by clicking &lt;strong&gt;ORGANISMS&lt;/strong&gt; in the header) and query for &lt;strong&gt;Firmicutes&lt;/strong&gt;. You are again presented with many options including the Firmicutes phylum itself (taxid:1239) as well as numerous species and strains. Click on the row for the phylum to view abstracts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Which taxa do you see abstracts for?&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;1.3. Information extraction&lt;/h3&gt;
&lt;p&gt;The goal of cooccurrence-based information extraction (IE) is to link entities (A, B, C) to each other based on them being mentioned together in documents (A → documents → B; B → documents → C).&lt;/p&gt;
&lt;p&gt;Go to https://diseases.jensenlab.org/ and query for &lt;strong&gt;Crohn disease&lt;/strong&gt;. Again, click on it on the disambiguation page (like in the ORGANISMS resource).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Which gene is most strongly associated with Crohn’s disease according to text mining?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Click on &lt;strong&gt;NOD2&lt;/strong&gt; in the text-mining table.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Do the abstracts in fact support an association between Crohn’s disease and NOD2?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Cooccurrence-based IE is a very generic approach, which can be used to find associations between any two types of entities for which we can do NER. For example, we can use the same approach to link Staphylococcus aureus together with the gene NOD2:&lt;/p&gt;
&lt;p&gt;https://organisms.jensenlab.org/Entity?documents=10&amp;amp;type1=-2&amp;amp;id1=1280&amp;amp;type2=9606&amp;amp;id2=ENSP00000300589&lt;/p&gt;
&lt;h3&gt;1.4. Knowledge discovery&lt;/h3&gt;
&lt;p&gt;The goal of knowledge discovery is to find indirect associations between entities (A, C) via other entities (B). In the so-called closed discovery problem, we search for B entities that can explain an observed association between A and C (A → B ← C), which may never have been mentioned together in the literature. For example, we saw above that both Crohn’s disease and Staphylococcus aureus have links to NOD2, which could mechanistically explain an observed association between Staphylococcus aureus and Crohn’s disease.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://arrowsmith.psych.uic.edu/cgi-bin/arrowsmith_uic/start.cgi"&gt;ARROWSMITH&lt;/a&gt; is a tool for discovering such associations in a systematic manner; its Two-Node Literature Search corresponds to the closed discovery problem.&lt;/p&gt;
&lt;p&gt;Open ARROWSMITH and do a basic two-node literature search using Staphylococcus aureus as A-literature and Crohn’s disease as C-literature. After some minutes you should see a ranked list of B-terms that were mentioned in both the A-literature and the C-literature.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Which is the top-ranking B-term?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Inspect some of the literature supporting the A–B and the B–C associations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Does B provide a plausible connection between A and C?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Exercise 2&lt;/h2&gt;
&lt;p&gt;In this exercise, we will focus on how one can utilize the text-mining tools used in exercise 1 to interpret the results from a microbiome analysis. To this end, we will start from the results published on the human colorectal cancer microbiome &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4299606/"&gt;(Zeller et al., 2014)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is important to note that there are currently no dedicated text-mining tools that have been designed to aid microbiome analysis. What we will do is thus to (ab)use existing text-mining tools and resources, to illustrate what is already now possible with text-mining and which will hopefully be possible to do in a more user-friendly manner in the future.&lt;/p&gt;
&lt;h3&gt;2.1. Using NER to dig deeper into the literature&lt;/h3&gt;
&lt;p&gt;Colorectal cancer studies have revealed a strong cooccurrence pattern between the proinflammatory Fusobacterium nucleatum and Parvimonas micra. This led to a systematic search for literature linking also the latter bacterial species to inflammatory response. A simple PubMed search retrieves only three publications:&lt;/p&gt;
&lt;p&gt;https://www.ncbi.nlm.nih.gov/pubmed/?term=%22Parvimonas+micra%22+%22inflammatory+response%22&lt;/p&gt;
&lt;p&gt;Of these, only one had been published when the colorectal cancer microbiome was being analyzed, and it sheds no light on the topic. However, since Parvimonas micra has an NCBI Taxon ID (taxid:33033) and inflammatory response is a GO term (GO:0006954), we can instead use the results of NER to retrieve relevant documents:&lt;/p&gt;
&lt;p&gt;http://organisms.jensenlab.org/Entity?documents=10&amp;amp;type1=-2&amp;amp;id1=33033&amp;amp;type2=-21&amp;amp;id2=GO:0006954&lt;/p&gt;
&lt;p&gt;Because NER makes use of synonyms, this retrieves several additional publications. Inspect these abstracts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Are they relevant and why were they were not found by the initial search?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One of the abstracts (Yoshioka et al., 2005) reveals a possible link between the two bacteria and oral inflammatory response: Parvimonas micra can bind to lipopolysaccharides on Gram-negative bacteria such as Fusobacterium nucleatum and thereby induce inflammatory response. This publication was missed by the PubMed query, because Parvimonas micra is referred to under its older name Peptostreptococcus micros. The species is thus mentioned, but a search for its current name will not retrieve it.&lt;/p&gt;
&lt;p&gt;Open the abstract by Yoshioka et al. in PubMed and run EXTRACT on it. Inspect the tagging of Peptostreptococcus micros.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Which name is listed for the species in the popup?&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;2.2. Retrieval of literature linking taxa to a disease&lt;/h3&gt;
&lt;p&gt;Above, we saw how existing text-mining resources can be used to retrieve abstracts that mention a species of interest with a disease of interest. Very similar to the previous exercise, we can easily look up the abstracts that mention, for example, Fusobacterium nucleatum (taxid:851) together with colorectal cancer (DOID:9256):&lt;/p&gt;
&lt;p&gt;https://organisms.jensenlab.org/Entity?documents=10&amp;amp;type1=-2&amp;amp;id1=851&amp;amp;type2=-26&amp;amp;id2=DOID:9256&lt;/p&gt;
&lt;p&gt;To do this systematically for all taxa found in a microbiome study, automation is desirable. One could obviously very easily produce a web page with links like the one above for a list of organisms. However, we will instead directly produce a list of PMIDs for each organism, in which the organism is co-mentioned with the disease of interest.&lt;/p&gt;
&lt;p&gt;The first step in such an information retrieval task is to find the set of documents that mention the disease of interest. The PMIDs of all abstracts that mention a given disease, including colorectal cancer, can be found in this file:&lt;/p&gt;
&lt;p&gt;http://download.jensenlab.org/disease_textmining_mentions.tsv&lt;/p&gt;
&lt;p&gt;The second step is to similarly retrieve the PMIDs that mention each organism, which can be found in this file:&lt;/p&gt;
&lt;p&gt;http://download.jensenlab.org/organism_textmining_mentions.tsv&lt;/p&gt;
&lt;p&gt;All we now need is a script that does the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Load the PMIDs associated with the disease of interest into memory&lt;/li&gt;
&lt;li&gt;Read the NCBI TaxIDs for each organisms of interest.&lt;/li&gt;
&lt;li&gt;For each PMID, check if it is associated with the disease and print it if this is the case&lt;br&gt;
We have made a &lt;a href="https://jensenlab.org/assets/textmining/disease_comentions.py"&gt;Python script&lt;/a&gt; that does this and prepared a &lt;a href="https://jensenlab.org/assets/textmining/organisms.txt"&gt;file with NCBI TaxIDs&lt;/a&gt; of the organisms of interest from the colorectal cancer microbiome study. Download both and run:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;```python disease_comentions.py DOID:9256 organisms.txt&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;The script writes its output to the terminal, which you can redirect to a file with the &amp;gt; operator if desired. The format of the tab-delimited output is the same as the input file with organism mentions: the first column contains the NCBI TaxID and the second column contains a space-delimited list of PMIDs. These PMIDs are the abstracts that mention the organism as well as the disease of interest.

These PMIDs can serve as a starting point for a variety of downstream analyses, including calculating simple count statistics, manually inspecting/curating all the associated articles, or retrieving all abstract texts for additional automatic text-mining analyses. The script can trivially be modified to retrieve PMIDs associating organisms with other types of named entities than diseases, such as tissues or environmental descriptors.

### 2.3. Characterization of microbiomes

Already prior to the microbiome study analyzed here, it had been noted that several bacteria associated with colorectal cancer were first described as oral pathogens. It had also been suggested that their invasion of the gut might cause or contribute to tumorigenesis (Warren et al., 2013).

To explore this in a systematic manner, we will investigate text-mined associations between bacteria identified in the colorectal cancer microbiome study and tissues.

The first step is thus to download the complete sets of text-mined associations between organisms (NCBI TaxIDs) and tissues (BTO terms):

http://download.jensenlab.org/organism_tissue_textmining_full.tsv

With this file at hand, we can count how many of the bacteria linked to colorectal cancer are associated with each tissue in the literature. We have made a Python script that does this and prepared a file with NCBI TaxIDs from the colorectal cancer microbiome study. Download both and run this command:

```python term_enrichment.py organism_tissue_textmining_full.tsv 5 organisms.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The arguments for this command are the file with organism–term associations, the z-score cutoff to be applied to these, and the file of organisms for which to count term associations. The results show that even at this very stringent z-score cutoff, three of the organisms are associated with each of the terms Dental plaque, Mouth, and Saliva.&lt;/p&gt;
&lt;p&gt;You can also count for both a foreground and a background set of organisms and test each tissue term for statistically significant overrepresentation in the foreground set. To do so also download the &lt;a href="https://jensenlab.org/assets/textmining/background.txt"&gt;file with all bacteria identified in the study&lt;/a&gt; for use as background and run:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python term_enrichment.py organism_tissue_textmining_full.tsv 5 organisms.txt background.txt 0.005&lt;/code&gt;
In this command, the additional last two arguments are the file with the background set of organisms and the p-value threshold. The output for each term includes its identifier, its name, the counts for both sets of organisms, and the uncorrected p-value. The results show that oral bacteria indeed appear to be overrepresented among the set of organisms associated with colorectal cancer, although the p-values should obviously must be corrected for multiple testing before claiming significance.&lt;/p&gt;
&lt;p&gt;These types of analyses are by no means limited to tissues. If the task asks for it, equivalent analyses can be done for, e.g., diseases or environmental descriptors.&lt;/p&gt;
&lt;p&gt;Test for a link to oral diseases using the following file of organism–disease associations:
http://download.jensenlab.org/organism_disease_textmining_full.tsv&lt;/p&gt;
&lt;h3&gt;2.4. Mining for indirect associations&lt;/h3&gt;
&lt;p&gt;After Fusobacterium nucleatum, the most overrepresented bacterial species in samples from colorectal cancer patients is Peptostreptococcus stomatis. A search for abstracts linking Peptostreptococcus stomatis to colorectal cancer retrieves a few publications; however, none of these shed any light on the association:&lt;/p&gt;
&lt;p&gt;https://organisms.jensenlab.org/Entity?documents=10&amp;amp;type1=-2&amp;amp;id1=341694&amp;amp;type2=-26&amp;amp;id2=DOID:9256&lt;/p&gt;
&lt;p&gt;Use ARROWSMITH to search for B-terms that connect the A-term Peptostreptococcus stomatis to the C-term colorectal cancer. Look through the list of suggested B-terms.&lt;/p&gt;
&lt;p&gt;Most of the top terms are very general terms related to microbiome sequencing. However, multiple terms related to oral squamous cell carcinomas are also found to be linked both to the species in question and to colorectal cancer. Inspect the underlying literature for this indirect association in ARROWSMITH.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Does this indirect association appear to hold up to closer scrutiny?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Similarly, query ARROWSMITH for terms connecting &lt;strong&gt;Lactobacillus ruminis&lt;/strong&gt; to &lt;strong&gt;colorectal cancer&lt;/strong&gt;. This should suggest inflammation as a possible (if somewhat vague) connection between the two.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Supporting literature&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jensen LJ, Saric S and Bork P (2006). Literature mining for the biologist: from information retrieval to biological discovery. Nature Reviews Genetics, 7:119–129./
&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/16418747"&gt;Abstract&lt;/a&gt; &lt;a href="https://doi.org/10.1038/nrg1768"&gt;Full text&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fleuren WWM and Wynand Alkema W (2015). Application of text mining in the biomedical domain. Methods, 74:97–106./
&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/25641519"&gt;Abstract&lt;/a&gt; &lt;a href="https://doi.org/10.1016/j.ymeth.2015.01.015"&gt;Full text&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brbic M, Piskorec M, Vidulin V, Krisko A, Smuc T and Supek F (2017). The landscape of microbial phenotypic traits and associated genes. Nucleic Acids Research, 44:10074–10090./
&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/27915291"&gt;Abstract&lt;/a&gt; &lt;a href="https://doi.org/10.1093/nar/gkw964"&gt;Full text&lt;/a&gt;&lt;/p&gt;</content><category term="text mining"></category></entry><entry><title>Univariate Analysis</title><link href="https://mohcinemadkour.github.io/posts/2018/01/SchemaSpy:%20Univariate%20Analysis/" rel="alternate"></link><published>2018-01-07T17:46:00-05:00</published><updated>2018-01-07T17:46:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-01-07:/posts/2018/01/SchemaSpy: Univariate Analysis/</id><summary type="html">&lt;h1&gt;Univariate Analysis&lt;/h1&gt;
&lt;h2&gt;Explaining Odds Ratios&lt;/h2&gt;
&lt;h3&gt;What is an odds ratio?&lt;/h3&gt;
&lt;p&gt;An odds ratio (OR) is a measure of association between an exposure and an outcome. The OR represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Univariate Analysis&lt;/h1&gt;
&lt;h2&gt;Explaining Odds Ratios&lt;/h2&gt;
&lt;h3&gt;What is an odds ratio?&lt;/h3&gt;
&lt;p&gt;An odds ratio (OR) is a measure of association between an exposure and an outcome. The OR represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure. Odds ratios are most commonly used in case-control studies, however they can also be used in cross-sectional and cohort study designs as well (with some modifications and/or assumptions).&lt;/p&gt;
&lt;h3&gt;When is it used?&lt;/h3&gt;
&lt;p&gt;Odds ratios are used to compare the relative odds of the occurrence of the outcome of interest (e.g. disease or disorder), given exposure to the variable of interest (e.g. health characteristic, aspect of medical history). The odds ratio can also be used to determine whether a particular exposure is a risk factor for a particular outcome, and to compare the magnitude of various risk factors for that outcome.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;OR=1 Exposure does not affect odds of outcome
OR&amp;gt;1 Exposure associated with higher odds of outcome
OR&amp;lt;1 Exposure associated with lower odds of outcome
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;What about confidence intervals?&lt;/h3&gt;
&lt;p&gt;The 95% confidence interval (CI) is used to estimate the precision of the OR. A large CI indicates a low level of precision of the OR, whereas a small CI indicates a higher precision of the OR. It is important to note however, that unlike the p value, the 95% CI does not report a measure’s statistical significance. In practice, the 95% CI is often used as a proxy for the presence of statistical significance if it does not overlap the null value (e.g. OR=1). Nevertheless, it would be inappropriate to interpret an OR with 95% CI that spans the null value as indicating evidence for lack of association between the exposure and outcome.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Worked Example&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In 1950, the Medical Research Council conducted a case-control study of smoking and lung cancer (Doll and Hill 1950). 649 male cancer patients were included (the cases), 647 of whom were reported to be smokers. 649 men without cancer were also included (controls), 622 of whom were reported to be smokers. The odds ratio of lung cancer for smokers compared with non-smokers can be calculated as (647&lt;em&gt;27)/(2&lt;/em&gt;622) = 14.04, i.e., the odds of lung cancer in smokers is estimated to be 14 times the odds of lung cancer in non-smokers. We would like to know how reliable this estimate is? The 95% confidence interval for this odds ratio is between 3.33 and 59.3. The interval is rather wide because the numbers of non-smokers, particularly for lung cancer cases, are very small. Increasing the confidence level to 99% this interval would increase to between 2.11 and 93.25.&lt;/p&gt;</content><category term="Univariate Analysis"></category></entry><entry><title>The role of unit tests in test automation</title><link href="https://mohcinemadkour.github.io/posts/2018/01/The%20role%20of%20unit%20tests%20in%20test%20automation/" rel="alternate"></link><published>2018-01-03T16:00:00-05:00</published><updated>2018-01-03T16:00:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-01-03:/posts/2018/01/The role of unit tests in test automation/</id><summary type="html">&lt;p&gt;Unit testing is a software development and testing approach in which the smallest testable parts of an application, called units, are individually and independently tested to see if they are operating properly.   Unit testing can be done manually but is usually automated.  Unit testing is a part of the test-driven …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Unit testing is a software development and testing approach in which the smallest testable parts of an application, called units, are individually and independently tested to see if they are operating properly.   Unit testing can be done manually but is usually automated.  Unit testing is a part of the test-driven development (TDD) methodology that requires developers to first write failing unit tests.  Then they write code in order to change the application until the test passes. Writing the failing test is important because it forces the developer to take into account all possible inputs, errors and outputs. &lt;/p&gt;
&lt;p&gt;The result of using TDD is that an agile team can accumulate a comprehensive suite of unit tests that can be run at any time to provide feedback that their software is still working.  If the new code breaks something and causes a test to fail,  TDD also makes it easier to pinpoint the problem, refactor the application and fix the bug.&lt;/p&gt;
&lt;p&gt;The AAA pattern&lt;/p&gt;
&lt;p&gt;The goal of unit testing is to isolate each part of a program and show that the individual parts work correctly.  This is in line with the YAGNI ("You ain't gonna need it") principle at the heart of the agile development practice of doing the simplest thing that can possibly work.  Using the YAGNI principle to build units of software, together with other practices such as continuous refactoring and continuous integration, make it easier to automate groups or suites of unit tests.  Unit test automation is a key component of a Continuous Delivery DevTestOps solution, that is, a continuously tested, two-way DevOps software delivery pipeline between an organization and its customers.&lt;/p&gt;
&lt;p&gt;Unit tests are designed for code that has no external dependencies, such as calls to the database or web services.  Because they focus on a specific behavior in a small section of a system under test (SUT), they're also relatively straight-forward to automate, especially if they are written in a standard format such as the AAA pattern.&lt;/p&gt;
&lt;p&gt;The AAA unit test pattern&lt;/p&gt;
&lt;p&gt;Image Source: Code Project
![Image Source: Code Project](images/AAA pattern.)&lt;/p&gt;
&lt;p&gt;The AAA (Arrange, Act, Assert ) pattern helps organize and clarify test code by breaking down a test case into the following functional sections:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;The Arrange section of a unit test initializes objects and sets the value of the data that is passed to the test case.
The Act section invokes the test case with the arranged parameters.
The Assert section verifies the test case behaves as expected.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Consider the following example test scenario:&lt;/p&gt;
&lt;p&gt;A unit test in AAA format tests a software unit that increments the number of products in an e-commerce shopping cart:&lt;/p&gt;
&lt;p&gt;Arrange&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Create a empty shopping cart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Act&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Add a product to the cart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Assert &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Number of products in cart increased by one
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's more about  the AAA pattern:&lt;/p&gt;
&lt;p&gt;Arrange Section&lt;/p&gt;
&lt;p&gt;In order to put yourself in a position where you can call a software unit and check that the result was correct, you first need to "prime the pump," or put the unit into a known beginning state.  When setting up the module to be tested, it may be necessary sometimes to surround that module with other collaborator modules.  For testing purposes, those collaborators could be test modules with actual or made-up data (also known mock objects, fakes, etc.). &lt;/p&gt;
&lt;p&gt;Mock objects are simulated objects created by a developer that mimic the behavior of real objects in controlled ways, similar to how crash test dummies are expected to simulate the dynamic behavior of humans in vehicle impacts.  A mock object, in the case of a database or e-commerce application, might be created as part of a unit test with a variety of fake data because real customer records may not exist yet or it would slow down testing if a complete customer database had to be accessed or initialized before running the test.&lt;/p&gt;
&lt;p&gt;Test-specific mock objects can used to verify application behavior&lt;/p&gt;
&lt;p&gt;Image source:  hackerchick
&lt;img alt="Image source:  hackerchick" src="images/mockobjects.png"&gt;&lt;/p&gt;
&lt;p&gt;Mock objects are used for much more than creating made-up test data in unit testing.  For example, using mock objects in place of real objects can make it easier to test a complex algorithm based on multiple objects being in particular states.  The use of mock objects is extensive in the literature on automated testing using xUnit testing frameworks. You can find an example here. &lt;/p&gt;
&lt;p&gt;In the shopping cart example, the Arrange part of the pattern involves creating a empty shopping cart  by initially setting the number of products in the cart to zero.  As we'll see later, there's a way to adapt the AAA syntax to handle more complicated scenarios.&lt;/p&gt;
&lt;p&gt;Act Section:&lt;/p&gt;
&lt;p&gt;This is the part of the test that exercises the unit of code under test by making a function or method call that returns a result or causes a reaction that can be observed.&lt;/p&gt;
&lt;p&gt;In the shopping cart example, the Act section takes place when Buy Item button on the shopping cart is pushed. &lt;/p&gt;
&lt;p&gt;Assert Section:&lt;/p&gt;
&lt;p&gt;The assertion section were you check to see that you have a result or reaction (include calls to other units of code) that matches your expectations. &lt;/p&gt;
&lt;p&gt;In the shopping cart example, the Assert section occurs when the number property is checked against your expectation (i.e. the number of products in the cart is increased by one every time the Buy Item button is pushed.)&lt;/p&gt;
&lt;p&gt;Following the AAA pattern consistently makes test code easier to read by clearly separating what is being tested from the setup and verification steps.  This helps when you need to reexamine sections of test code to see if they're still doing what it should be doing, such as following a previous set of successful test steps.&lt;/p&gt;
&lt;p&gt;In the bottom-up testing style of test-driven development, unit tests written in the AAA syntax will help you know exactly where to search to find a bug when a unit test fails.  Kent Beck, who popularized the TDD concept in his book Test Driven Development: By Example, states that TDD has two basic rules:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Never write a single line of code unless you have a failing automated test.
Eliminate duplication.  In software engineering, don&amp;#39;t repeat yourself (DRY) is a principle of agile software development, aimed at reducing repetition of information of all kinds, which is especially useful in multi-tier architectures
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;According to Beck, a good unit test in TDD should be able to do all of the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Run fast (they have short setups, run times, and break downs).
Run in isolation (you should be able to reorder them).
Use data that makes them easy to read and to understand.
Use real data (or copies of production data) when they need to.
Represent one step towards your overall goal.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This means unit tests need to be narrowly focused and shouldn't try to test too many different things at once.  An example of a unit test that tries to do too many things is shown in a test scenario involving a sweater purchase in an e-commerce shopping cart application.  The Arrange section in this example assumes that the pipe has been primed and you have sweaters in your inventory database, that another unit is able to show the inventory to your customer and still other units are be able to process the customer payment and remove items from inventory.&lt;/p&gt;
&lt;p&gt;Here's some pseudo code for a unit test in the AAA format for this kind of functionality:&lt;/p&gt;
&lt;p&gt;Arrange&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;setup Sweater Inventory (mostly likely with mock database objects)
set Sweater Inventory Count to 5      
when Sweater Inventory is requested to remove N items, then count = count - N
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Act&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;call the Unit Under Test to remove 3 items from inventory
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Assert&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;the number of sweaters in the Sweater Inventory is 2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As you can see, writing large unit tests in this way can quickly become complex and convoluted,  especially when you need to test end-to-end functionality for a complete 6-step e-commerce shopping-cart application, i.e.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Access Homepage –&amp;gt; 
Customer Search results –&amp;gt;
Product details  –&amp;gt;
Customer login (or Register New customer) –&amp;gt;
Payment details –&amp;gt;
Order confirmation
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A simpler and better to way to use the AAA unit test scaffolding, both for unit and higher-level tests, is by using Behavior-Driven Development (BDD), which BDD pioneer Dan North defines this way:&lt;/p&gt;
&lt;p&gt;"BDD is a second-generation, outside–in, pull-based, multiple-stakeholder, multiple-scale, high-automation, agile methodology. It describes a cycle of interactions with well-defined outputs, resulting in the delivery of working, tested software that matters."
![andolasoft](images/bdd n tdd.jpg)
BDD is an enhancement of TDD&lt;/p&gt;
&lt;p&gt;Image Source:  andolasoft&lt;/p&gt;
&lt;p&gt;The main advantage of BDD is that it encourages collaboration between developers, QA and non-technical or business participants on a software project.  It extends TDD by writing test cases in a natural language that non-programmers and domain experts can read. BDD features are usually defined in a GIVEN WHEN and THEN (GWT) format, which is a semi-structured way of writing down test cases.  A BDD feature  or user story needs to follow the following structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Describe who is the primary stakeholder of the feature
What effect the stakeholder wants the feature to have
What business value the stakeholder will derive from this effect
Acceptance criteria or scenarios
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A brief example of a BDD feature in this format looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Feature&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;   &lt;span class="n"&gt;Items&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;abandoned&lt;/span&gt; &lt;span class="n"&gt;shopping&lt;/span&gt; &lt;span class="n"&gt;carts&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="n"&gt;be&lt;/span&gt; &lt;span class="n"&gt;returned&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;inventory&lt;/span&gt;
&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="n"&gt;order&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt; &lt;span class="n"&gt;track&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;inventory&lt;/span&gt;
&lt;span class="n"&gt;As&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="n"&gt;store&lt;/span&gt; &lt;span class="n"&gt;owner&lt;/span&gt;
&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="n"&gt;want&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt; &lt;span class="n"&gt;items&lt;/span&gt; &lt;span class="n"&gt;back&lt;/span&gt; &lt;span class="n"&gt;into&lt;/span&gt; &lt;span class="n"&gt;inventory&lt;/span&gt; &lt;span class="n"&gt;when&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="n"&gt;shopping&lt;/span&gt; &lt;span class="n"&gt;cart&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;abandoned&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Scenario 1: On-line shopping cart items not purchased within 30 minutes go back into inventory&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Given that a customer puts a black sweater into his shopping cart
And I have three black sweaters in inventory.
When he does not complete the purchase with 30 minutes (i.e. abandons the shopping cart)
Then I should have four black sweaters in inventory.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In TDD, the developers write the tests while in BDD the automated specifications are created by users or testers (with developers writing the underlying code that implements the test.) 
![Test Automation Pyramid](/images/automation pyramid.png)
Test Automation Pyramid&lt;/p&gt;
&lt;p&gt;Image Source:  Effective Testing Practices in an Agile Environment&lt;/p&gt;
&lt;p&gt;Outside-in vs. Inside-out Testing&lt;/p&gt;
&lt;p&gt;Agile teams generally follow one of two approaches when it comes to testing their applications, either outside-In  or inside-out.  In the outside-in approach, teams start by focusing on the end user's perspective and attempt to describe high-level desired functionality and goals for the software under test in the form of user stories.  In every iteration or Sprint, user stories are refined until the agile team and the Product Owner/Customer Representative can agree on the acceptance criteria, which determine that a User Story works as planned.  Testing then goes 'inward' and code is written to test smaller and small components until you reach the unit-test level.&lt;/p&gt;
&lt;p&gt;In the inside-out or bottom-up approach, agile teams start with unit tests at the lowest level of the Test Automation Pyramid (see Figure 5 above).  As the code evolves due to refactoring, testing efforts evolve as well as the team moves upward to acceptance level testing, which tests business logic at the API or service level.  The top of the pyramid and the last thing tested is the user interface (UI).&lt;/p&gt;
&lt;p&gt;Inside-out and outside-in are different but complementary approaches to testing.  Software quality control relies on the related notions of verification and validation (V&amp;amp;V) that check to see that a software system meets specifications and that it fulfills its intended purpose.  The terms verification and validation are often used interchangeably but have different meanings: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Verification&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Checks&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;software&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;respect&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;specifications&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Is our team building the code right?&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Validation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Checks&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;software&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;respect&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;customer&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;expectations&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Are we building the right code?&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On modern agile DevOps projects, validation and verification steps overlap and take place continuously since agile team members must engage with customers and other stakeholders throughout the project-- to do things like prioritizing bug fixes and enhancements on the team's project backlog --  and not just after a separate test phase at the end of the project.  This requires effective communication at all levels of the business since team members need to be understand what features need to be built and who needs each feature.&lt;/p&gt;
&lt;p&gt;BDD and TDD use syntax to describe three test states that are roughly equivalent:&lt;/p&gt;
&lt;p&gt;Given = Arrange 
When = Act,
Then   = Assert&lt;/p&gt;
&lt;p&gt;In  TDD, "inside-out" development starts with the innermost components and proceeds towards the user interface building on the previously constructed components.  BDD makes it easier to for agile teams to design outside-in but then code inside-out.  Because of the increased communication and collaboration among developers, QA and non-technical or business participants on a software project that the BDD/GWT syntax promotes,  developers and testers are able to anticipate how to test the outer software when writing tests for the inner software.  This means that tests at the Acceptance and GUI level can better take advantage of already-built tests, reducing the need for the Mock Objects mentioned earlier. &lt;/p&gt;
&lt;p&gt;Using TDD and the AAA syntax encourages developers to write small, independent tests and to continually refactor their code.  BDD supports TDD by helping bridge the gap between unit tests and higher-level acceptance and integration tests.  BDD and the GWT syntax are useful in business environments, where work done by developers needs to be mapped to business value. Because the two approaches complement each other so well, you should combine them with test management tools to get the best results on your DevOps test automation projects.&lt;/p&gt;</content><category term="Unit Test"></category><category term="Software"></category></entry><entry><title>Cassandra Security</title><link href="https://mohcinemadkour.github.io/posts/2018/01/Cassandra%20Security/" rel="alternate"></link><published>2018-01-02T11:46:00-05:00</published><updated>2018-01-02T11:46:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-01-02:/posts/2018/01/Cassandra Security/</id><summary type="html">&lt;h1&gt;Securing Cassandra&lt;/h1&gt;
&lt;p&gt;Security is always at odds with usability, particularly in the context of operations and development. More so when dealing with a distributed system such as Apache Cassandra. This presentation will cover the steps required to completely secure a Cassandra cluster to meet most regulatory and compliance guidelines. Specific …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Securing Cassandra&lt;/h1&gt;
&lt;p&gt;Security is always at odds with usability, particularly in the context of operations and development. More so when dealing with a distributed system such as Apache Cassandra. This presentation will cover the steps required to completely secure a Cassandra cluster to meet most regulatory and compliance guidelines. Specific topics include:
- considerations and recommendations for encrypting Cassandra data at rest 
- how and why to secure client to server and server to server communications
- authentication and access control best practices
- limiting access to management and tooling
- what to expect when enabling security features in production&lt;/p&gt;
&lt;h2&gt;SSL Ecyption&lt;/h2&gt;
&lt;p&gt;Cassandra provides secure communication between a client and a database cluster, and between nodes in a cluster. Enabling SSL encryption ensures that data in flight is not compromised and is transferred securely. Client-to-node and node-to-node encryption are independently configured. Cassandra tools (cqlsh, nodetool, DevCenter) can be configured to use SSL encryption. The DataStax drivers can be configured to secure traffic between the driver and Cassandra.&lt;/p&gt;
&lt;h3&gt;Encrypting Cassandra with SSL&lt;/h3&gt;
&lt;p&gt;Briefly, SSL works in the following manner. Two entities, either software or hardware, that are communicating with one another. The entities an be a client and node or peers in a cluster. These entities must exchange information to set up trust between them. Each entity that will provide such information must have a generated key that consists of a private key that only the entity stores and a public key that can be exchanged with other entities. If the client wants to connect to the server, the client requests the secure connection and the server sends a certificate that includes its public key. The client checks the validity of the certificate by exchanging information with the server, which the server validates with its private key. If a two-way validation is desired, this process must be carried out in both directions. Private keys and certificates are stored in the keystore and public keys are stored in the truststore. For systems using a Certificate Authority (CA), the truststore can store certificates signed by the CA for verification. Both keystores and truststores have passwords assigned, referred to as the keypass and storepass.&lt;/p&gt;
&lt;p&gt;Apache Cassandra provides these SSL encryption features for . &lt;/p&gt;
&lt;h4&gt;Client-to-node encrypted communication&lt;/h4&gt;
&lt;p&gt;Client-to-node encryption protects data in flight from client machines to a database cluster using SSL (Secure Sockets Layer). It establishes a secure channel between the client and the coordinator node.&lt;/p&gt;
&lt;p&gt;https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/secureSSLIntro.html&lt;/p&gt;</content><category term="Spark Processing"></category><category term="Spark Streaming"></category></entry><entry><title>Shibboleth</title><link href="https://mohcinemadkour.github.io/posts/2018/01/Shibboleth2/" rel="alternate"></link><published>2018-01-02T11:46:00-05:00</published><updated>2018-01-02T11:46:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-01-02:/posts/2018/01/Shibboleth2/</id><summary type="html">&lt;p&gt;Shibboleth is among the world’s most widely deployed federated identity solutions, connecting users to applications both within and between organizations. Every software component of the Shibboleth system is free and open source.&lt;/p&gt;
&lt;p&gt;Shibboleth is an open-source project that provides Single Sign-On capabilities and allows sites to make informed authorization …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Shibboleth is among the world’s most widely deployed federated identity solutions, connecting users to applications both within and between organizations. Every software component of the Shibboleth system is free and open source.&lt;/p&gt;
&lt;p&gt;Shibboleth is an open-source project that provides Single Sign-On capabilities and allows sites to make informed authorization decisions for individual access of protected online resources in a privacy-preserving manner.&lt;/p&gt;
&lt;p&gt;Shibboleth is a 'single-sign in', or logging-in system for computer networks and the Internet. It allows people to sign in, using just one 'identity', to various systems run by 'federations' of different organizations or institutions. The federations are often universities or public service organizations. The Shibboleth Internet2 middleware initiative created an architecture and open-source implementation for identity management and federated identity-based authentication and authorization (or access control) infrastructure based on Security Assertion Markup Language (SAML). Federated identity allows the sharing of information about users from one security domain to the other organizations in a federation. This allows for cross-domain single sign-on and removes the need for content providers to maintain user names and passwords. Identity providers (IdPs) supply user information, while service providers (SPs) consume this information and give access to secure content. &lt;/p&gt;
&lt;h1&gt;How Shibboleth Works: Basic Concepts&lt;/h1&gt;
&lt;p&gt;At its core Shibboleth works the same as every other web-based Single Sign-on (SSO) system. What distinguishes Shibboleth from other products in this field is its adherence to standards and its ability to provide SSO support to services outside of a user’s organization while still protecting their privacy.&lt;/p&gt;
&lt;h1&gt;The main elements of a web-based SSO system are:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Web Browser – represents the user within the SSO process&lt;/li&gt;
&lt;li&gt;Resource – contains restricted access content that the user wants&lt;/li&gt;
&lt;li&gt;Identity Provider (IdP) – authenticates the user&lt;/li&gt;
&lt;li&gt;Service Provider (SP) – performs the SSO process for the resource&lt;/li&gt;
&lt;/ul&gt;</content><category term="Idealist"></category></entry><entry><title>SPARK AND HDFS storage : building application from Python</title><link href="https://mohcinemadkour.github.io/posts/2017/12/Spark%20HDFS%20Guide/" rel="alternate"></link><published>2017-12-29T11:50:00-05:00</published><updated>2017-12-29T11:50:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2017-12-29:/posts/2017/12/Spark HDFS Guide/</id><summary type="html">&lt;h1&gt;Install HDFS&lt;/h1&gt;
&lt;p&gt;PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.&lt;/p&gt;
&lt;p&gt;readlink -f $(which java) to check the path to java install
I ended up adding …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Install HDFS&lt;/h1&gt;
&lt;p&gt;PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.&lt;/p&gt;
&lt;p&gt;readlink -f $(which java) to check the path to java install
I ended up adding the following in ~/.bashrc:&lt;/p&gt;
&lt;p&gt;export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64&lt;/p&gt;
&lt;p&gt;export HADOOP_INSTALL=/home/mohcine/Sofwares/hadoop-3.0.0&lt;/p&gt;
&lt;p&gt;export PATH=$PATH:$HADOOP_INSTALL/bin&lt;/p&gt;
&lt;p&gt;PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.&lt;/p&gt;
&lt;h1&gt;Getting started&lt;/h1&gt;
&lt;p&gt;These examples give a quick overview of the Spark API. Spark is built on the concept of distributed datasets, which contain arbitrary Java or Python objects. You create a dataset from external data, then apply parallel operations to it. The building block of the Spark API is its RDD API. In the RDD API, there are two types of operations: transformations, which define a new dataset based on previous ones, and actions, which kick off a job to execute on a cluster. On top of Spark’s RDD API, high level APIs are provided, e.g. DataFrame API and Machine Learning API. These high level APIs provide a concise way to conduct certain data operations. In this page, we will show examples using RDD API as well as examples using high level APIs.&lt;/p&gt;</content><category term="HDFS"></category><category term="Spark"></category></entry><entry><title>Machines, cores, executors, tasks, and receivers in Spark</title><link href="https://mohcinemadkour.github.io/posts/2017/12/Machines,%20cores,%20executors,%20tasks,%20and%20receivers%20in%20Spark/" rel="alternate"></link><published>2017-12-26T14:46:00-05:00</published><updated>2017-12-26T14:46:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2017-12-26:/posts/2017/12/Machines, cores, executors, tasks, and receivers in Spark/</id><summary type="html">&lt;h1&gt;Machines, cores, executors, tasks, and receivers in Spark&lt;/h1&gt;
&lt;p&gt;The subsequent sections of this article talk a lot about parallelism in Spark and in Kafka. You need at least a basic understanding of some Spark terminology to be able to follow the discussion in those sections.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Spark cluster contains 1 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1&gt;Machines, cores, executors, tasks, and receivers in Spark&lt;/h1&gt;
&lt;p&gt;The subsequent sections of this article talk a lot about parallelism in Spark and in Kafka. You need at least a basic understanding of some Spark terminology to be able to follow the discussion in those sections.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Spark cluster contains 1+ worker nodes aka slave machines (simplified view; I exclude pieces like cluster managers here.)&lt;/li&gt;
&lt;li&gt;A worker node can run 1+ executors.&lt;/li&gt;
&lt;li&gt;An executor is a process launched for an application on a worker node, which runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. An executor has a certain amount of cores aka “slots” available to run tasks assigned to it.&lt;/li&gt;
&lt;li&gt;A task is a unit of work that will be sent to one executor. That is, it runs (part of) the actual computation of your application. The SparkContext sends those tasks for the executors to run. Each task occupies one slot aka core in the parent executor.&lt;/li&gt;
&lt;li&gt;A receiver (API, docs) is run within an executor as a long-running task. Each receiver is responsible for exactly one so-called input DStream (e.g. an input stream for reading from Kafka), and each receiver – and thus input DStream – occupies one core/slot.&lt;/li&gt;
&lt;li&gt;An input DStream: an input DStream is a special DStream that connects Spark Streaming to external data sources for reading input data. For each external data source (e.g. Kafka) you need one such input DStream implementation. Once Spark Streaming is “connected” to an external data source via such input DStreams, any subsequent DStream transformations will create “normal” DStreams.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Spark’s execution model, each application gets its own executors, which stay up for the duration of the whole application and run 1+ tasks in multiple threads. This isolation approach is similar to Storm’s model of execution. This architecture becomes more complicated once you introduce cluster managers like YARN or Mesos, which I do not cover here. See Cluster Overview in the Spark docs for further details.&lt;/p&gt;
&lt;h1&gt;Primer on topics, partitions, and parallelism in Kafka&lt;/h1&gt;
&lt;p&gt;Kafka stores data in topics, with each topic consisting of a configurable number of partitions. The number of partitions of a topic is very important for performance considerations as this number is an upper bound on the consumer parallelism: if a topic has N partitions, then your application can only consume this topic with a maximum of N threads in parallel. (At least this is the case when you use Kafka’s built-in Scala/Java consumer API.)&lt;/p&gt;
&lt;p&gt;When I say “application” I should rather say consumer group in Kafka’s terminology. A consumer group, identified by a string of your choosing, is the cluster-wide identifier for a logical consumer application. All consumers that are part of the same consumer group share the burden of reading from a given Kafka topic, and only a maximum of N (= number of partitions) threads across all the consumers in the same group will be able to read from the topic. Any excess threads will sit idle.&lt;/p&gt;
&lt;p&gt;Multiple Kafka consumer groups can be run in parallel: Of course you can run multiple, independent logical consumer applications against the same Kafka topic. Here, each logical application will run its consumer threads under a unique consumer group id. Each application can then also use different read parallelisms (see below). When I am talking about the various ways to configure read parallelism in the following sections, then I am referring to the settings of a single one of these logical consumer applications. &lt;/p&gt;
&lt;p&gt;Here are some simplified examples.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your application uses the consumer group id “terran” to read from a Kafka topic “zerg.hydra” that has 10 partitions. If you configure your application to consume the topic with only 1 thread, then this single thread will read data from all 10 partitions.&lt;/li&gt;
&lt;li&gt;Same as above, but this time you configure 5 consumer threads. Here, each thread will read from 2 partitions.&lt;/li&gt;
&lt;li&gt;Same as above, but this time you configure 10 consumer threads. Here, each thread will read from a single partition.&lt;/li&gt;
&lt;li&gt;Same as above, but this time you configure 14 consumer threads. Here, 10 of the 14 threads will read from a single partition each, and the remaining 4 threads will be idle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s introduce some real-world complexity in this simple picture – the rebalancing event in Kafka. Rebalancing is a lifecycle event in Kafka that occurs when consumers join or leave a consumer group (there are more conditions that trigger rebalancing but these are not important in this context; see my Kafka training deck for details on rebalancing).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your application uses the consumer group id “terran” and starts consuming with 1 thread. This thread will read from all 10 partitions. During runtime, you’ll increase the number of threads from 1 to 14. That is, there is suddenly a change of parallelism for the same consumer group. This triggers rebalancing in Kafka. Once rebalancing completes, you will have 10 of 14 threads consuming from a single partition each, and the 4 remaining threads will be idle. And as you might have guessed, the initial thread will now read from only one partition and will no longer see data from the other nine.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have now a basic understanding of topics, partitions, and the number of partitions as an upper bound for the parallelism when reading from Kafka. But what are the resulting implications for an application – such as a Spark Streaming job or Storm topology – that reads its input data from Kafka?&lt;/p&gt;
&lt;p&gt;1- Read parallelism: You typically want to read from all N partitions of a Kafka topic in parallel by consuming with N threads. And depending on the data volume you want to spread those threads across different NICs, which typically means across different machines. In Storm, this is achieved by setting the parallelism of the Kafka spout to N via TopologyBuilder#setSpout(). The Spark equivalent is a bit trickier, and I will describe how to do this in further detail below.
2- Downstream processing parallelism: Once retrieved from Kafka you want to process the data in parallel. Depending on your use case this level of parallelism must be different from the read parallelism. If your use case is CPU-bound, for instance, you want to have many more processing threads than read threads; this is achieved by shuffling or “fanning out” the data via the network from the few read threads to the many processing threads. Hence you pay for the access to more cores with increased network communication, serialization overhead, etc. In Storm, you perform such a shuffling via a shuffle grouping from the Kafka spout to the next downstream bolt. The Spark equivalent is the repartition transformation on DStreams.&lt;/p&gt;
&lt;p&gt;The important takeaway is that it is possible – and often desired – to decouple the level of parallelisms for reading from Kafka and for processing the data once read. In the next sections I will describe the various options you have at your disposal to configure read parallelism and downstream processing parallelism in Spark Streaming.&lt;/p&gt;
&lt;p&gt;** Extracted from &lt;a href="http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/"&gt;Michael G. Noll Blog&lt;/a&gt;&lt;/p&gt;</content><category term="kafka"></category><category term="Spark"></category></entry><entry><title>Spark Streaming and Kafka Integration Guide for python</title><link href="https://mohcinemadkour.github.io/posts/2017/12/Spark%20Streaming%20+%20Kafka%20Integration%20Guide/" rel="alternate"></link><published>2017-12-26T11:50:00-05:00</published><updated>2017-12-26T11:50:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2017-12-26:/posts/2017/12/Spark Streaming + Kafka Integration Guide/</id><summary type="html">&lt;h1&gt;Spark Streaming + Kafka Integration Guide for python&lt;/h1&gt;
&lt;p&gt;In this post I shed some light on the current state of Kafka integration in Spark Streaming AND how to configure Spark Streaming to receive data from Kafka. All this with the disclaimer that this happens to be my first experiment with Spark …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Spark Streaming + Kafka Integration Guide for python&lt;/h1&gt;
&lt;p&gt;In this post I shed some light on the current state of Kafka integration in Spark Streaming AND how to configure Spark Streaming to receive data from Kafka. All this with the disclaimer that this happens to be my first experiment with Spark Streaming.&lt;/p&gt;
&lt;p&gt;First of all, Spark Streaming is a sub-project of Apache Spark. Spark is a batch processing platform similar to Apache Hadoop, and Spark Streaming is a real-time processing tool that runs on top of the Spark engine.&lt;/p&gt;
&lt;p&gt;There are two approaches to this - the old approach using Receivers and Kafka’s high-level API, and a new approach (introduced in Spark 1.3) without using Receivers. They have different programming models, performance characteristics, and semantics guarantees. Both approaches are considered stable APIs as of the current version of Spark (2.11-1.0.0).&lt;/p&gt;
&lt;h2&gt;Approach 1: Receiver-based Approach&lt;/h2&gt;
&lt;p&gt;This approach uses a Receiver to receive the data. The Receiver is implemented using the Kafka high-level consumer API. As with all receivers, the data received from Kafka through a Receiver is stored in Spark executors, and then jobs launched by Spark Streaming processes the data.&lt;/p&gt;
&lt;p&gt;However, under default configuration, this approach can lose data under failures (see receiver reliability. To ensure zero-data loss, you have to additionally enable Write Ahead Logs in Spark Streaming (introduced in Spark 1.2). This synchronously saves all the received Kafka data into write ahead logs on a distributed file system (e.g HDFS), so that all the data can be recovered on failure. See Deploying section in the streaming programming guide for more details on Write Ahead Logs.&lt;/p&gt;
&lt;p&gt;To use this approach in your streaming application,  First, In the streaming application code, import KafkaUtils and create an input DStream as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming.kafka&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;

&lt;span class="n"&gt;kafkaStream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;streamingContext&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ZK&lt;/span&gt; &lt;span class="n"&gt;quorum&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;consumer&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt; &lt;span class="nb"&gt;id&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;per&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt; &lt;span class="n"&gt;number&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;Kafka&lt;/span&gt; &lt;span class="n"&gt;partitions&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;consume&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;By default, the Python API will decode Kafka data as UTF8 encoded strings. You can specify your custom decoding function to decode the byte arrays in Kafka records to any arbitrary data type as in this example&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt; Counts words in UTF8 encoded, &amp;#39;\n&amp;#39; delimited text received from the network every second.&lt;/span&gt;
&lt;span class="sd"&gt; Usage: kafka_wordcount.py &amp;lt;zk&amp;gt; &amp;lt;topic&amp;gt;&lt;/span&gt;
&lt;span class="sd"&gt; To run this on your local machine, you need to setup Kafka and create a producer first, see&lt;/span&gt;
&lt;span class="sd"&gt; http://kafka.apache.org/documentation.html#quickstart&lt;/span&gt;
&lt;span class="sd"&gt; and then run the example&lt;/span&gt;
&lt;span class="sd"&gt;    `$ bin/spark-submit --jars \&lt;/span&gt;
&lt;span class="sd"&gt;      external/kafka-assembly/target/scala-*/spark-streaming-kafka-assembly-*.jar \&lt;/span&gt;
&lt;span class="sd"&gt;      examples/src/main/python/streaming/kafka_wordcount.py \&lt;/span&gt;
&lt;span class="sd"&gt;      localhost:2181 test`&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming.kafka&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Usage: kafka_wordcount.py &amp;lt;zk&amp;gt; &amp;lt;topic&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;appName&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;PythonStreamingKafkaWordCount&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ssc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;zkQuorum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;topic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="n"&gt;kvs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zkQuorum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;spark-streaming-consumer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="n"&gt;lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kvs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatMap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; \
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; \
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduceByKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;awaitTermination&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Deploying :  Run on terminal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 /home/mohcine/Spark_Streaming_kafka.py localhost:2182
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Approach 2: Direct Approach (No Receivers)&lt;/h2&gt;
&lt;p&gt;This new receiver-less “direct” approach has been introduced in Spark 1.3 to ensure stronger end-to-end guarantees. Instead of using receivers to receive data, this approach periodically queries Kafka for the latest offsets in each topic+partition, and accordingly defines the offset ranges to process in each batch. When the jobs to process the data are launched, Kafka’s simple consumer API is used to read the defined ranges of offsets from Kafka (similar to read files from a file system). Note that this feature was introduced in Spark 1.3 for the Scala and Java API, in Spark 1.4 for the Python API.&lt;/p&gt;
&lt;p&gt;This approach has the following advantages over the receiver-based approach (i.e. Approach 1).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Simplified Parallelism: No need to create multiple input Kafka streams and union them. With directStream, Spark Streaming will create as many RDD partitions as there are Kafka partitions to consume, which will all read data from Kafka in parallel. So there is a one-to-one mapping between Kafka and RDD partitions, which is easier to understand and tune.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Efficiency: Achieving zero-data loss in the first approach required the data to be stored in a Write Ahead Log, which further replicated the data. This is actually inefficient as the data effectively gets replicated twice - once by Kafka, and a second time by the Write Ahead Log. This second approach eliminates the problem as there is no receiver, and hence no need for Write Ahead Logs. As long as you have sufficient Kafka retention, messages can be recovered from Kafka.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exactly-once semantics: The first approach uses Kafka’s high level API to store consumed offsets in Zookeeper. This is traditionally the way to consume data from Kafka. While this approach (in combination with write ahead logs) can ensure zero data loss (i.e. at-least once semantics), there is a small chance some records may get consumed twice under some failures. This occurs because of inconsistencies between data reliably received by Spark Streaming and offsets tracked by Zookeeper. Hence, in this second approach, we use simple Kafka API that does not use Zookeeper. Offsets are tracked by Spark Streaming within its checkpoints. This eliminates inconsistencies between Spark Streaming and Zookeeper/Kafka, and so each record is received by Spark Streaming effectively exactly once despite failures. In order to achieve exactly-once semantics for output of your results, your output operation that saves the data to an external data store must be either idempotent, or an atomic transaction that saves results and offsets (see Semantics of output operations in the main programming guide for further information).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that one disadvantage of this approach is that it does not update offsets in Zookeeper, hence Zookeeper-based Kafka monitoring tools will not show progress. However, you can access the offsets processed by this approach in each batch and update Zookeeper yourself (see below).&lt;/p&gt;
&lt;p&gt;Next, we discuss how to use this approach in your streaming application.
In the streaming application code, import KafkaUtils and create an input DStream as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming.kafka&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;
&lt;span class="n"&gt;directKafkaStream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDirectStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;metadata.broker.list&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;brokers&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can also pass a messageHandler to createDirectStream to access KafkaMessageAndMetadata that contains metadata about the current message and transform it to any desired type. By default, the Python API will decode Kafka data as UTF8 encoded strings. You can specify your custom decoding function to decode the byte arrays in Kafka records to any arbitrary data type. See the following example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt; Counts words in UTF8 encoded, &amp;#39;\n&amp;#39; delimited text directly received from Kafka in every 2 seconds.&lt;/span&gt;
&lt;span class="sd"&gt; Usage: direct_kafka_wordcount.py &amp;lt;broker_list&amp;gt; &amp;lt;topic&amp;gt;&lt;/span&gt;
&lt;span class="sd"&gt; To run this on your local machine, you need to setup Kafka and create a producer first, see&lt;/span&gt;
&lt;span class="sd"&gt; http://kafka.apache.org/documentation.html#quickstart&lt;/span&gt;
&lt;span class="sd"&gt; and then run the example&lt;/span&gt;
&lt;span class="sd"&gt;    `$ bin/spark-submit --jars \&lt;/span&gt;
&lt;span class="sd"&gt;      external/kafka-assembly/target/scala-*/spark-streaming-kafka-assembly-*.jar \&lt;/span&gt;
&lt;span class="sd"&gt;      examples/src/main/python/streaming/direct_kafka_wordcount.py \&lt;/span&gt;
&lt;span class="sd"&gt;      localhost:9092 test`&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming.kafka&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Usage: direct_kafka_wordcount.py &amp;lt;broker_list&amp;gt; &amp;lt;topic&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;appName&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;PythonStreamingDirectKafkaWordCount&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ssc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;brokers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;topic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="n"&gt;kvs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDirectStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;topic&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;metadata.broker.list&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;brokers&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="n"&gt;lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kvs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatMap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; \
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; \
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduceByKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;awaitTermination&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the Kafka parameters, you must specify either metadata.broker.list or bootstrap.servers. By default, it will start consuming from the latest offset of each Kafka partition. If you set configuration auto.offset.reset in Kafka parameters to smallest, then it will start consuming from the smallest offset.&lt;/p&gt;
&lt;p&gt;You can also start consuming from any arbitrary offset using other variations of KafkaUtils.createDirectStream. Furthermore, if you want to access the Kafka offsets consumed in each batch, you can do the following.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;offsetRanges = []
def storeOffsetRanges(rdd):
global offsetRanges
offsetRanges = rdd.offsetRanges()
return rdd
def printOffsetRanges(rdd):
for o in offsetRanges:
     print &amp;quot;%s %s %s %s&amp;quot; % (o.topic, o.partition, o.fromOffset, o.untilOffset)
directKafkaStream \
 .transform(storeOffsetRanges) \
 .foreachRDD(printOffsetRanges)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can use this to update Zookeeper yourself if you want Zookeeper-based Kafka monitoring tools to show progress of the streaming application.&lt;/p&gt;
&lt;p&gt;Note that the typecast to HasOffsetRanges will only succeed if it is done in the first method called on the directKafkaStream, not later down a chain of methods. You can use transform() instead of foreachRDD() as your first method call in order to access offsets, then call further Spark methods. However, be aware that the one-to-one mapping between RDD partition and Kafka partition does not remain after any methods that shuffle or repartition, e.g. reduceByKey() or window().&lt;/p&gt;
&lt;p&gt;Another thing to note is that since this approach does not use Receivers, the standard receiver-related (that is, configurations of the form spark.streaming.receiver.&lt;em&gt; ) will not apply to the input DStreams created by this approach (will apply to other input DStreams though). Instead, use the configurations spark.streaming.kafka.&lt;/em&gt;. An important one is spark.streaming.kafka.maxRatePerPartition which is the maximum rate (in messages per second) at which each Kafka partition will be read by this direct API.&lt;/p&gt;</content><category term="kafka"></category><category term="Spark"></category></entry><entry><title>Getting Started with Spark Streaming with Python and Kafka</title><link href="https://mohcinemadkour.github.io/posts/2017/12/Getting%20Started%20with%20Spark%20Streaming%20with%20Python%20and%20Kafka/" rel="alternate"></link><published>2017-12-26T11:42:00-05:00</published><updated>2017-12-26T11:42:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2017-12-26:/posts/2017/12/Getting Started with Spark Streaming with Python and Kafka/</id><summary type="html">&lt;h1&gt;Getting Started with Spark Streaming with Python and Kafka&lt;/h1&gt;
&lt;p&gt;Last month I wrote a &lt;a href="https://www.rittmanmead.com/blog/2016/12/etl-offload-with-spark-and-amazon-emr-part-5/"&gt;series of articles&lt;/a&gt; in which I looked at the use of Spark for performing data transformation and manipulation. This was in the context of replatforming an existing Oracle-based ETL and datawarehouse solution onto cheaper and more …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Getting Started with Spark Streaming with Python and Kafka&lt;/h1&gt;
&lt;p&gt;Last month I wrote a &lt;a href="https://www.rittmanmead.com/blog/2016/12/etl-offload-with-spark-and-amazon-emr-part-5/"&gt;series of articles&lt;/a&gt; in which I looked at the use of Spark for performing data transformation and manipulation. This was in the context of replatforming an existing Oracle-based ETL and datawarehouse solution onto cheaper and more elastic alternatives. The processing that I wrote was very much batch-focussed; read a set of files from block storage ('disk'), process and enrich the data, and write it back to block storage.&lt;/p&gt;
&lt;p&gt;In this article I am going to look at &lt;a href="http://spark.apache.org/streaming/"&gt;Spark Streaming&lt;/a&gt;. This is one of several libraries that the &lt;a href="http://spark.apache.org"&gt;Spark platform&lt;/a&gt; provides (others include &lt;a href="http://spark.apache.org/sql/"&gt;Spark SQL&lt;/a&gt;, &lt;a href="http://spark.apache.org/mllib/"&gt;Spark MLlib&lt;/a&gt;, and &lt;a href="http://spark.apache.org/graphx/"&gt;Spark GraphX&lt;/a&gt;). Spark Streaming provides a way of processing "unbounded" data - commonly referred to as "streaming" data. It does this by breaking it up into microbatches, and supporting windowing capabilities for processing across multiple batches. &lt;/p&gt;
&lt;p&gt;&lt;img alt="streaming-flow" src="/images/streaming-flow.png"&gt;&lt;/p&gt;
&lt;p&gt;The use-case I'm going to put together is - almost inevitably for a generic unbounded data example - using Twitter, read from a Kafka topic.  We'll start simply, counting the number of tweets per user within each batch and doing some very simple string manipulations. After that we'll see how to do the same but over a period of time (windowing). In the next blog we'll extend this further into a more useful example, still based on Twitter but demonstrating how to satisfy some real-world requirements in the processing.&lt;/p&gt;
&lt;p&gt;I developed all of this code using Jupyter Notebooks. I've written before about how awesome notebooks are (as well as Jupyter, there's Apache Zeppelin). As well as providing a superb development environment in which the results of code can be seen, Jupyter gives the option to download a Notebook to [Markdown]](https://en.wikipedia.org/wiki/Markdown), on which this blog runs - so in fact what you're reading here comes natively from the notebook in which I developed the code. Pretty cool.&lt;/p&gt;
&lt;p&gt;I used the docker image &lt;a href="https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook"&gt;all-spark-notebook&lt;/a&gt; to provide both Jupyter and the Spark runtime environment. The only external aspect was a Kafka cluster that I had already, with tweets from the live Twitter feed on a kafka topic imaginatively called &lt;code&gt;twitter&lt;/code&gt;. &lt;/p&gt;
&lt;h2&gt;Preparing the Environment&lt;/h2&gt;
&lt;p&gt;We need to make sure that the packages we're going to use are available to Spark. Instead of downloading &lt;code&gt;jar&lt;/code&gt; files and worrying about paths, we can instead use the &lt;code&gt;--packages&lt;/code&gt; option and specify the group/artifact/version based on what's available on &lt;a href="http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.spark%22%20AND%20a%3A%22spark-streaming-kafka-0-8-assembly_2.11%22"&gt;Maven&lt;/a&gt; and Spark will handle the downloading. We specify &lt;code&gt;PYSPARK_SUBMIT_ARGS&lt;/code&gt; for this to get passed correctly when executing from within Jupyter. &lt;/p&gt;
&lt;p&gt;To run the code in Jupyter, you can put the cursor in each cell and press Shift-Enter to run it each cell at a time -- or you can use menu option &lt;code&gt;Kernel&lt;/code&gt; -&amp;gt; &lt;code&gt;Restart &amp;amp; Run All&lt;/code&gt;. When a cell is executing you'll see a &lt;code&gt;[*]&lt;/code&gt; next to it, and once the execution is complete this changes to &lt;code&gt;[y]&lt;/code&gt; where &lt;code&gt;y&lt;/code&gt; is execution step number. Any output from that step will be shown immediately below it.&lt;/p&gt;
&lt;p&gt;To run the code standalone, you would download the &lt;code&gt;.py&lt;/code&gt; from Jupyter, and execute it using &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/usr/local/spark-2.0.2-bin-hadoop2.7/bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 spark_code.py
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;PYSPARK_SUBMIT_ARGS&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.0.2 pyspark-shell&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Import dependencies&lt;/h3&gt;
&lt;p&gt;We need to import the necessary pySpark modules for Spark, Spark Streaming, and Spark Streaming with Kafka. We also need the python &lt;code&gt;json&lt;/code&gt; module for parsing the inbound twitter data&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#    Spark&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;
&lt;span class="c1"&gt;#    Spark Streaming&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;
&lt;span class="c1"&gt;#    Kafka&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming.kafka&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;
&lt;span class="c1"&gt;#    json parsing&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create Spark context&lt;/h3&gt;
&lt;p&gt;The Spark context is the primary object under which everything else is called. The &lt;code&gt;setLogLevel&lt;/code&gt; call is optional, but saves a lot of noise on stdout that otherwise can swamp the actual outputs from the job. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOrCreate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLogLevel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;WARN&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create Streaming Context&lt;/h3&gt;
&lt;p&gt;We pass the Spark context (from above) along with the batch duration (here, 60 seconds). &lt;/p&gt;
&lt;p&gt;See the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext"&gt;API reference&lt;/a&gt; and &lt;a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#initializing-streamingcontext"&gt;programming guide&lt;/a&gt; for more details. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ssc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;py4j&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;py4j&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;java_import&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gateway&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jvm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;org.apache.spark.sql.*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Connect to Kafka&lt;/p&gt;
&lt;p&gt;Using the native Spark Streaming Kafka capabilities, we use the streaming context from above to connect to our Kafka cluster. The topic connected to is &lt;code&gt;twitter&lt;/code&gt;, from consumer group &lt;code&gt;spark-streaming&lt;/code&gt;. The latter is an arbitrary name that can be changed as required. &lt;/p&gt;
&lt;p&gt;For more information see the &lt;a href="http://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming.kafka&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;
&lt;span class="n"&gt;kafkaStream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;cdh57-01-node-01.moffatt.me:2181&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;spark-streaming&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;twitter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;________________________________________________________________________________________________

  Spark Streaming&amp;#39;s Kafka libraries not found in class path. Try one of the following.

  1. Include the Kafka library and its dependencies with in the
     spark-submit command as

     $ bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8:2.2.0 ...

  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,
     Group Id = org.apache.spark, Artifact Id = spark-streaming-kafka-0-8-assembly, Version = 2.2.0.
     Then, include the jar in the spark-submit command as

     $ bin/spark-submit --jars &amp;lt;spark-streaming-kafka-0-8-assembly.jar&amp;gt; ...

________________________________________________________________________________________________





---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&amp;lt;ipython-input-5-5c350be4f6a8&amp;gt; in &amp;lt;module&amp;gt;()
      1 from pyspark.streaming.kafka import KafkaUtils
----&amp;gt; 2 kafkaStream = KafkaUtils.createStream(ssc, &amp;#39;cdh57-01-node-01.moffatt.me:2181&amp;#39;, &amp;#39;spark-streaming&amp;#39;, {&amp;#39;twitter&amp;#39;:1})


/home/mohcine/Sofwares/spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.pyc in createStream(ssc, zkQuorum, groupId, topics, kafkaParams, storageLevel, keyDecoder, valueDecoder)
     67             raise TypeError(&amp;quot;topics should be dict&amp;quot;)
     68         jlevel = ssc._sc._getJavaStorageLevel(storageLevel)
---&amp;gt; 69         helper = KafkaUtils._get_helper(ssc._sc)
     70         jstream = helper.createStream(ssc._jssc, kafkaParams, topics, jlevel)
     71         ser = PairDeserializer(NoOpSerializer(), NoOpSerializer())


/home/mohcine/Sofwares/spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.pyc in _get_helper(sc)
    193     def _get_helper(sc):
    194         try:
--&amp;gt; 195             return sc._jvm.org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper()
    196         except TypeError as e:
    197             if str(e) == &amp;quot;&amp;#39;JavaPackage&amp;#39; object is not callable&amp;quot;:


TypeError: &amp;#39;JavaPackage&amp;#39; object is not callable
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Message Processing&lt;/h2&gt;
&lt;h3&gt;Parse the inbound message as json&lt;/h3&gt;
&lt;p&gt;The inbound stream is a &lt;a href="http://spark.apache.org/docs/2.0.0/api/python/pyspark.streaming.html#pyspark.streaming.DStream"&gt;&lt;code&gt;DStream&lt;/code&gt;&lt;/a&gt;, which supports various built-in &lt;a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams"&gt;transformations&lt;/a&gt; such as &lt;code&gt;map&lt;/code&gt; which is used here to parse the inbound messages from their native JSON format. &lt;/p&gt;
&lt;p&gt;Note that this will fail horribly if the inbound message &lt;em&gt;isn't&lt;/em&gt; valid JSON. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;parsed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kafkaStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Count number of tweets in the batch&lt;/h3&gt;
&lt;p&gt;The &lt;a href="http://spark.apache.org/docs/2.0.0/api/python/pyspark.streaming.html#pyspark.streaming.DStream"&gt;&lt;code&gt;DStream&lt;/code&gt;&lt;/a&gt; object provides native functions to count the number of messages in the batch, and to print them to the output: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark.apache.org/docs/2.0.0/api/python/pyspark.streaming.html#pyspark.streaming.DStream.count"&gt;&lt;code&gt;count&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark.apache.org/docs/2.0.0/api/python/pyspark.streaming.html#pyspark.streaming.DStream.pprint"&gt;&lt;code&gt;pprint&lt;/code&gt;&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use the &lt;code&gt;map&lt;/code&gt; function to add in some text explaining the value printed. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that nothing gets written to output from the Spark Streaming context and descendent objects until the Spark Streaming Context is started, which happens later in the code&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;em&gt;&lt;code&gt;pprint&lt;/code&gt; by default only prints the first 10 values&lt;/em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;parsed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Tweets in this batch: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that if you jump ahead and try to use Windowing at this point, for example to count the number of tweets in the last hour using the &lt;code&gt;countByWindow&lt;/code&gt; function, it'll fail. This is because we've not set up the streaming context with a checkpoint directory yet. You'll get the error: &lt;code&gt;java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().&lt;/code&gt;. See later on in the blog for details about how to do this. &lt;/p&gt;
&lt;h3&gt;Extract Author name from each tweet&lt;/h3&gt;
&lt;p&gt;Tweets come through in a JSON structure, of which you can see an &lt;a href="https://gist.github.com/rmoff/3968605712f437a1f37e7be52129cade"&gt;example here&lt;/a&gt;. We're going to analyse tweets by author, which is accessible in the json structure at &lt;code&gt;user.screen_name&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;The &lt;a href="https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/"&gt;&lt;code&gt;lambda&lt;/code&gt;&lt;/a&gt; anonymous function is used to apply the &lt;code&gt;map&lt;/code&gt; to each RDD within the DStream. The result is a DStream holding just the author's screenname for each tweet in the original DStream.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;authors_dstream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parsed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;tweet&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tweet&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;screen_name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Count the number of tweets per author&lt;/h3&gt;
&lt;p&gt;With our authors DStream, we can now count them using the &lt;code&gt;countByValue&lt;/code&gt; function. This is conceptually the same as this quasi-SQL statement: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT   AUTHOR, COUNT(*)
FROM     DSTREAM
GROUP BY AUTHOR
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;_Using &lt;code&gt;countByValue&lt;/code&gt; is a more legible way of doing the same thing that you'll see done in tutorials elsewhere with a map / reduceBy. _&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;author_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;authors_dstream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countByValue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;author_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Sort the author count&lt;/h3&gt;
&lt;p&gt;If you try and use the &lt;code&gt;sortBy&lt;/code&gt; function directly against the DStream you get an error: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;#39;TransformedDStream&amp;#39; object has no attribute &amp;#39;sortBy&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is because sort is not a built-in &lt;a href="http://spark.apache.org/docs/2.0.0/api/python/pyspark.streaming.html#pyspark.streaming.DStream"&gt;DStream&lt;/a&gt; function, we use the &lt;a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#transform-operation"&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt; function to access &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy"&gt;&lt;code&gt;sortBy&lt;/code&gt;&lt;/a&gt; from pySpark. &lt;/p&gt;
&lt;p&gt;To use &lt;code&gt;sortBy&lt;/code&gt; you specify a lambda function to define the sort order. Here we're going to do it based on first the author name (index 0 of the RDD), and then of that order, by number of tweets (index 1 of the RDD). You'll note these index references being used in the &lt;code&gt;sortBy&lt;/code&gt; lambda function &lt;code&gt;x[0]&lt;/code&gt; and &lt;code&gt;x[1]&lt;/code&gt;. Thanks to &lt;a href="http://stackoverflow.com/a/41485394/350613"&gt;user6910411&lt;/a&gt; on StackOverflow for a better way of doing this. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Here I'm using &lt;code&gt;\&lt;/code&gt; as line continuation characters to make the code more legible.&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;author_counts_sorted_dstream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;author_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;\
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;foo&lt;/span&gt;\
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortBy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))))&lt;/span&gt;
&lt;span class="c1"&gt;#   .sortBy(lambda x:(x[0].lower(), -x[1]))\&lt;/span&gt;
&lt;span class="c1"&gt;#  ))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;author_counts_sorted_dstream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Get top 5 authors by tweet count&lt;/h3&gt;
&lt;p&gt;To display just the top five authors, based on number of tweets in the batch period, we'll using the &lt;a href="http://spark.apache.org/docs/2.0.2/api/python/pyspark.html#pyspark.RDD.take"&gt;&lt;code&gt;take&lt;/code&gt;&lt;/a&gt; function. My first attempt at this failed with: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;AttributeError: &amp;#39;list&amp;#39; object has no attribute &amp;#39;_jrdd&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Per my &lt;a href="http://stackoverflow.com/questions/41483746/transformed-dstream-in-pyspark-gives-error-when-pprint-called-on-it"&gt;woes on StackOverflow&lt;/a&gt; a &lt;code&gt;parallelize&lt;/code&gt; is necessary to return the values into a DStream form.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;top_five_authors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;author_counts_sorted_dstream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;\
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parallelize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;top_five_authors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Get authors with more than one tweet, or whose username starts with 'a'&lt;/h3&gt;
&lt;p&gt;Let's get a bit more fancy now - filtering the resulting list of authors to only show the ones who have tweeted more than once in our batch window, or -arbitrarily- whose screenname begins with &lt;code&gt;rm&lt;/code&gt;..&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;filtered_authors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;author_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;\
                                                &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; \
                                                &lt;span class="ow"&gt;or&lt;/span&gt; \
                                                &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;startswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rm&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We'll print this list of authors matching the criteria, sorted by the number of tweets. Note how the sort is being done inline to the calling of the &lt;code&gt;pprint&lt;/code&gt; function. Assigning variables and then &lt;code&gt;pprint&lt;/code&gt;ing them as I've done above is only done for clarity. It also makes sense if you're going to subsequently reuse the derived stream variable (such as with the &lt;code&gt;author_counts&lt;/code&gt; in this code). &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;filtered_authors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;\
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;\
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortBy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;\
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;List the most common words in the tweets&lt;/h3&gt;
&lt;p&gt;Every example has to have a version of wordcount, right? Here's an all-in-one with line continuations to make it clearer what's going on. It makes for tidier code, but it also makes it harder to debug...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;parsed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;\
    &lt;span class="n"&gt;flatMap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;tweet&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;tweet&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countByValue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;\
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortBy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Start the streaming context&lt;/h2&gt;
&lt;p&gt;Having defined the streaming context, now we're ready to actually start it! When you run this cell, the program will start, and you'll see the result of all the &lt;code&gt;pprint&lt;/code&gt; functions above appear in the output to this cell below. If you're running it outside of Jupyter (via &lt;code&gt;spark-submit&lt;/code&gt;) then you'll see the output on stdout.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I've added a &lt;code&gt;timeout&lt;/code&gt; to deliberately cancel the execution after three minutes. In practice, you would not set this :)&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;awaitTermination&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;180&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So there we have it, a very simple Spark Streaming application doing some basic processing against an inbound data stream from Kafka.&lt;/p&gt;
&lt;h2&gt;Windowed Stream Processing&lt;/h2&gt;
&lt;p&gt;Now let's have a look at how we can do windowed processing. This is where data is processed based on a 'window' which is a multiple of the batch duration that we worked with above. So instead of counting how many tweets there are every batch (say, 5 seconds), we could instead count how many there are per hour - an hour (/60 minutes/3600 seconds is the &lt;em&gt;window&lt;/em&gt; interval). We can perform this count potentially every time the batch runs; how frequently we do the count is known as the &lt;em&gt;slide&lt;/em&gt; interval.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;img alt="streaming-dstream-window" src="/images/streaming-dstream-window.png"&gt;
Image credit, and more details about window processing, &lt;a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The first thing to do to enable windowed processing in Spark Streaming is to launch the Streaming Context with a &lt;a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing"&gt;checkpoint directory&lt;/a&gt; configured. This is used to store information between batches if necessary, and also to recover from failures. You need to rework your code into the pattern &lt;a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#how-to-configure-checkpointing"&gt;shown here&lt;/a&gt;. All the code to be executed by the streaming context goes in a function - which makes it less easy to present in a step-by-step form in a notebook as I have above. &lt;/p&gt;
&lt;h3&gt;Reset the Environment&lt;/h3&gt;
&lt;p&gt;If you're running this code in the same session as above, first go to the Jupyter &lt;strong&gt;Kernel&lt;/strong&gt; menu and select &lt;strong&gt;Restart&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Prepare the environment&lt;/h3&gt;
&lt;p&gt;These are the same steps as above. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;PYSPARK_SUBMIT_ARGS&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell&amp;#39;&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.streaming.kafka&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Define the stream processing code&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;createContext&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;appName&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;PythonSparkStreamingKafka_RM_02&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLogLevel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;WARN&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ssc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Define Kafka Consumer&lt;/span&gt;
    &lt;span class="n"&gt;kafkaStream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KafkaUtils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;cdh57-01-node-01.moffatt.me:2181&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;spark-streaming2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;twitter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="c1"&gt;## --- Processing&lt;/span&gt;
    &lt;span class="c1"&gt;# Extract tweets&lt;/span&gt;
    &lt;span class="n"&gt;parsed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kafkaStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

    &lt;span class="c1"&gt;# Count number of tweets in the batch&lt;/span&gt;
    &lt;span class="n"&gt;count_this_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kafkaStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Tweets this batch: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Count by windowed time period&lt;/span&gt;
    &lt;span class="n"&gt;count_windowed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kafkaStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countByWindow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Tweets total (One minute rolling count): &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Get authors&lt;/span&gt;
    &lt;span class="n"&gt;authors_dstream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parsed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;tweet&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tweet&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;screen_name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# Count each value and number of occurences &lt;/span&gt;
    &lt;span class="n"&gt;count_values_this_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;authors_dstream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countByValue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;\
                                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;\
                                  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortBy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;\
                              &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Author counts this batch:&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;Value &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;Count &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

    &lt;span class="c1"&gt;# Count each value and number of occurences in the batch windowed&lt;/span&gt;
    &lt;span class="n"&gt;count_values_windowed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;authors_dstream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countByValueAndWindow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;\
                                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;rdd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;\
                                  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortBy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;\
                            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Author counts (One minute rolling):&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;Value &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;Count &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

    &lt;span class="c1"&gt;# Write total tweet counts to stdout&lt;/span&gt;
    &lt;span class="c1"&gt;# Done with a union here instead of two separate pprint statements just to make it cleaner to display&lt;/span&gt;
    &lt;span class="n"&gt;count_this_batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;union&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count_windowed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# Write tweet author counts to stdout&lt;/span&gt;
    &lt;span class="n"&gt;count_values_this_batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;count_values_windowed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ssc&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Launch the stream processing&lt;/h3&gt;
&lt;p&gt;This uses local disk to store the checkpoint data. In a Production deployment this would be on resilient storage such as HDFS.&lt;/p&gt;
&lt;p&gt;Note that, by design, if you restart this code using the same checkpoint folder, it will execute the &lt;em&gt;previous&lt;/em&gt; code - so if you need to amend the code being executed, specify a different checkpoint folder.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ssc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StreamingContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOrCreate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/tmp/checkpoint_v06&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;createContext&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ssc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;awaitTermination&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can see in the above output cell the full output from the job, but let's take some extracts and walk through them. &lt;/p&gt;
&lt;h3&gt;Total tweet counts&lt;/h3&gt;
&lt;p&gt;First, the total tweet counts. In the first slide window, they're the same, since we only have one batch of data so far: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------------------------------------------
Time: 2017-01-11 17:08:55
-------------------------------------------
Tweets this batch: 782
Tweets total (One minute rolling count): 782
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Five seconds later, we have 25 tweets in the current batch - giving us a total of 807 (782 + 25): &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------------------------------------------
Time: 2017-01-11 17:09:00
-------------------------------------------
Tweets this batch: 25
Tweets total (One minute rolling count): 807
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Fast forward just over a minute and we see that the windowed count for a minute is not just going up - in some cases it goes down - since our window is now not simply the full duration of the inbound data stream, but is shifting along and giving a total count for (now - 60 seconds)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------------------------------------------
Time: 2017-01-11 17:09:50
-------------------------------------------
Tweets this batch: 28
Tweets total (One minute rolling count): 1012

-------------------------------------------
Time: 2017-01-11 17:09:55
-------------------------------------------
Tweets this batch: 24
Tweets total (One minute rolling count): 254
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Count by Author&lt;/h3&gt;
&lt;p&gt;In the first batch, as with the total tweets, the batch tally is the same as the windowed one: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------------------------------------------
Time: 2017-01-11 17:08:55
-------------------------------------------
Author counts this batch:   Value AnnaSabryan   Count 8
Author counts this batch:   Value KHALILSAFADO  Count 7
Author counts this batch:   Value socialvidpress    Count 6
Author counts this batch:   Value SabSad_   Count 5
Author counts this batch:   Value CooleeBravo   Count 5
...

-------------------------------------------
Time: 2017-01-11 17:08:55
-------------------------------------------
Author counts (One minute rolling): Value AnnaSabryan   Count 8
Author counts (One minute rolling): Value KHALILSAFADO  Count 7
Author counts (One minute rolling): Value socialvidpress    Count 6
Author counts (One minute rolling): Value SabSad_   Count 5
Author counts (One minute rolling): Value CooleeBravo   Count 5
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;But notice in subsequent batches the rolling totals are accumulating for each author. Here we can see &lt;code&gt;KHALILSAFADO&lt;/code&gt; (with a previous rolling total of 7, as above) has another tweet in this batch, giving a rolling total of 8: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-------------------------------------------
Time: 2017-01-11 17:09:00
-------------------------------------------
Author counts this batch:   Value DawnExperience    Count 1
Author counts this batch:   Value KHALILSAFADO  Count 1
Author counts this batch:   Value Alchemister5  Count 1
Author counts this batch:   Value uused2callme  Count 1
Author counts this batch:   Value comfyjongin   Count 1
...

-------------------------------------------
Time: 2017-01-11 17:09:00
-------------------------------------------
Author counts (One minute rolling): Value AnnaSabryan   Count 9
Author counts (One minute rolling): Value KHALILSAFADO  Count 8
Author counts (One minute rolling): Value socialvidpress    Count 6
Author counts (One minute rolling): Value SabSad_   Count 5
Author counts (One minute rolling): Value CooleeBravo   Count 5
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Processing unbounded data sets, or "stream processing", is a new way of looking at what has always been done as batch in the past. Whilst intra-day ETL and frequent batch executions have brought latencies down, they are still standalone executions with optional bespoke code in place to handle intra-batch accumulations. With Spark Streaming we have a framework that enables this processing to be done natively and with support as default for both within-batch and across-batch (windowing). &lt;/p&gt;
&lt;p&gt;Here I used Spark Streaming because of its native support for Python, a language that I am familiar with and is (in my view) more accessible to non-coders than Java or Scala. Jupyter Notebooks are a fantastic environment in which to prototype code, and for a local environment providing both Jupyter and Spark it all you can't beat the Docker image &lt;a href="https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook"&gt;all-spark-notebook&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;There are other stream processing frameworks and languages out there, including Apache Flink, Kafka Streams, and Apache Beam, to name but three. Apache Storm and Apache Samza are also relevant, but whilst were early to the party seem to crop up less frequently in stream processing discussions and literature nowadays. &lt;/p&gt;
&lt;p&gt;In the next blog we'll see how to extend this Spark Stremaing further with processing that includes: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Matching tweet contents to predefined list of filter terms, and filtering out retweets&lt;/li&gt;
&lt;li&gt;Including only tweets that include URLs, and comparing those URLs to a whitelist of domains&lt;/li&gt;
&lt;li&gt;Sending tweets matching a given condition to a Kafka topic&lt;/li&gt;
&lt;li&gt;Keeping a tally of tweet counts per batch and over a longer period of time, as well as counts for terms matched within the tweets&lt;/li&gt;
&lt;/ul&gt;</content><category term="kafka"></category><category term="Spark"></category></entry><entry><title>Start Apache Kafka with kafka instance and apache kafka client</title><link href="https://mohcinemadkour.github.io/posts/2017/12/Kafka%20Intro/" rel="alternate"></link><published>2017-12-23T15:15:00-05:00</published><updated>2017-12-23T15:15:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2017-12-23:/posts/2017/12/Kafka Intro/</id><summary type="html">&lt;h1&gt;Start Apache Kafka with kafka instance and apache kafka client&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;System Architecture&lt;/strong&gt;
There are a bunch of processes that we need to start to run our cluster :
1. Zookeeper : Which is used by Kafka to maintain state between the nodes of the cluster.
2. Kafka brokers : The “pipes” in our …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Start Apache Kafka with kafka instance and apache kafka client&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;System Architecture&lt;/strong&gt;
There are a bunch of processes that we need to start to run our cluster :
1. Zookeeper : Which is used by Kafka to maintain state between the nodes of the cluster.
2. Kafka brokers : The “pipes” in our pipeline, which store and emit data.
3. Producers : That insert data into the cluster.
4. Consumers : That read data from the cluster.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Architecture" src="/images/basic_arch.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Starting Zookeeper&lt;/h2&gt;
&lt;p&gt;Zookeeper is a key value store used to maintain server state. Kafka requires a Zookeeper server in order to run, so the first thing we need to do is start a Zookeeper instance.&lt;/p&gt;
&lt;p&gt;Inside the extracted kafka_2.11-1.0.0, you will conveniently find a bin/zookeeper-server-start.sh file (which is used to start the server), and a config/zookeeper.properties (which provides the default configuration for the zookeeper server to run)&lt;/p&gt;
&lt;p&gt;Start the server by running (inside the kafka folders root) :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~/Software/kafka_2.11-1.0.0/bin/zookeeper-server-start.sh config/zookeeper.properties
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You should see a confirmation that the server has started.&lt;/p&gt;
&lt;p&gt;If you inspect the config/zookeeper.properties file, you should see the clientPort property set to 2181, which is the port that your zookeeper server is currently listening on.&lt;/p&gt;
&lt;h2&gt;Starting our brokers&lt;/h2&gt;
&lt;p&gt;Kafka brokers form the heart of the system, and act as the pipelines where our data is stored and distributed.&lt;/p&gt;
&lt;p&gt;Similar to how we started Zookeeper, there are two files that represent the file to start (bin/kafka-server-start.sh) and configure (config/server.properties) the broker server.&lt;/p&gt;
&lt;p&gt;Since we would like to showcase the distributed nature of kafka, let’s start up 3 brokers, as shown in the previous diagram.&lt;/p&gt;
&lt;p&gt;If you open the config/server.properties file, you will see a whole bunch of configuration that you, for the most part, do not have to worry about. There are, however, 3 properties, that have to be unique for each broker instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;broker.id=0
listeners=PLAINTEXT://:9092
log.dirs=/tmp/kafka-logs
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since we will have 3 servers, it’s better to maintain 3 configuration files for each server. Copy the config/server.properties file and make 3 files for each server instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cp config/server.properties config/server.1.properties
cp config/server.properties config/server.2.properties
cp config/server.properties config/server.3.properties
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Change the above 3 properties for each copy of the file so that they are all unique.&lt;/p&gt;
&lt;p&gt;server.1.properties&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;broker.id=1
listeners=PLAINTEXT://:9093
log.dirs=/tmp/kafka-logs1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;server.2.properties&lt;/p&gt;
&lt;p&gt;broker.id=2
listeners=PLAINTEXT://:9094
log.dirs=/tmp/kafka-logs2&lt;/p&gt;
&lt;p&gt;server.3.properties&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;broker.id=3
listeners=PLAINTEXT://:9095
log.dirs=/tmp/kafka-logs3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Also, create the log directories that we configured:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir /tmp/kafka-logs1
mkdir /tmp/kafka-logs2
mkdir /tmp/kafka-logs3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, we can start the broker instances. Run the below three commands on different terminal sessions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bin/kafka-server-start.sh config/server.1.properties

bin/kafka-server-start.sh config/server.2.properties

bin/kafka-server-start.sh config/server.3.properties
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You should see a startup message when the brokers start successfully, as well as logs on the Zookeeper instance that tell you of a new connection with each broker.&lt;/p&gt;
&lt;h2&gt;Creating a topic&lt;/h2&gt;
&lt;p&gt;Before we can start putting data into your cluster, we need to create a topic to which the data will belong. To do this, run the command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bin/kafka-topics.sh --create --topic my-kafka-topic --zookeeper localhost:2181 --partitions 3 --replication-factor 2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The paritions options lets you decide how many brokers you want your data to be split between. Since we set up 3 brokers, we can set this option to 3.&lt;/p&gt;
&lt;p&gt;The replication-factor describes how many copies of you data you want (in case one of the brokers goes down, you still have your data on the others).&lt;/p&gt;
&lt;p&gt;Once you create the topic, you should see a confirmation message.&lt;/p&gt;
&lt;h2&gt;The producer instance&lt;/h2&gt;
&lt;p&gt;The “producer” is the process that puts data into our Kafka cluster. The command line tools in the bin directory provide us with a console producer, that inputs data into the cluster every time your enter text into the console.&lt;/p&gt;
&lt;p&gt;To start the console producer, run the command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bin/kafka-console-producer.sh --broker-list localhost:9093,localhost:9094,localhost:9095 --topic my-kafka-topic
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The broker-list option points the producer to the addresses of the brokers that we just provisioned, and the topic option specifies the topic you want the data to come under.&lt;/p&gt;
&lt;p&gt;You should now see a command prompt, in which you can enter a bunch of text which gets inserted into the Kafka cluster you just created every time you hit enter.&lt;/p&gt;
&lt;h2&gt;Consumers&lt;/h2&gt;
&lt;p&gt;The only thing left to do is read data from the cluster.&lt;/p&gt;
&lt;p&gt;Run the command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bin/kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic my-kafka-topic --from-beginning
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The bootstrap-server can be any one of the brokers in the cluster, and the topic should be the same as the topic under which you producers inserted data into the cluster.&lt;/p&gt;
&lt;p&gt;The from-beginning option tells the cluster that you want all the messages that it currently has with it, even messages that we put into it previously.&lt;/p&gt;
&lt;p&gt;When you run the above command, you should immediately see all the messages that you input using the producer, logged onto your console.&lt;/p&gt;
&lt;p&gt;Additionally, if you input anymore messages with the producer while the consumer is running, you should see it output into the console in real time.&lt;/p&gt;
&lt;p&gt;'''And in this way, Kafka acts sort of like a persistent message queue, saving the messages that were not yet read by the consumer, while passing on new messages as they come while the consumer is running'''&lt;/p&gt;
&lt;h2&gt;Messing things up&lt;/h2&gt;
&lt;p&gt;Now that we are all done setting up and running a Kafka cluster on our system, let’s test how persistent Kafka can be.&lt;/p&gt;
&lt;p&gt;Shut down one of the three brokers that you ran, and you should see that your cluster is still running fine. This means that Kafka is tolerant to some of its nodes failing.&lt;/p&gt;
&lt;p&gt;Try starting another consumer in a different terminal window:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bin/kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic my-kafka-topic --from-beginning --group group2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The only thing we’ve added here is the group option, which differentiates one consumer from another. Once you start this, you should see all messages getting logged on the console from the beginning. Even though one of our brokers was shut down, our data was not lost. This is because the replication factor of 2 that we set earlier ensured that a copy of our data was present on multiple brokers.&lt;/p&gt;
&lt;p&gt;You can play around with your setup in many more ways. What happens if you take down another broker? What if you had 5 brokers and took 2 of them down? What if you changed the replication factor for your topic?&lt;/p&gt;
&lt;p&gt;The best way to know how resilient Kafka is, is to experiment with it yourself.&lt;/p&gt;</content><category term="Apache kafka"></category></entry><entry><title>Alternating Least Square example with SPARK</title><link href="https://mohcinemadkour.github.io/posts/2017/04/sparkim/" rel="alternate"></link><published>2017-04-09T16:00:00-04:00</published><updated>2017-04-09T16:00:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2017-04-09:/posts/2017/04/sparkim/</id><summary type="html">&lt;h1&gt;The data&lt;/h1&gt;
&lt;p&gt;This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.
FullFile.png&lt;/p&gt;
&lt;h1&gt;ALS&lt;/h1&gt;
&lt;p&gt;&lt;img alt="image" src="/images/ALS.png"&gt;&lt;/p&gt;
&lt;h1&gt;Step 1 - Create …&lt;/h1&gt;</summary><content type="html">&lt;h1&gt;The data&lt;/h1&gt;
&lt;p&gt;This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.
FullFile.png&lt;/p&gt;
&lt;h1&gt;ALS&lt;/h1&gt;
&lt;p&gt;&lt;img alt="image" src="/images/ALS.png"&gt;&lt;/p&gt;
&lt;h1&gt;Step 1 - Create an RDD from the CSV File&lt;/h1&gt;
&lt;h2&gt;1.1 - Download the data&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    #Download the data from github to the local directory
    !rm &amp;#39;OnlineRetail.csv.gz&amp;#39; -f
    !wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;1.2 - Put the csv into an RDD (at first, each row in the RDD is a string which correlates to a line in the csv) and show the first three lines.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Use the Spark context (sc) to get the list of possible methods. sc.&lt;TAB&gt;&lt;/li&gt;
&lt;li&gt;Use the textFile() method&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;loadRetailData = sc.textFile(&amp;quot;OnlineRetail.csv.gz&amp;quot;)
loadRetailData.take(3)
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Step 2 - Prepare and shape the data: "80% of a Data Scientists job"&lt;/h1&gt;
&lt;h2&gt;2.1 - Remove the header from the RDD and split the remaining lines by comma.&lt;/h2&gt;
&lt;p&gt;The header is the first line in the RDD -- use first() to obtain it.
Use the filter() method to filter out all lines which are not equal to the header line.
Map the split() method to the remaining lines to split on ","&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;header = loadRetailData.first()
splitColumns = loadRetailData.filter(lambda line: line != header).map(lambda l: l.split(&amp;quot;,&amp;quot;))
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;2.2 - Filter the remaining lines using &lt;a href="https://docs.python.org/2.6/howto/regex.html"&gt;regular expressions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The original file at UCI's Machine Learning Repository has commas in the product description. Those have been removed to expediate the lab. Only keep rows that have a quantity greater than 0, a non-empty customerID, and a non-blank stock code after removing non-numeric characters.&lt;/p&gt;
&lt;p&gt;-Examine the header to determine which fields need to be used to filter the data.
- Use the filter() method for the first two requirements. Note -- you may have to cast values.
- Look at the &lt;a href="https://docs.python.org/2.6/howto/regex.html"&gt;re.sub()&lt;/a&gt; method&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;
&lt;span class="n"&gt;filteredRetailData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;splitColumns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;\D&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;2.3 - Map each line to a SQL Row and create a Dataframe from the result. Register the Dataframe as an SQL temp table.&lt;/h2&gt;
&lt;p&gt;Use the following for the Row column names: inv, stockCode, description, quant, invDate, price, custId, country. inv, stockCode, quant and custId should be integers.
price is a float. description and country are strings (the default).&lt;/p&gt;
&lt;p&gt;Hint: When you replaced non-digit characters using the regular expression above, you replaced them in the context of a test. You'll have to do it again when creating the stockCode Row value. &lt;/p&gt;
&lt;p&gt;-We haven't used SQLContext or Row in this notebook, so you will have to import them from the pyspark.sql package and then create a SQLContext
-You can create a Row using a map(). For example:
example = myRDD.map(lambda x: Row(v1=x[1], v2=int(x[2]), v3=float(x[3]))
Note how we set the column names this way
-use createDataFrame() in your SQLContext. Then register the dataframe with registerTempTable()
from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SQLContext&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;
     &lt;span class="n"&gt;sqlContext&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SQLContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SQLContext&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;
&lt;span class="n"&gt;sqlContext&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SQLContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;retailRows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filteredRetailData&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;stockCode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;\D&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt; &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;quant&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;invDate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;custId&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;country&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;retailDf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sqlContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;retailRows&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;retailDf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;registerTempTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;retailPurchases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;2.4 - Keep only the data we need (custId, stockCode, and rank)&lt;/h2&gt;
&lt;p&gt;The Alternating Least Squares algorithm requires three values. In this case, we're going to use the Customer ID (custId), stock code (stockCode) and a ranking value. In this situation there is not a ranking value within the data, so we will create one. We will set a value of 1 to indicate a purchase since these are all actual orders. Set that value to "purch".&lt;/p&gt;
&lt;p&gt;After doing the select, group by custId and stockCode. 
- To add a fixed value within a select statement, use something like select x,y,1 as purch from z
- - Use the group by statement to group results. To group by two values, separate them by commas (i.e. group by x,y)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;query = &amp;quot; SELECT custId, stockCode, 1 as purch FROM retailPurchases group by custId, stockCode&amp;quot;
uniqueCombDf = sqlContext.sql(query)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;2.5 - Randomly split the data into a testing set (10% of the data), a cross validation set (10% of the data) a training set (80% of the data)&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;testDf, cvDf, trainDf = uniqueCombDf.randomSplit([.1,.1,.8])
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Step 3 - Build recommendation models&lt;/h1&gt;
&lt;h2&gt;3.1 - Use the training dataframe to train a model with Alternating Least Squares using the ALS class&lt;/h2&gt;
&lt;p&gt;ALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called ‘factor’ matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix.&lt;/p&gt;
&lt;p&gt;Latent Factors / rank
    The number of columns in the user-feature and product-feature matricies
Iterations / maxIter
    The number of factorization runs&lt;/p&gt;
&lt;p&gt;To use the ALS class type:
from pyspark.ml.recommendation import ALS&lt;/p&gt;
&lt;p&gt;When running ALS, we need to create two separate instances. For both instances userCol is custId, itemCol is stockCode and ratingCol is purch.&lt;/p&gt;
&lt;p&gt;For the first instance, use a rank of 15 and set iterations to 5.
For the second instance, use a rank of 2 and set iterations to 10.
Run fit() on both instances using the training dataframe.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.ml.recommendation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ALS&lt;/span&gt;
&lt;span class="n"&gt;als1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxIter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;userCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;custId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;itemCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stockCode&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratingCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;purch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;als1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainDf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.ml.recommendation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ALS&lt;/span&gt;
&lt;span class="n"&gt;als1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxIter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;userCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;custId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;itemCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stockCode&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratingCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;purch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;als1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainDf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;als2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxIter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;userCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;custId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;itemCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stockCode&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratingCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;purch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;als2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainDf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Step 4 - Test the models&lt;/h1&gt;
&lt;p&gt;Use the models to predict what the user will rate a certain item. The closer our model is to 1 for an item a user has already purchased, the better.&lt;/p&gt;
&lt;h2&gt;4.1 - Evaluate the model with the cross validation dataframe by using the transform function.&lt;/h2&gt;
&lt;p&gt;Some of the users or purchases in the cross validation data may not have been in the training data. Let's remove the ones that aren't. To do this obtain all the the custId and stockCode values from the training data and filter out any lines with those values from the cross-validation data.&lt;/p&gt;
&lt;p&gt;-At the end, print out how many cross-validation lines we had at the start -- and the new number afterwords.
-Use map() to return a specific value (i.e. foo = foo.map(lambda x: x.value)) and put them all in a set (i.e. foo1 = set(foo))
-You need all the returned values (remember they might be spread all across the cluster!) so run collect() on the results of the map(). (i.e. foo1 = set(foo.collect()))
- Use the filter() to filter out any values in the cross-validation dataframe which are in the stockCode or custId sets. Use toDF() to change the results to a dataframe.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;customers = set(trainDf.rdd.map(lambda line: line.custId).collect())
stock = set(trainDf.rdd.map(lambda line: line.stockCode).collect())
filteredCvDf = cvDf.rdd.filter(lambda line: line.stockCode in stock and line.custId in customers).toDF()
print cvDf.count()
print filteredCvDf.count()
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Step 4.2 - Make Predictions using transform()&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;predictions1 = model1.transform(filteredCvDf)
predictions2 = model2.transform(filteredCvDf)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;4.3 - Calculate and print the Mean Squared Error. For all ratings, subtract the prediction from the actual purchase (1), square the result, and take the mean of all of the squared differences.&lt;/h2&gt;
&lt;p&gt;The lower the result number, the better the model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;meanSquaredError1 = predictions1.map(lambda line: (line.purch - line.prediction)**2).mean()
meanSquaredError2 = predictions2.map(lambda line: (line.purch - line.prediction)**2).mean()
print &amp;#39;Mean squared error = %.4f for our first model&amp;#39; % meanSquaredError1
print &amp;#39;Mean squared error = %.4f for our second model&amp;#39; % meanSquaredError2
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;4.4 - Confirm the model by testing it with the test data and the best hyperparameters found during cross-validation&lt;/h2&gt;
&lt;p&gt;Filter the test dataframe (testDf) the same way as the cross-validation dataframe. Then run the transform() and calculate the mean squared error. It should be the same as the value calcuated above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;filteredTestDf = testDf.rdd.filter(lambda line: line.stockCode in stock and line.custId in customers).toDF()
predictions3 = model2.transform(filteredTestDf)
meanSquaredError3 = predictions3.map(lambda line: (line.purch - line.prediction)**2).mean()
print &amp;#39;Mean squared error = %.4f for our best model&amp;#39; % meanSquaredError3
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Step 5 - Implement the model&lt;/h1&gt;
&lt;h2&gt;5.1 - First, create a dataframe in which each row has the user id and an item id.&lt;/h2&gt;
&lt;p&gt;Use the Dataframe methods to create a Dataframe with a specific user and that user's purchased products.
    First, use the Dataframe filter() to filter out all custId's but 15544.
    Then use the select() to only return the custId column.
    Now use distinct() to ensure we only have the single custId.
    Do a join() with the distinct values from the stockCode column. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;user = trainDf.filter(trainDf.custId == 15544)
userCustId = user.select(&amp;quot;custId&amp;quot;)
userCustIdDistinct = userCustId.distinct()
stockCode = trainDf.select(&amp;quot;stockCode&amp;quot;)
stockCodeDistinct = stockCode.distinct()
userItems = userCustIdDistinct.join(stockCodeDistinct)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;5.2 - Use 'transform' to rate each item.&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bestRecsDf = model2.transform(userItems)
bestRecsDf.first()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to print the top five recommendations, we need to sort() them in descending orde&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="images/user.png"&gt;&lt;/p&gt;
&lt;p&gt;This user seems to have purchased a lot of childrens gifts and some holiday items. The recommendation engine we created suggested some items along these lines&lt;/p&gt;</content><category term="Spark"></category><category term="machine learning"></category></entry><entry><title>Leaflet for R</title><link href="https://mohcinemadkour.github.io/posts/2017/04/RLeaflet/" rel="alternate"></link><published>2017-04-08T10:00:00-04:00</published><updated>2017-04-08T10:00:00-04:00</updated><author><name>Mohcine madkour</name></author><id>tag:mohcinemadkour.github.io,2017-04-08:/posts/2017/04/RLeaflet/</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Leaflet is one of the most popular open-source JavaScript libraries for interactive maps. It’s used by websites ranging from The New York Times and The Washington Post to GitHub and Flickr, as well as GIS specialists like OpenStreetMap, Mapbox, and CartoDB.
This R package makes it easy to …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Leaflet is one of the most popular open-source JavaScript libraries for interactive maps. It’s used by websites ranging from The New York Times and The Washington Post to GitHub and Flickr, as well as GIS specialists like OpenStreetMap, Mapbox, and CartoDB.
This R package makes it easy to integrate and control Leaflet maps in R.&lt;/p&gt;
&lt;p&gt;This R package makes it easy to integrate and control Leaflet maps in R.&lt;/p&gt;
&lt;h1&gt;Installation&lt;/h1&gt;
&lt;p&gt;To install this R package, run this command at your R prompt:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;install.packages(&amp;quot;leaflet&amp;quot;)
# to install the development version from Github, run
# devtools::install_github(&amp;quot;rstudio/leaflet&amp;quot;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Basic Usage&lt;/p&gt;
&lt;p&gt;You create a Leaflet map with these basic steps:&lt;/p&gt;
&lt;p&gt;1- Create a map widget by calling leaflet().
2- Add layers (i.e., features) to the map by using layer functions (e.g.  addTiles, addMarkers, addPolygons) to modify the map widget.
3- Repeat step 2 as desired.
4- Print the map widget to display it.&lt;/p&gt;
&lt;p&gt;Here’s a basic example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;leaflet&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;dplyr&lt;span class="p"&gt;)&lt;/span&gt;
m &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; leaflet&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  addTiles&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;  &lt;span class="c1"&gt;# Add default OpenStreetMap map tiles&lt;/span&gt;
  addMarkers&lt;span class="p"&gt;(&lt;/span&gt;lng&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;174.768&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lat&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;-36.852&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; popup&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;The birthplace of R&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
m  &lt;span class="c1"&gt;# Print the map&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Leaflet Heat Maps&lt;/h1&gt;
&lt;p&gt;Create Map&lt;/p&gt;
&lt;p&gt;We start by creating a map of the location.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;rMaps&lt;span class="p"&gt;)&lt;/span&gt;
L2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; Leaflet&lt;span class="o"&gt;$&lt;/span&gt;new&lt;span class="p"&gt;()&lt;/span&gt;
L2&lt;span class="o"&gt;$&lt;/span&gt;setView&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;29.7632836&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="m"&gt;-95.3632715&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
L2&lt;span class="o"&gt;$&lt;/span&gt;tileLayer&lt;span class="p"&gt;(&lt;/span&gt;provider &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;MapQuestOpen.OSM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
L2
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Get Data&lt;/h1&gt;
&lt;p&gt;We will use the crime dataset from the ggmap package that contains a tidied up version of Houston crime data from January 2010 to August 2010.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data(crime, package = &amp;#39;ggmap&amp;#39;)
library(plyr)
crime_dat = ddply(crime, .(lat, lon), summarise, count = length(address))
crime_dat = toJSONArray2(na.omit(crime_dat), json = F, names = F)
cat(rjson::toJSON(crime_dat[1:2]))

[[27.5071143,-99.5055471,1],[29.4836146,-95.0618715,10]

Add HeatMap
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have the map and the data, the next step is to add the data to the map as a heatmap layer. Thanks to the Leaflet.heat plugin written by the Vladimir Agafonkin, the author of LeafletJS, this is really easy to do, with a little bit of custom javascript.&lt;/p&gt;
&lt;h1&gt;Add leaflet-heat plugin. Thanks to Vladimir Agafonkin&lt;/h1&gt;
&lt;p&gt;L2$addAssets(jshead = c(
  "http://leaflet.github.io/Leaflet.heat/dist/leaflet-heat.js"
))&lt;/p&gt;
&lt;h1&gt;Add javascript to modify underlying chart&lt;/h1&gt;
&lt;p&gt;L2$setTemplate(afterScript = sprintf("
&lt;script&gt;
  var addressPoints = %s
  var heat = L.heatLayer(addressPoints).addTo(map)         &lt;br&gt;
&lt;/script&gt;
", rjson::toJSON(crime_dat)
))&lt;/p&gt;
&lt;p&gt;L2&lt;/p&gt;</content><category term="Data Visualization"></category><category term="R"></category></entry><entry><title>An interactive visual of Houston 311 calls</title><link href="https://mohcinemadkour.github.io/posts/2017/04/Vis311R/" rel="alternate"></link><published>2017-04-08T10:00:00-04:00</published><updated>2017-04-08T10:00:00-04:00</updated><author><name>Mohcine madkour</name></author><id>tag:mohcinemadkour.github.io,2017-04-08:/posts/2017/04/Vis311R/</id><summary type="html">&lt;h1&gt;An interactive visual of Houston 311 calls&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Houston receives 311 calls for non-emergency services from it's residents, businesses and visitors. The response time for these calls is longer than those for emergency (911) calls. Accordingly the resources allocated to these services may not be as highly funded, resourced and …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;An interactive visual of Houston 311 calls&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Houston receives 311 calls for non-emergency services from it's residents, businesses and visitors. The response time for these calls is longer than those for emergency (911) calls. Accordingly the resources allocated to these services may not be as highly funded, resourced and/or prioritized as the emergency services. It is why the Houston authorities need to optimize the use and allocation of the resources available to service these calls.&lt;/p&gt;
&lt;p&gt;Help Houston city to optimize the use and allocation of the resources available to service 311 calls.&lt;/p&gt;
&lt;p&gt;Objective&lt;/p&gt;
&lt;p&gt;The goal of this visual analysis is to aid the NYC authorities in optimizing the use of the limited resources available to service 311 calls/requests. There are 3 dimensions chosen along which this optimization can be done, time, location and skill type.&lt;/p&gt;
&lt;h1&gt;barplot()&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    pol = read.csv(&amp;quot;http://www.calvin.edu/~stob/data/csbv.csv&amp;quot;)
    barplot(table(pol$Political04), main=&amp;quot;Political Leanings, Calvin Freshman 2004&amp;quot;)
    barplot(table(pol$Political04), horiz=T)
    barplot(table(pol$Political04),col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;orange&amp;quot;))
    barplot(table(pol$Political04),col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;orange&amp;quot;),names=c(&amp;quot;Conservative&amp;quot;,&amp;quot;Far Right&amp;quot;,&amp;quot;Liberal&amp;quot;,&amp;quot;Centrist))
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="/images/barplot.png"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   barplot(xtabs(~sex + Political04, data=pol), legend=c(&amp;quot;Female&amp;quot;,&amp;quot;Male&amp;quot;), beside=T)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="/images/barplotNB.png"&gt;&lt;/p&gt;
&lt;h1&gt;boxplot()&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data(iris)
boxplot(iris$Sepal.Length)
boxplot(iris$Sepal.Length, col=&amp;quot;yellow&amp;quot;)
boxplot(Sepal.Length ~ Species, data=iris)
boxplot(Sepal.Length ~ Species, data=iris, col=&amp;quot;yellow&amp;quot;, ylab=&amp;quot;Sepal length&amp;quot;,main=&amp;quot;Iris Sepal Length by Species&amp;quot;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="/images/boxplot.png"&gt;&lt;/p&gt;
&lt;h1&gt;plot()&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data(faithful)
plot(waiting~eruptions,data=faithful)
plot(waiting~eruptions,data=faithful,cex=.5)
plot(waiting~eruptions,data=faithful,pch=6)
plot(waiting~eruptions,data=faithful,pch=19)
plot(waiting~eruptions,data=faithful,cex=.5,pch=19,col=&amp;quot;blue&amp;quot;)
plot(waiting~eruptions, data=faithful, cex=.5, pch=19, col=&amp;quot;blue&amp;quot;, main=&amp;quot;Old    Faithful Eruptions&amp;quot;,
ylab=&amp;quot;Wait time between eruptions&amp;quot;, xlab=&amp;quot;Duration of eruption&amp;quot;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="/images/plot.PNG"&gt;&lt;/p&gt;</content><category term="Data Visualization"></category><category term="R"></category><category term="Houston"></category><category term="311 calls"></category></entry><entry><title>Viusalization in R</title><link href="https://mohcinemadkour.github.io/posts/2017/04/VisR/" rel="alternate"></link><published>2017-04-08T10:00:00-04:00</published><updated>2017-04-08T10:00:00-04:00</updated><author><name>Mohcine madkour</name></author><id>tag:mohcinemadkour.github.io,2017-04-08:/posts/2017/04/VisR/</id><summary type="html">&lt;h1&gt;barplot()&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    pol = read.csv(&amp;quot;http://www.calvin.edu/~stob/data/csbv.csv&amp;quot;)
    barplot(table(pol$Political04), main=&amp;quot;Political Leanings, Calvin Freshman 2004&amp;quot;)
    barplot(table(pol$Political04), horiz=T)
    barplot(table(pol$Political04),col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;orange&amp;quot;))
    barplot(table(pol$Political04),col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;orange&amp;quot;),names=c …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;h1&gt;barplot()&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    pol = read.csv(&amp;quot;http://www.calvin.edu/~stob/data/csbv.csv&amp;quot;)
    barplot(table(pol$Political04), main=&amp;quot;Political Leanings, Calvin Freshman 2004&amp;quot;)
    barplot(table(pol$Political04), horiz=T)
    barplot(table(pol$Political04),col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;orange&amp;quot;))
    barplot(table(pol$Political04),col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;orange&amp;quot;),names=c(&amp;quot;Conservative&amp;quot;,&amp;quot;Far Right&amp;quot;,&amp;quot;Liberal&amp;quot;,&amp;quot;Centrist))
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="/images/barplot.PNG"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   barplot(xtabs(~sex + Political04, data=pol), legend=c(&amp;quot;Female&amp;quot;,&amp;quot;Male&amp;quot;), beside=T)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="/images/barplotNB.PNG"&gt;&lt;/p&gt;
&lt;h1&gt;boxplot()&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data(iris)
boxplot(iris$Sepal.Length)
boxplot(iris$Sepal.Length, col=&amp;quot;yellow&amp;quot;)
boxplot(Sepal.Length ~ Species, data=iris)
boxplot(Sepal.Length ~ Species, data=iris, col=&amp;quot;yellow&amp;quot;, ylab=&amp;quot;Sepal length&amp;quot;,main=&amp;quot;Iris Sepal Length by Species&amp;quot;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="/images/boxplot.PNG"&gt;&lt;/p&gt;
&lt;h1&gt;plot()&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data(faithful)
plot(waiting~eruptions,data=faithful)
plot(waiting~eruptions,data=faithful,cex=.5)
plot(waiting~eruptions,data=faithful,pch=6)
plot(waiting~eruptions,data=faithful,pch=19)
plot(waiting~eruptions,data=faithful,cex=.5,pch=19,col=&amp;quot;blue&amp;quot;)
plot(waiting~eruptions, data=faithful, cex=.5, pch=19, col=&amp;quot;blue&amp;quot;, main=&amp;quot;Old    Faithful Eruptions&amp;quot;,
ylab=&amp;quot;Wait time between eruptions&amp;quot;, xlab=&amp;quot;Duration of eruption&amp;quot;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="image" src="/images/plot.PNG"&gt;&lt;/p&gt;
&lt;h1&gt;Heatmap&lt;/h1&gt;
&lt;h2&gt;Installing and loading required packages&lt;/h2&gt;
&lt;p&gt;At first glance, this section might look a little bit more complicated then it need be, since executing library(packagename) is already sufficient to load required R packages if they are already installed.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if (!require(&amp;quot;gplots&amp;quot;)) {
install.packages(&amp;quot;gplots&amp;quot;, dependencies = TRUE)
library(gplots)
}
if (!require(&amp;quot;RColorBrewer&amp;quot;)) {
install.packages(&amp;quot;RColorBrewer&amp;quot;, dependencies = TRUE)
library(RColorBrewer)
}
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Reading in data and transform it into matrix format&lt;/h2&gt;
&lt;p&gt;We can feed in our data into R from many different data file formats, including ASCII formatted text files, Excel spreadsheets and so on. For this tutorial, we assume that our data is formatted as Comma-Separated Values (CSV); probably one of the most common data file formats.&lt;/p&gt;
&lt;p&gt;When we open the CSV file in our favorite plain text editor instead of using a spread sheet program (Excel, Numbers, etc.), it looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#heat map example data set,,,,
#12/08/13 sr,,,,
#
,var1,var2,var3,var4
measurement1,0.094,0.668,0.4153,0.4613
measurement2,0.1138,-0.3847,0.2671,0.1529
measurement3,0.1893,0.3303,0.5821,0.2632
measurement4,-0.0102,-0.4259,-0.5967,0.18
measurement5,0.1587,0.2948,0.153,-0.2208
measurement6,-0.4558,0.2244,0.6619,0.0457
measurement7,-0.6241,-0.3119,0.3642,0.2003
measurement8,-0.227,0.499,0.3067,0.3289
measurement9,0.7365,-0.0872,-0.069,-0.4252
measurement10,0.9761,0.4355,0.8663,0.8107
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When we are reading the data from our CSV file into R and assign it to the variable data, note the two lines of comments preceding the main data in our CSV file, indicated by an octothorpe (#) character. Since we don’t need those lines to plot our heat map, we can ignore them by via the comment.char argument in the read.csv() function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;../datasets/heatmaps_in_r.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; comment.char&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One tricky part of the heatmap.2() function is that it requires the data in a numerical matrix format in order to plot it. By default, data that we read from files using R’s read.table() or read.csv() functions is stored in a data table format. The matrix format differs from the data table format by the fact that a matrix can only hold one type of data, e.g., numerical, strings, or logical. Fortunately, we don’t have to worry about the row that contains our column names (var1, var2, var3, var4) since the read.csv() function treats the first line of data as table header by default. But we would run into trouble if we want to include the row names (measurement1, measurment2, etc.) in our numerical matrix. For our own convenience, we store those row names in the first column as variable rnames, which we can use later to assign row names to our matrix after the conversion.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rnames &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; data&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we transform the numerical data from the variable data (column 2 to 5) into a matrix and assign it to a new variable mat_data&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mat_data &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;data.matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="kp"&gt;ncol&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Instead of using the rather fiddly expression ncol(data)], which returns the total number of columns from the data table, we could also provide the integer 5 directly in order to specify the last column that we want to include. However, ncol(data)] is more convenient for larger data sets so that we don’t need to count all columns to get the index of the last column for specifying the upper boundary. Next, we assign the column names, which we have saved as rnames previously, to the matrix via&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kp"&gt;rownames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;mat_data&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; rnames
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Customizing and plotting the heat map&lt;/h2&gt;
&lt;p&gt;Finally, we have our data in the “right” format in order to create our heat map, but before we get down to business, let us have a brief look at some options for customization.&lt;/p&gt;
&lt;h2&gt;Optional: Choosing custom color palettes and color breaks&lt;/h2&gt;
&lt;p&gt;Instead of using the default colors of the heatmap.2() function, I want to show you how to use the RColorBrewer package for creating our own color palettes. Here, we go with the most popular choice for heat maps: A color range from green over yellow to red.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    my_palette &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; colorRampPalette&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;yellow&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;green&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;n &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;299&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are many different ways to specify colors in R. I find it most convenient to assign colors by their name. A nice overview of the different color names in R can be found at http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf&lt;/p&gt;
&lt;p&gt;The argument (n = 299) lets us define how many individuals colors we want to have in our palette. Obviously, the higher the number of individual colors, the smoother the transition will be; the number 299 should be sufficiently large enough for a smooth transition. By default, RColorBrewer will divide the colors evenly so that every color in our palette will be an interval of individual colors of similar size. However, sometimes we want to have a little skewed color range depending on the data we are analyzing. Let’s assume that our example data set consists of Pearson correlation coefficients (i.e., R values) ranging from –1 to 1, and we are particularly interested in samples that have a (relatively) high correlation: R values in the range between 0.8 to 1.0. We want to highlight these samples in our heat map by only showing values from 0.8 to 1 in green. In this case, we can define our color breaks “unevenly” by using the following code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;col_breaks = c(seq(-1,0,length=100), # for red
seq(0,0.8,length=100),  # for yellow
seq(0.81,1,length=100)) # for green
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Optional: Saving the heat map as PNG file&lt;/h2&gt;
&lt;p&gt;R supports a variety of different vector graphics formats, such as SVG, PostScript, and PDFs, and raster graphics (bitmaps) like JPEG, PNG, TIFF, BMP, etc. Each format comes with its own advantages and disadvantages, and depending on the particular purposes (websites, journal articles, PowerPoint presentations, archiving … ) we chose one file format over the other. I don’t want to discuss all the details about when to use which particular file format in this tutorial but instead use a more common PNG format for our heat map. I picked PNG instead of JPEG, because PNG offers lossless compression (JPEG is a lossy image format) at the small cost of a slightly larger file size. However, you could completely omit the png() function in your script if you just want to show the heat map in an interactive screen in R.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;png(&amp;quot;../images/heatmaps_in_r.png&amp;quot;,    # create PNG for the heat map        
width = 5*300,        # 5 x 300 pixels
height = 5*300,
res = 300,            # 300 pixels per inch
pointsize = 8)        # smaller font size
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The default parameters of the png() function would yield a relatively small PNG file at very low resolution, which is not really practical for heat maps. Thus we provide additional arguments for the image width, height and the resolution. The units of width and height are pixels, not inches. So if we want to create a 5x5 inch image with 300 pixels per inch, we have to do a little math here: [1500 pixels] / [300 pixels/inch] = 5 inches. Also, we choose a slightly smaller font size of 8 pt.&lt;/p&gt;
&lt;p&gt;Be careful to not forget to close the png() plotting device at the end of you script via the function dev.off() otherwise you probably won’t be able to open the PNG file to view it.
Plotting the heat map&lt;/p&gt;
&lt;p&gt;Now, let’s get down to business and take a look at the heatmap.2() function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    heatmap.2(mat_data,
      cellnote = mat_data,  # same data set for cell labels
      main = &amp;quot;Correlation&amp;quot;, # heat map title
      notecol=&amp;quot;black&amp;quot;,      # change font color of cell labels to black
      density.info=&amp;quot;none&amp;quot;,  # turns off density plot inside color legend
      trace=&amp;quot;none&amp;quot;,         # turns off trace lines inside the heat map
      margins =c(12,9),     # widens margins around plot
      col=my_palette,       # use on color palette defined earlier
      breaks=col_breaks,    # enable color transition at specified limits
      dendrogram=&amp;quot;row&amp;quot;,     # only draw a row dendrogram
      Colv=&amp;quot;NA&amp;quot;)            # turn off column clustering
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Update Feb 19, 2014 - Clustering Methods&lt;/p&gt;
&lt;p&gt;If you want to change the default clustering method (complete linkage method with Euclidean distance measure), this can be done as follows: For a square matrix, we can define the distance and cluster based on our matrix data by&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;distance = dist(mat_data, method = &amp;quot;manhattan&amp;quot;)
cluster = hclust(distance, method = &amp;quot;ward&amp;quot;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And eventually plug it into the heatmap.2() function&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;heatmap.2(mat_data,
  ...
  Rowv = as.dendrogram(cluster), # apply default clustering method
  Colv = as.dendrogram(cluster)) # apply default clustering method
)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Update Mar 2, 2014 - Categorizing Measurements&lt;/p&gt;
&lt;p&gt;I was just asked how to categorize the input variables by applying row or column labels. For example, if we want to group the “measurement” variables into 3 different categories: measurement 1-3 = category 1 measurement 4-6 = category 2 measurement 7-10 = category 3. My solution would be to simply provide RowSideColors as additional argument to the heatmap.2() function. E.g.,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;heatmap.2(mat_data,
  ...
  RowSideColors = c(    # grouping row-variables into different
     rep(&amp;quot;gray&amp;quot;, 3),   # categories, Measurement 1-3: green
     rep(&amp;quot;blue&amp;quot;, 3),    # Measurement 4-6: blue
     rep(&amp;quot;black&amp;quot;, 4)),    # Measurement 7-10: red
  ...
)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that we could also provide similar labels to the column variables via the ColSideColors argument. Another useful addition would be to add a color legend for our new category labels. The code for this particular example would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;par(lend = 1)           # square line ends for the color legend
legend(&amp;quot;topright&amp;quot;,      # location of the legend on the heatmap plot
    legend = c(&amp;quot;category1&amp;quot;, &amp;quot;category2&amp;quot;, &amp;quot;category3&amp;quot;), # category labels
    col = c(&amp;quot;gray&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;black&amp;quot;),  # color key
    lty= 1,             # line style
    lwd = 10            # line width
)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="The figure below shows how our modified heatmap would look like after we applied row categorization and provided a color legend:" src="/images/heatmaps_in_r_categorizing.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/mohcinemadkour/RPlots/tree/master/heatmaps"&gt;Check out the code source in githup&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;About ggplot2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Developed by Hadley Wickham in 2005.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implements the graphics scheme described in the book &lt;em&gt;The Grammar of Graphics&lt;/em&gt; by Leland Wilkinson.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Does not create interactive or 3D graphics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;~~As of February 2014, officially in maintenance mode, meaning no new features will be added.~~ ggplot2 2.0 released in December 2015 with over 100 fixes and improvements. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Grammar of Graphics&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;Grammar of Graphics&lt;/em&gt; boiled down to 5 bullets, courtesy of Wickham (2016, p. 4):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a statistical graphic is a mapping from data to &lt;strong&gt;aes&lt;/strong&gt;thetic attributes (location, color, shape, size) of &lt;strong&gt;geom&lt;/strong&gt;etric objects (points, lines, bars). &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the geometric objects are drawn in a specific &lt;strong&gt;coord&lt;/strong&gt;inate system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;scale&lt;/strong&gt;s control the mapping from data to aesthetics and provide tools to read the plot (ie, axes and legends).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the plot may also contain &lt;strong&gt;stat&lt;/strong&gt;istical transformations of the data (means, medians, bins of data, trend lines).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;facet&lt;/strong&gt;ing can be used to generate the same plot for different subsets of the data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Grammar of Graphics - illustration&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt text" src="ggbasics.jpg"&gt;&lt;/p&gt;
&lt;p&gt;www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf&lt;/p&gt;
&lt;h2&gt;Basic ggplot2 syntax&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Specify data, aesthetics and geometric shapes&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;&lt;code&gt;ggplot(data, aes(x=, y=, color=, shape=, size=)) +&lt;/code&gt; &lt;br&gt;
&lt;code&gt;geom_point()&lt;/code&gt;, or &lt;code&gt;geom_histogram()&lt;/code&gt;, or &lt;code&gt;geom_boxplot()&lt;/code&gt;, etc.   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This combination is very effective for exploratory graphs. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The data must be a data frame.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;aes()&lt;/code&gt; function maps columns of the data frame to aesthetic properties of geometric shapes to be plotted.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ggplot()&lt;/code&gt; defines the plot; the &lt;code&gt;geoms&lt;/code&gt; show the data; layers are added with &lt;code&gt;+&lt;/code&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some examples should make this clear&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The iris data set&lt;/h2&gt;
&lt;p&gt;This is a famous data set from 1936, courtesy of Sir Ronald Fisher, that comes with R and is excellent for demonstrating ggplot2.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;str&lt;span class="p"&gt;(&lt;/span&gt;iris&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;An iris&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt text" src="iris_petal_sepal.jpg"&gt;&lt;/p&gt;
&lt;p&gt;sebastianraschka.com/Images_old/2014_python_lda/&lt;/p&gt;
&lt;h2&gt;ggplot2 example - scatter plot coded by species&lt;/h2&gt;
&lt;p&gt;```{r, message=FALSE, fig.height=3, fig.width=6}
library(ggplot2) # once per session
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + geom_point() &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## ggplot2 example - scatter plot coded by species

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species, shape=Species)) + 
  geom_point() 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;ggplot2 example - scatter plot coded by species, size by Petal.Length&lt;/h2&gt;
&lt;p&gt;```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color = Species, size = Petal.Length)) + 
  geom_point() &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## ggplot2 example - add multiple geoms (points and smooth line)

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + 
  geom_point() + geom_smooth(method=&amp;quot;lm&amp;quot;)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;ggplot2 example - boxplot (statistical transformation)&lt;/h2&gt;
&lt;p&gt;```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Species, y = Sepal.Width)) + 
  geom_boxplot() &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## Moving beyond `ggplot` + `geoms`

* A natural next step in exploratory graphing is to create plots of subsets of data. These are called facets in ggplot2.

* Use `facet_wrap()` if you want to facet by one variable and have `ggplot2` control the layout. Example:   

     + `+ facet_wrap( ~ var)`

- Use `facet_grid()` if you want to facet by one and/or two variables and control layout yourself.     

Examples:    
+ `facet_grid(. ~ var1)` - facets in columns   
+ `facet_grid(var1 ~ .)` - facets in rows   
+ `facet_grid(var1 ~ var2)` - facets in rows and columns   



## ggplot2 example - `facet_wrap` (common scales)
```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width)) + 
  geom_point() + facet_wrap(~ Species)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;ggplot2 example - &lt;code&gt;facet_wrap&lt;/code&gt; (free x scales)&lt;/h2&gt;
&lt;p&gt;```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width)) + 
  geom_point() + facet_wrap(~ Species, scales = "free_x")&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## ggplot2 example - `facet_grid` (histograms)

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width)) +
  geom_histogram() + facet_grid(Species ~ .) 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Customizing scales&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Scales control the mapping from data to aesthetics and provide tools to read the plot (ie, axes and legends).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Every aesthetic has a default scale. To modify a scale, use a &lt;code&gt;scale&lt;/code&gt; function. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All scale functions have a common naming scheme:
&lt;code&gt;scale&lt;/code&gt; &lt;code&gt;_&lt;/code&gt; name of aesthetic &lt;code&gt;_&lt;/code&gt; name of scale&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Examples: &lt;code&gt;scale_y_continuous&lt;/code&gt;, &lt;code&gt;scale_color_discrete&lt;/code&gt;, &lt;code&gt;scale_fill_manual&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Heads up: The documentation for &lt;code&gt;ggplot2&lt;/code&gt; scale functions will frequently use functions from the &lt;code&gt;scales&lt;/code&gt; package (also by Wickham)!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ggplot2 example - update scale for y-axis&lt;/h2&gt;
&lt;p&gt;```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + geom_point() +
  scale_y_continuous(limits=c(0,5), breaks=seq(0,5,0.5))&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## ggplot2 example - update scale for color

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + geom_point() +
  scale_color_manual(name=&amp;quot;Iris Species&amp;quot;, 
                     values=c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;black&amp;quot;))
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;stat functions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;All &lt;code&gt;geoms&lt;/code&gt; perform a default statistical transformation. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For example, &lt;code&gt;geom_histogram()&lt;/code&gt; bins the data before plotting. &lt;code&gt;geom_smooth()&lt;/code&gt; fits a line through the data according to a specified method.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In some cases the transformation is the "identity", which just means plot the raw data. For example, &lt;code&gt;geom_point()&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;These transformations are done by &lt;code&gt;stat&lt;/code&gt; functions. The naming scheme is &lt;code&gt;stat_&lt;/code&gt; followed by the name of the transformation. For example, &lt;code&gt;stat_bin&lt;/code&gt;, &lt;code&gt;stat_smooth&lt;/code&gt;, &lt;code&gt;stat_boxplot&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Every geom has a default stat, every stat has a default geom.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ggplot2 example - geom using default stat&lt;/h2&gt;
&lt;p&gt;```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) + 
  geom_point() + geom_smooth() &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## ggplot2 example - stat using default geom

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) + 
  geom_point() + stat_smooth() 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Why should I care about &lt;code&gt;stat&lt;/code&gt; versus &lt;code&gt;geom&lt;/code&gt;?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The stat and geom functions can use each other's arguments.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When consulting the documentation for a particular &lt;code&gt;geom&lt;/code&gt; you'll notice there is also documentation for an associated &lt;code&gt;stat&lt;/code&gt;, and vice versa. (an exception is &lt;code&gt;geom_point&lt;/code&gt; and &lt;code&gt;stat_identity&lt;/code&gt;.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Understanding how geoms and statistical transformations work together in ggplot2 can help you master the syntax faster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Update themes and labels&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The default ggplot2 theme is excellent. It follows the advice of several landmark papers regarding statistics and visual perception. (Wickham 2016, p. 176)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;However you can change the theme using ggplot2's themeing system. To date, there are seven built-in themes: &lt;code&gt;theme_gray&lt;/code&gt; (&lt;em&gt;default&lt;/em&gt;), &lt;code&gt;theme_bw&lt;/code&gt;, &lt;code&gt;theme_linedraw&lt;/code&gt;, &lt;code&gt;theme_light&lt;/code&gt;, &lt;code&gt;theme_dark&lt;/code&gt;, &lt;code&gt;theme_minimal&lt;/code&gt;, &lt;code&gt;theme_classic&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can also update axis labels and titles using the &lt;code&gt;labs&lt;/code&gt; function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ggplot2 example - update labels&lt;/h2&gt;
&lt;p&gt;```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 color=Species)) + geom_point() +
  labs(title="Sepal vs. Petal", 
       x="Petal Width (cm)", y="Sepal Width (cm)") &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;## ggplot2 example - change theme

```{r, message=FALSE, fig.height=3, fig.width=6}
ggplot(iris, aes(x = Petal.Width, y = Sepal.Width, 
                 shape=Species)) + geom_point() +
  theme_bw()
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;ggplot2 - some tips&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Can do a lot with &lt;code&gt;ggplot(data, aes()) + geom&lt;/code&gt;!&lt;/li&gt;
&lt;li&gt;Data must be a data frame (not a matrix or collection of vectors)&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;ggplot2&lt;/code&gt; documentation has many good examples&lt;/li&gt;
&lt;li&gt;Prepare to invest some time if you want master ggplot2; &lt;a href="https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf"&gt;the RStudio ggplot2 cheat sheet&lt;/a&gt; can help.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's go to R!&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wickham, H. (2016), &lt;em&gt;ggplot2: Elegant Graphics for Data Analysis&lt;/em&gt; (2nd ed), Springer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wickham, H. (2010), "A Layered Grammar of Graphics", &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt;, Volume 19, Number 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chang, W. (2013), &lt;em&gt;R Graphics Cookbook&lt;/em&gt;, O'Reilly.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ggplot2 cheat sheet&lt;/strong&gt;  &lt;br&gt;
https://www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cookbook for R - Graphs&lt;/strong&gt; &lt;br&gt;
http://www.cookbook-r.com/Graphs/&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Official ggplot2 web site&lt;/strong&gt;  &lt;br&gt;
http://ggplot2.org/&lt;/p&gt;
&lt;h2&gt;StatLab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Thanks for coming today!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For help and advice with your data analysis, contact StatLab to set up an appointment: statlab@virginia.edu&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sign up for more workshops or see past workshops:
http://data.library.virginia.edu/training/&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Register for the Research Data Services newsletter to stay up-to-date on StatLab events and resources: http://data.library.virginia.edu/newsletters/&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="Data Visualization"></category><category term="R"></category></entry><entry><title>Install and cofigure Spark, Kafka, Cassandra, Zookeper</title><link href="https://mohcinemadkour.github.io/posts/2017/01/Apache%20Spark%20Zookeper%20Kafka%20Cassandra/" rel="alternate"></link><published>2017-01-12T16:00:00-05:00</published><updated>2017-01-12T16:00:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2017-01-12:/posts/2017/01/Apache Spark Zookeper Kafka Cassandra/</id><summary type="html">&lt;h1&gt;Anaconda&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;export PATH=&amp;quot;/home/mohcine/anaconda2/bin:$PATH&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Spark:&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;snotebook&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nx"&gt;SPARK_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;/home/mohcine/Sofwares/spark-2.2.0-bin-hadoop2.7&lt;/span&gt;
&lt;span class="kr"&gt;export&lt;/span&gt; &lt;span class="nx"&gt;PYSPARK_DRIVER_PYTHON&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;jupyter&amp;quot;&lt;/span&gt;
&lt;span class="kr"&gt;export&lt;/span&gt; &lt;span class="nx"&gt;PYSPARK_DRIVER_PYTHON_OPTS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;notebook&amp;quot;&lt;/span&gt;
&lt;span class="nx"&gt;$SPARK_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;pyspark&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nx"&gt;master&lt;/span&gt; &lt;span class="nx"&gt;local&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Zookeper:&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install zookeeperd,   
Check if it is running
netstat -ant | grep :2181
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Kafka:&lt;/h1&gt;
&lt;p&gt;download kafka on the …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Anaconda&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;export PATH=&amp;quot;/home/mohcine/anaconda2/bin:$PATH&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Spark:&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;snotebook&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nx"&gt;SPARK_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;/home/mohcine/Sofwares/spark-2.2.0-bin-hadoop2.7&lt;/span&gt;
&lt;span class="kr"&gt;export&lt;/span&gt; &lt;span class="nx"&gt;PYSPARK_DRIVER_PYTHON&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;jupyter&amp;quot;&lt;/span&gt;
&lt;span class="kr"&gt;export&lt;/span&gt; &lt;span class="nx"&gt;PYSPARK_DRIVER_PYTHON_OPTS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;notebook&amp;quot;&lt;/span&gt;
&lt;span class="nx"&gt;$SPARK_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;pyspark&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nx"&gt;master&lt;/span&gt; &lt;span class="nx"&gt;local&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Zookeper:&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install zookeeperd,   
Check if it is running
netstat -ant | grep :2181
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Kafka:&lt;/h1&gt;
&lt;p&gt;download kafka on the location : /opt&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Start server: 
sudo  /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Start the Kafka server as a background process:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo  /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties  /tmp/kafka.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Testing server&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo /opt/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1  --partitions 1 --topic testing
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ask Zookeeper to list available topics on Apache Kafka&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo /opt/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Publish a sample messages to Apache Kafka topic called testing by using the following producer command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testing
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Use consumer command to check for messages on Apache Kafka Topic called testing&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo /opt/kafka/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic testing --from-beginning
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Cassandra:&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;export CASSANDRA_HOME=/opt/apache-cassandra-3.11.1
export PATH=$CASSANDRA_HOME/bin:$PATH
    &amp;gt;cd $CASSANDRA_HOME/bin
    &amp;gt;casandra
    &amp;gt;cqlsh
&lt;/pre&gt;&lt;/div&gt;</content><category term="Apache"></category></entry><entry><title>Quote of the Day</title><link href="https://mohcinemadkour.github.io/posts/2016/02/Quote%20of%20the%20Day/" rel="alternate"></link><published>2016-02-18T16:00:00-05:00</published><updated>2016-02-18T16:00:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2016-02-18:/posts/2016/02/Quote of the Day/</id><summary type="html">&lt;p&gt;Selfish behaviors are reward driven and innate, wired deeply into the survival mechanisms of the primitive brain, and when consistently reinforced, they will run away to greed, with its associated craving for money, food, or power. On the other hand, the self restraint and the empathy for others that are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Selfish behaviors are reward driven and innate, wired deeply into the survival mechanisms of the primitive brain, and when consistently reinforced, they will run away to greed, with its associated craving for money, food, or power. On the other hand, the self restraint and the empathy for others that are so important in fostering physical and mental health are learned behaviors —largely functions of the new human cortex and thus culturally dependent. These social behaviors are fragile and learned by imitations much as we learn language.
&lt;em&gt;~Peter Whybrow, American Mania&lt;/em&gt;&lt;/p&gt;</content><category term="quotes"></category></entry><entry><title>RDF Parser API</title><link href="https://mohcinemadkour.github.io/posts/2016/02/RDF%20Triple%20API/" rel="alternate"></link><published>2016-02-09T16:00:00-05:00</published><updated>2016-02-09T16:00:00-05:00</updated><author><name>Mohcine madkour</name></author><id>tag:mohcinemadkour.github.io,2016-02-09:/posts/2016/02/RDF Triple API/</id><summary type="html">&lt;h1&gt;RDF-Triple-API&lt;/h1&gt;
&lt;p&gt;A simple API for extracting the RDF triple (subject, predicate, object) of any sentence. The parsed sentence is also returned in addition to the triple.&lt;/p&gt;
&lt;p&gt;The algorithm implemented is taken from [this paper] (http://ailab.ijs.si/dunja/SiKDD2007/Papers/Rusu_Trippels.pdf) by Delia Rusu.&lt;/p&gt;
&lt;p&gt;The sentence is parsed …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;RDF-Triple-API&lt;/h1&gt;
&lt;p&gt;A simple API for extracting the RDF triple (subject, predicate, object) of any sentence. The parsed sentence is also returned in addition to the triple.&lt;/p&gt;
&lt;p&gt;The algorithm implemented is taken from [this paper] (http://ailab.ijs.si/dunja/SiKDD2007/Papers/Rusu_Trippels.pdf) by Delia Rusu.&lt;/p&gt;
&lt;p&gt;The sentence is parsed using the [stanford parser] (http://nlp.stanford.edu/software/lex-parser.shtml)&lt;/p&gt;
&lt;p&gt;The endpoint for the api is http://www.newventify.com/rdf and has url parameter &lt;code&gt;sentence&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;A complete request would look like the following: &lt;a href="http://www.newventify.com/rdf?sentence=The man stood next to the refrigerator"&gt;http://www.newventify.com/rdf?sentence=The man stood next to the refrigerator&lt;/a&gt; and will return&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{
  "object": {
    "POS": "NN", 
    "Tree Attributes": [], 
    "Word Attributes": [
      [
        "the", 
        "DT"
      ]
    ], 
    "word": "refrigerator"
  }, 
  "parse_tree": "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['The']), Tree('NN', ['man'])]), Tree('VP', [Tree('VBD', ['stood']), Tree('ADVP', [Tree('JJ', ['next'])]), Tree('PP', [Tree('TO', ['to']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['refrigerator'])])])])])])", 
  "predicate": {
    "POS": "VB", 
    "Tree Attributes": [
      "Tree('ADVP', [Tree('JJ', ['next'])])"
    ], 
    "Word Attributes": [], 
    "word": "stood"
  }, 
  "rdf": [
    "man", 
    "stood", 
    "refrigerator"
  ], 
  "sentence": "The man stood next to the refrigerator", 
  "subject": {
    "POS": "NN", 
    "Tree Attributes": [], 
    "Word Attributes": [
      [
        "The", 
        "DT"
      ]
    ], 
    "word": "man"
  }
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/mohcinemadkour/RDF-Triple-API/blob/master/rdf_triple.py"&gt;Check out the source code here&lt;/a&gt;&lt;/p&gt;</content><category term="Semantic Wev"></category></entry><entry><title>Clustering of vaccine temporal data in timeline</title><link href="https://mohcinemadkour.github.io/posts/2015/02/Temporal%20clinical%20events%20clustering/" rel="alternate"></link><published>2015-02-18T16:00:00-05:00</published><updated>2015-02-18T16:00:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2015-02-18:/posts/2015/02/Temporal clinical events clustering/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Events in clinical narratives are naturally associated with medical trials, including surgery, vaccination, lab test, medication, medical procedure, and diagnosis. They are interrelated with many temporal relations. The grouping of medical events onto temporal clusters is a key to applications such as longitudinal studies, clinical question answering, and information retrieval …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Events in clinical narratives are naturally associated with medical trials, including surgery, vaccination, lab test, medication, medical procedure, and diagnosis. They are interrelated with many temporal relations. The grouping of medical events onto temporal clusters is a key to applications such as longitudinal studies, clinical question answering, and information retrieval. However, it is difficult to define clinical event quantitatively or consistently in coarse time-bins (e.g. before vaccination or after admission). In this article, I developed the K-means classifier to enable labeling a sequence of medical events with predefined time-bins. The features set is based solely on temporal distance similarity between boundaries of events. The result of the solution is integrated with the &lt;a href="https://timeline.knightlab.com/"&gt;KnightLab timeline JS tool&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this article I investigate the task of tagging a sequence of events using a clustering algorithm. For this purpose I assume that each medical note can be associated with a predefined set of coarse of times that I refer to as time bins. For our example of VAERS note, the potential time-bins are: “before vaccination”, “soon after vaccination”, and “way after vaccination”. The time-bin “before vaccination” is intended to capture past medical history of the patient including the medical state of the patient on time of vaccination; “soon after vaccination” captures medical events that occurred immediately after the vaccination; and “way after vaccination” captures medical events that occurred after an extended duration from the vaccination. The issue in clustering events in predefined time-bins is that the time duration of each timebin varies based on the patient. For instance, the coarse of time “soon
after vaccination” could be the first few hours after or a few days
after depending on the general conditions. For that I consider that
related events happen in relatively close proximity of time. I use a
non-hierarchical clustering to classify the set of events. I consider
the temporal distance between events as the measure of similarity
between events of same clusters and dissimilarity between events of
different clusters.&lt;/p&gt;
&lt;h2&gt;K-Means Clustering&lt;/h2&gt;
&lt;p&gt;K-means is one of the simplest algorithms for solving the
clustering problem. Clustering is an unsupervised learning
problem whereby I aim to group subsets of entities with one
another based on a temporal distance similarity. The idea is to define
k centroids for the k assumed clusters and to associate each point
belonging to a given data set to the nearest center. A point represents
the time instant of the event or the center of interval if its time
interval event. When no point is pending, the first step is completed
and an early group age is done. At this point I re-calculate k new
centroids as barycenter of the clusters resulting from the previous
step. After I have these k new centroids, I re-bind the same data
set points to their nearest new center. A loop has been generated. As
a result of this loop the k centers change their location step by step
until no more changes are done or in other words centres do not
move any more.&lt;/p&gt;
&lt;h2&gt;Timeline View&lt;/h2&gt;
&lt;p&gt;For data that relates to temporal events, the Timeline Widget adds an interesting dimension to your exhibit.&lt;/p&gt;
&lt;p&gt;The nobelists.js data file lists the years when the Nobelists won their prizes, so I can plot each one on a time line. To display timelines in Exhibit you need to include a separate utility, the Timeline widget. The Timeline widget is a bit bulky, so Exhibit doesn't include it by default. You have to include the time extension to Exhibit. Open the file nobelists.html, find the reference to exhibit-api.js and add the following script element after it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;data-ex-role=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;view&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    data-ex-view-class=&amp;quot;Timeline&amp;quot;  
    data-ex-start=&amp;quot;.time&amp;quot; 
    data-ex-end=&amp;quot;.time2&amp;quot; 
    data-ex-color-key=&amp;quot;.cluster&amp;quot; 
    data-ex-top-band-unit=&amp;quot;month&amp;quot; 
    data-ex-bottom-band-unit=&amp;quot;year&amp;quot; 
    data-ex-top-band-pixels-per-unit=&amp;quot;90&amp;quot; 
    data-ex-bottom-band-pixels-per-unit=&amp;quot;400&amp;quot;
        &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;data-ex-role=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lens&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;lt;span&lt;/span&gt; &lt;span class="na"&gt;data-ex-content=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.hour&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/span&amp;gt;&lt;/span&gt;: 
                &lt;span class="nt"&gt;&amp;lt;span&lt;/span&gt; &lt;span class="na"&gt;data-ex-content=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.label&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Visualization&lt;/h2&gt;
&lt;p&gt;I visualize the results using the Exhibit dashboard
solution. The timeline dashboard enables intuitive cluster analysis
by user interactions. Also our visualization allows summarizing by
the various types of events information. &lt;img alt="Here is a screeshot of the visualization" src="/images/timeline.png"&gt; Check out the &lt;a href="http://htmlpreview.github.io/?https://github.com/mohcinemadkour/Event-Timeline/blob/master/index.html"&gt;visualization of clustered events&lt;/a&gt;&lt;/p&gt;</content><category term="K-means"></category><category term="data visualization"></category><category term="VAERS Reports"></category></entry></feed>