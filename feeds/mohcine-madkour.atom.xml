<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Mohcine Madkour - Mohcine Madkour</title><link href="https://mohcinemadkour.github.io/" rel="alternate"></link><link href="https://mohcinemadkour.github.io/feeds/mohcine-madkour.atom.xml" rel="self"></link><id>https://mohcinemadkour.github.io/</id><updated>2018-06-14T13:01:00-04:00</updated><entry><title>Deep Reinforcement Learning</title><link href="https://mohcinemadkour.github.io/posts/2018/06/Deep%20Reinforcement%20Learning/" rel="alternate"></link><published>2018-06-14T13:01:00-04:00</published><updated>2018-06-14T13:01:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-06-14:/posts/2018/06/Deep Reinforcement Learning/</id><summary type="html">&lt;p&gt;Today, we will explore Reinforcement Learning – a goal-oriented learning based on interaction with environment. Reinforcement Learning is said to be the hope of true artificial intelligence. And it is rightly said so, because the potential that Reinforcement Learning possesses is immense.
Reinforcement learning refers to goal-oriented algorithms, which learn how …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today, we will explore Reinforcement Learning – a goal-oriented learning based on interaction with environment. Reinforcement Learning is said to be the hope of true artificial intelligence. And it is rightly said so, because the potential that Reinforcement Learning possesses is immense.
Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps; for example, maximize the points won in a game over many moves. They can start from a blank slate, and under the right conditions they achieve superhuman performance. Like a child incentivized by spankings and candy, these algorithms are penalized when they make the wrong decisions and rewarded when they make the right ones – this is reinforcement.&lt;/p&gt;
&lt;h1&gt;Introduction to reinforcement learning&lt;/h1&gt;
&lt;h2&gt;The learning paradigm&lt;/h2&gt;
&lt;p&gt;The RL is kind of learning by doing, with no supervisor, but only a reward signal. The Feedback is  delayed and not instantaneous. In this kind of learning the time really matters and the agent’s actions affect the subsequent data it receives
A reward Rt is a scalar feedback signal, It indicates how well agent is doing at step t. The agent’s job is to maximise cumulative reward. Reinforcement learning is based on the reward hypothesis, which states that all goals can be described by the &lt;strong&gt;Maximisation of expected cumulative reward&lt;/strong&gt;. 
Examples of Rewards can be +ve reward for following desired trajectory and −ve reward for crashing.
The goal is to select actions to maximise total future reward. The actions may have long term consequences, and the reward may be delayed. Sometimes It may be better to sacrifice immediate reward to gain more long-term reward
Examples are numerous. For example a financial investment (may take months to mature), and Refuelling a helicopter (might prevent a crash in several hours)
Two fundamental problems in sequential decision making 
&lt;img alt="supervised vs rl" src="/images/dsrl.png"&gt;&lt;/p&gt;
&lt;h2&gt;Sequential Decision Making&lt;/h2&gt;
&lt;p&gt;The reinforcement learning id a Sequential Decision Making process. In general there is two types of environmnets: Fully Observable Environments which is recommended for Markov decision process in where the Agent state about the environmnet is identical with the environment state and with the information state; and the Partially Observable Environments in which the Partially Markov decision process can be applied. In this environment, the agent indirectly observes environment. The Agent must construct its own state representation whcih includes complete history, beliefs of environment state. The Recurrent neural network can be used in this case.&lt;/p&gt;
&lt;h2&gt;Components of an RL Agent&lt;/h2&gt;
&lt;p&gt;An RL agent may include one or more of these components: Policy: agent’s behaviour function, Value function: how good is each state and/or action, and Model: agent’s representation of the environment. The &lt;strong&gt;Policy&lt;/strong&gt; is the agent’s behaviour. It is a map from state to action, e.g. We have two types : Deterministic policy: a = π(s) and Stochastic policy: π(a|s) = P[At = a|St = s]. The &lt;strong&gt;Value Function&lt;/strong&gt; is a prediction of future reward. It is used to evaluate the goodness/badness of states And therefore to select between actions, e.g.]:&lt;strong&gt;vπ(s)&lt;/strong&gt; = Eπ [Rt+1 + γRt+2 + γ2 Rt+3 + ... | St = s]. &lt;strong&gt;The model&lt;/strong&gt; predicts what the environment will do next. The P predicts the next state, and the R predicts the next (immediate) reward.
Pss'= P[St+1 = s | St = s, At = a], Ras = E [Rt+1 |St = s, At = a]&lt;/p&gt;
&lt;h1&gt;Types of reinforcement learning algorithms&lt;/h1&gt;
&lt;p&gt;RL algorithms that satisfy the &lt;em&gt;Markov property&lt;/em&gt; are called the &lt;em&gt;Markov Decision Processes (MDP)&lt;/em&gt;. The Markov property assumes that the current state is independent of the path that leads to that particular state. 
&lt;img alt="Markovian environments and Non Markovian environments" src="/images/markov.png"&gt;
Hence, in Markovian problems a memoryless property of a stochastic process is assumed. In practice it means that the probability distribution of the future states depends only on the current state and not on the sequence of events that preceded. This is a useful property for stochastic processes as it allows for analysing the future by setting the present
&lt;img alt="State Transition from state s to state s'" src="/images/fig1_rl.png"&gt;
An MDPs consist of state (s), action (a) sets and given any state and action to be taken, a transition probability function of each possible next state (s’) illustrated in figure 1. In addition, each taken action to arrive to the next state is rewarded giving each of all possible actions a reward value
depending on the type of action. Each visited state is accredited by a value given to it according to a &lt;strong&gt;value function V(s)&lt;/strong&gt; which represents how good it is for an agent to be in a given state. The value of a state s under a policy π is then denoted as Vπ(s) which in theory denotes the expected return when starting in state s and following a sequence of states to be visited according to the order defined in π thereafter. When this theorem is applied to a model-free control problem, the &lt;strong&gt;state-value function&lt;/strong&gt; may
not suffice as it does not show what action was taken for the state value to be acquired. Therefore, a similar function has been introduced representing an estimation of the value of each possible action in a state. This is described as the &lt;strong&gt;action-value function&lt;/strong&gt; for policy π Qπ(s,a). Figure 2 illustrates an example of the relationship between the action-value function and the state-value function. In 2.a. the action-values are shown for each direction of the propagation, North, East, South, and West respectively. &lt;strong&gt;The state value function represents then the highest action-value possible in that state which is the action North in the example&lt;/strong&gt;.&lt;img alt="1a 1b" src="/images/rl_fig2.png"&gt;&lt;/p&gt;
&lt;p&gt;The optimal policy is denoted as the superscript asterisk to the action-value-function Q(s,a) and state value-function V(s). Formally, the optimal value function is then given by:
&lt;img alt="Eq1" src="/images/eq1.png"&gt;
Where Q*(s,a) is given by:
&lt;img alt="Eq2" src="/images/eq2.png"&gt;
Herein, T(s, a, s’) is the transition probability to the next state s’ given state s and action a. γ presents the discount factor which is usually smaller than 1 and is used to discount for earlier values in order to assign
higher values for sooner rewards. This is necessary to converge the algorithm.
Substituting equation 3 in 2 gives the Bellman equation:
&lt;img alt="Eq3" src="/images/eq3.png"&gt;
These updates will be appended to the states that were visited resulting (after a significant number of iterations) in state values showing how good to be in that state. In order to be able to choose between the states to select a policy, &lt;strong&gt;as many states as possible need to be visited&lt;/strong&gt; in order to converge to an accurate estimation of the state value. Acquiring the highest reward depends on these visited states and the reward accumulated. However, in order to discover more states and potentially higher rewards, the agent needs to take actions it has never taken before. This is referred to as the &lt;strong&gt;trade-off between exploitation and exploration&lt;/strong&gt;. This trade-off could be achieved by setting a variable denoted as Epsilon (ε) which gives the extent of exploration versus exploitation. A fully exploiting policy is referred to as an
epsilon-greedy policy and holds a value of 0 for ε. Correspondingly, a fully exploring policy gives a value of 1 to ε and is referred to as an epsilon-soft policy. The learning can therefore be tuned between these two extremes in order to allow for convergence towards an optimal value by occasionally exploring new states and actions.&lt;/p&gt;
&lt;h2&gt;Categorisies of RL agents&lt;/h2&gt;
&lt;p&gt;Reinforcement learning is like trial-and-error learning. The agent should discover a good policy from its experiences of the environment and Without losing too much reward along the way. The &lt;strong&gt;Exploration&lt;/strong&gt; finds more information about the environment. The &lt;strong&gt;Exploitation&lt;/strong&gt; exploits known information to maximise reward. It is usually important to explore as well as exploit.
An agent can evaluate the future Given a policy (&lt;strong&gt;Prediction&lt;/strong&gt;) or optimise the future and find the best policy (&lt;strong&gt;Control&lt;/strong&gt;)
There is five types of agents: &lt;strong&gt;Value Based&lt;/strong&gt; No Policy (Implicit)+ Value Function, &lt;strong&gt;Policy Based&lt;/strong&gt;: Policy + No Value Function, &lt;strong&gt;Actor Critic&lt;/strong&gt;: Policy+ Value Function, &lt;strong&gt;Model Free&lt;/strong&gt;: Policy and/or Value Function+ No Model
, &lt;strong&gt;Model Based&lt;/strong&gt;: Policy and/or Value Function+ Model
&lt;img alt="RL Agents" src="/images/RLAgents.png"&gt;&lt;/p&gt;
&lt;h2&gt;Classes of RL algorithms&lt;/h2&gt;
&lt;p&gt;RL knows three fundamental classes of methods for solving these learning problems: &lt;strong&gt;Dynamic Programming (DP)&lt;/strong&gt;, &lt;strong&gt;Monte Carlo methods&lt;/strong&gt;,  &lt;strong&gt;Temporal-difference learning&lt;/strong&gt;
Dependent on the problem at stake, each of these methods could be more suitable than the other. &lt;strong&gt;DP&lt;/strong&gt; methods are model-based and require therefore a complete and accurate model of the environment i.e. all the aforementioned functions of the environment need to be known to initiate learning. However,
the environment is not always defined prior to the learning process which poses a challenge to this method. This is where the two other &lt;strong&gt;model-free&lt;/strong&gt; learning methods come in handy. The &lt;strong&gt;Monte Carlo&lt;/strong&gt; algorithms only require an experience sample such as a data set in which the states, actions and rewards
of the (simulated) interaction with the environment. In comparison with DP methods, no model of the &lt;strong&gt;transition probability function&lt;/strong&gt; is required and neither the &lt;strong&gt;dynamics&lt;/strong&gt; of the environment. Monte Carlo algorithms solve the RL problem by &lt;strong&gt;averaging&lt;/strong&gt; sample return of each &lt;strong&gt;episode&lt;/strong&gt;. Only after the termination of an episode, that the value &lt;strong&gt;estimation&lt;/strong&gt; and &lt;strong&gt;policies&lt;/strong&gt; are updated. Hence, it is based on averages of complete returns of the value functions of each state. This class of algorithms does not exploit Markov property described before and is therefore more efficient in &lt;strong&gt;non-Markovian&lt;/strong&gt; environments. On the other hand, &lt;strong&gt;Temporal-Difference methods&lt;/strong&gt; do also not require a model of the environment but are like DP solving for incrementing &lt;strong&gt;step-by-step&lt;/strong&gt; rather than &lt;strong&gt;episode-by-episode&lt;/strong&gt;. Hence, TD methods exploit the &lt;strong&gt;Markovian property&lt;/strong&gt; and perform usually better in Markovian environments.
The choice between these two classes of model-free RL algorithms very much depends on the type of data set available. For continuous processes in which there are no fixed episodic transitions, &lt;strong&gt;Monte Carlo&lt;/strong&gt; methods may not be the optimal solution as they average the return only at the end of each episode. &lt;strong&gt;TD&lt;/strong&gt; algorithms might then be a better solution as they assign a reward incrementally over each state. This allows them to converge faster towards an optimal policy for large data sets with a large number state spaces.&lt;/p&gt;
&lt;h2&gt;Temporal-difference learning: On-policy and off-policy TD control&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TD&lt;/strong&gt; algorithms comprise two important RL classes of algorithms divided in &lt;strong&gt;Off-Policy&lt;/strong&gt; and &lt;strong&gt;On-Policy&lt;/strong&gt; TD control algorithm classes. The difference between the two lays in the policy that is learned from the simulation or set of experiences (data). &lt;strong&gt;On-Policy TD control&lt;/strong&gt; algorithms are often referred to as &lt;strong&gt;SARSA algorithms&lt;/strong&gt; in which the letters refer to the sequence of State, Action, Reward associated with the state transition, next State, next Action. This sequence is followed in each &lt;strong&gt;time-step&lt;/strong&gt; and is used to update the &lt;strong&gt;action-value&lt;/strong&gt; of these two states according:
&lt;img alt="Eq4" src="/images/eq4.png"&gt;
Here, α represents the step-size parameter which functions as the exponentially moving average parameter. It is especially useful for &lt;strong&gt;non-stationary&lt;/strong&gt; environments for weighting recent rewards more heavily than long-past ones. This could also be illustrated by rearranging the above equation to:
&lt;img alt="Eq5" src="/images/eq5.png"&gt;
If α is a number smaller than one for non-stationary environments which indicates that recent updates weight more than previous ones. This transition happens after every nonterminal state. The Q (st+1 ,at+1 ) components of every terminal state is defined as zero. Hence, every terminal state has an update value of 0. &lt;strong&gt;SARSA&lt;/strong&gt; is called an on-policy algorithm because it updates the &lt;strong&gt;action-value-function&lt;/strong&gt; according to the &lt;strong&gt;policy&lt;/strong&gt; it is taking in every &lt;strong&gt;step&lt;/strong&gt;. Therefore, it takes the epsilon-policy into account in order to arrive the optimal policy for a certain problem. &lt;strong&gt;Off-policy&lt;/strong&gt; algorithms approximate the best possible policy even when that policy is not taken by the agent. Hence, &lt;strong&gt;Off-Policy&lt;/strong&gt; algorithms base the update of the &lt;strong&gt;state action-value&lt;/strong&gt; function on the assumption of &lt;strong&gt;optimal behaviour&lt;/strong&gt; without taking into account the &lt;strong&gt;epsilon policy&lt;/strong&gt; (the chance to take a negative action). The cliff figure shows a suitable example given by Sutton and Barto (1998) and which illustrates the policy outcome differences between the two types of TD algorithms &lt;img alt="Cliff)" src="/images/cliff.png"&gt;. The cliff represents states with high negative reward. Since &lt;strong&gt;SARSA&lt;/strong&gt; takes the &lt;strong&gt;epsilon policy&lt;/strong&gt; into account, it learns that at some instances a non-optimal action will be taken which results in a high negative reward. Hence, it will learn to take the safe path rather than the optimal path. &lt;strong&gt;Q-learning algorithms&lt;/strong&gt; on the other hand, will take the optimal path by which the highest total reward could be achieved. This is because it does not take the &lt;strong&gt;epsilon probability&lt;/strong&gt; into account of taking an extremely negative action. This class of algorithms is denoted by the following equation:
&lt;img alt="Eq6" src="/images/eq6.png"&gt;
This difference will inevitably influence the suitability for the type of application. &lt;/p&gt;
&lt;h1&gt;Markov Decision Processes&lt;/h1&gt;
&lt;p&gt;Markov decision processes formally describe an environment for reinforcement learning Where the environment is fully observable, i.e. The current state completely characterises the process.
&lt;img alt="MDP" src="/images/MDP.png"&gt;
Almost all RL problems can be formalised as MDPs, e.g.Optimal control primarily deals with continuous MDPs, Partially observable problems can be converted into MDPs, Bandits are MDPs with one state
The Markov Property states that "The future is independent of the past given the present” in other ways a state St is Markov if and only if
P [S t+1 | S t ] = P [S t+1 | S 1 , ..., S t ]
The state captures all relevant information from the history and once the state is known, the history may be thrown away. i.e. The state is a sufficient statistic of the future.&lt;/p&gt;
&lt;p&gt;For a Markov state s and successor state s' , the state transition
probability is defined by Pss' = P[St+1 = s'| St = s]. The State transition matrix P defines transition probabilities from all states s to all successor states s' &lt;img alt="State Transition Matrix" src="/images/State_Transition_Matrix.png"&gt;where each row of the matrix sums to 1&lt;/p&gt;
&lt;p&gt;A Markov process is a memoryless random process, i.e. a sequence
of random states S1 , S2 , ... with the Markov property. Otherwise it is a tuple &lt;S,P&gt; with S is a (finite) set of states, P is a state transition probability matrix, Pss'= P [S t+1 = s'| St = s]
&lt;img alt="Example" src="/images/markov_process.png"&gt;&lt;/p&gt;
&lt;p&gt;A Markov reward process is a Markov chain with values.
Definition: A Markov Reward Process is a tuple &lt;S, P, R, γ&gt;
S is a finite set of states
P is a state transition probability matrix,
P ss'= P [St+1 = s'| St = s]
R is a reward function, Rs = E [Rt+1 | St = s]
γ is a discount factor, γ ∈ [0, 1]&lt;/p&gt;
&lt;p&gt;The return Gt is the total discounted reward from time-step t.
G t = Rt+1 + γRt+2 + ...
The discount γ ∈ [0, 1] is the present value of future rewards
The value of receiving reward R after k + 1 time-steps is γkR.
This values immediate reward above delayed reward.
γ close to 0 leads to ”myopic” evaluation
γ close to 1 leads to ”far-sighted” evaluation&lt;/p&gt;
&lt;p&gt;Most Markov reward and decision processes are discounted:
&lt;em&gt; Mathematically convenient to discount rewards
&lt;/em&gt; Avoids infinite returns in cyclic Markov processes
&lt;em&gt; Uncertainty about the future may not be fully represented
&lt;/em&gt; If the reward is financial, immediate rewards may earn more
interest than delayed rewards
&lt;em&gt; Animal/human behaviour shows preference for immediate
reward
&lt;/em&gt; It is sometimes possible to use undiscounted Markov reward
processes (i.e. γ = 1), e.g. if all sequences terminate.&lt;/p&gt;
&lt;p&gt;The value function v (s) gives the long-term value of state s
Definition : The state value function v (s) of an MRP is the expected return
starting from state s v (s) = E [G t | S t = s]
&lt;img alt="Value Function" src="/images/Value_Function.png"&gt;&lt;/p&gt;
&lt;h1&gt;Final Words&lt;/h1&gt;
&lt;p&gt;Reinforcement learning is extremely fun but hard topic. I am excited to learn more!&lt;/p&gt;</content><category term="Deep Reinforcement Learning"></category></entry><entry><title>Analysing Model Perfromance from Receiver Operator Characteristic (ROC) and Recall and Precision (PR) curves</title><link href="https://mohcinemadkour.github.io/posts/2018/06/Analysing%20Model%20Perfromance%20from%20ROC,%20and%20Recall%20and%20Precision%20curves/" rel="alternate"></link><published>2018-06-09T16:00:00-04:00</published><updated>2018-06-09T16:00:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2018-06-09:/posts/2018/06/Analysing Model Perfromance from ROC, and Recall and Precision curves/</id><summary type="html">&lt;p&gt;ROC and PR curves are commonly used to present results for binary decision problems in machine learning. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Each point of the ROC curve (i.e. threshold) corresponds to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;ROC and PR curves are commonly used to present results for binary decision problems in machine learning. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Each point of the ROC curve (i.e. threshold) corresponds to specific values of sensitivity and specificity. The area under the ROC curve (AUC) is a summary measure of performance that indicates whether on average a true positive is ranked higher than a false positives. If model A has higher AUC than model B, model A is performing better on average, but there still could be specific areas of the ROC space where model B is better (i.e. thresholds for which sensitivity and specificity are higher for model B than A. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. There is deep connection between ROC space and PR space, such that a curve dominates in ROC space if  and only if it dominates in PR space. The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision, x-axis, relates to a low false positive rate, and high recall, y-axis, relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).&lt;/p&gt;
&lt;h2&gt;Sensitivity (positive in disease)&lt;/h2&gt;
&lt;p&gt;Sensitivity is the ability of a test to correctly classify an individual as ′diseased′&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Sensitivity = a / a+c
= a (true positive) / a+c (true positive + false negative)
= Probability of being test positive when disease present.
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Specificity (negative in health)&lt;/h2&gt;
&lt;p&gt;The ability of a test to correctly classify an individual as disease- free is called the test′s specificity&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Specificity = d / b+d
= d (true negative) / b+d (true negative + false positive)
= Probability of being test negative when disease absent.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sensitivity and specificity are inversely proportional, meaning that as the sensitivity increases, the specificity decreases and vice versa.&lt;/p&gt;
&lt;h2&gt;Positive Predictive Value (PPV)&lt;/h2&gt;
&lt;p&gt;It is the percentage of patients with a positive test who actually have the disease. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;PPV: = a / a+b
= a (true positive) / a+b (true positive + false positive)
= Probability (patient having disease when test is positive)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Negative Predictive Value (NPV)&lt;/h2&gt;
&lt;p&gt;It is the percentage of patients with a negative test who do not have the disease.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;NPV:    =   d / c+d
=   d (true negative) / c+d (false negative + true negative)
=   Probability (patient not having disease when test is negative)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Positive and negative predictive values are influenced by the prevalence of disease in the population that is being tested. If we  test in a high prevalence setting, it is more likely that persons who test positive truly have disease than if the test is performed in a population with low prevalence. So the PPV will increase with increasing prevalence and NPV decreases with increase in prevalence.&lt;/p&gt;
&lt;h2&gt;Methods to find the ‘optimal’ threshold point&lt;/h2&gt;
&lt;p&gt;Three criteria  are  used to  find  optimal  threshold point  from  ROC  curve.  These criteria are known as points on curve closest to the (0, 1), Youden index, and minimize cost criterion. First two methods give equal weight to sensitivity and specificity and impose no ethical, cost, and no prevalence  constraints.  The  third  criterion  considers  cost  which  mainly  includes financial  cost  for  correct  and  false  diagnosis,  cost  of  discomfort  to  person  caused  by treatment, and cost of further investigation when needed.  This method is rarely used in medical literature because it is difficult to estimate the respective costs and prevalence is often difficult to assess.&lt;/p&gt;
&lt;p&gt;Youden index  is  more  commonly  used  criterion  because  this  index  reflects  the  intension  to maximize the correct classification 
rate and is easy to calculate. It maximizes the vertical distance from line of equality to the point [x, y] as shown in Figure. The x represents (1-specificity) and y represents sensitivity.  In  other  words,  the  Youden  index  J  is  the  point on the ROC  curve  which  is farthest  from  line  of  equality  (diagonal  line).  The  main  aim of  Youden  index  is  to 
maximize the difference between TPR (sensitivity) and FPR (1 –specificty) and little algebra yields J = max[sensitivity+specificty].  The  value  of  J  for  continuous  test  can  be  located  by  doing  a  search  of plausible  values  where  sum  of  sensitivity  and  specificity  can be  maximum:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;j= model_metric[&amp;#39;thres&amp;#39;].iloc[model_metric[&amp;#39;yod_index&amp;#39;].idxmax()-1]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sometimes a second cutoff that is bigger than j but less than 1 is needed. This cutoff can be used to stratify the positively predicted values to moderate and high prediction for example(needed for risk prediction stratification). This cutoff can be calculated using the accuracy measurement using the following method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cutoff2=cu.cal_cutoff2(model_metric)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    def cal_cutoff2(data):
    val=0
    for i in range(len(data)-10):
    if((abs(data[&amp;#39;acc&amp;#39;].iloc[i]-data[&amp;#39;acc&amp;#39;].iloc[i+10]))&amp;lt;0.002):
        val=data[&amp;#39;thres&amp;#39;].iloc[i]
        break
return(val)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The chart bellow illustrates the relationship between the different performance metrics ( prevalence is exluded) in an example of 4 estimators apllied on 4 -classes data&lt;img alt=" PPV and NPV  relationship" src="/images/NPV-PPV-Accracy-Youden.png"&gt;&lt;/p&gt;
&lt;h1&gt;Calculation of performance metrics&lt;/h1&gt;
&lt;p&gt;Here is the python code for the calculation of  performance metrics &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def calculate_metric&lt;span class="p"&gt;(&lt;/span&gt;outcome&lt;span class="p"&gt;,&lt;/span&gt; score&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    obser &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;outcome&lt;span class="p"&gt;))&lt;/span&gt;
    obser&lt;span class="p"&gt;[[&lt;/span&gt;i &lt;span class="kr"&gt;for&lt;/span&gt; i&lt;span class="p"&gt;,&lt;/span&gt; x &lt;span class="kr"&gt;in&lt;/span&gt; enumerate&lt;span class="p"&gt;(&lt;/span&gt;outcome&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;if&lt;/span&gt; x &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt;
    obser &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;float&lt;span class="p"&gt;(&lt;/span&gt;i&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;for&lt;/span&gt; i &lt;span class="kr"&gt;in&lt;/span&gt; obser&lt;span class="p"&gt;]&lt;/span&gt;
    score &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;float&lt;span class="p"&gt;(&lt;/span&gt;i&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;for&lt;/span&gt; i &lt;span class="kr"&gt;in&lt;/span&gt; score&lt;span class="p"&gt;]&lt;/span&gt;
    prev &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    thres &lt;span class="o"&gt;=&lt;/span&gt; np.arange&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;1.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c1"&gt;#(0.01,0.98,0.01)&lt;/span&gt;
    xval &lt;span class="o"&gt;=&lt;/span&gt; thres
    acc &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    ppv &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    npv &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    sen &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    spe &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    yod &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    auc &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    recall &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    precision &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;
    F1 &lt;span class="o"&gt;=&lt;/span&gt; np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt; 
    &lt;span class="kr"&gt;for&lt;/span&gt; l &lt;span class="kr"&gt;in&lt;/span&gt; &lt;span class="kp"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;thres&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        plotdata &lt;span class="o"&gt;=&lt;/span&gt; ROC_parameters&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;,&lt;/span&gt;score&lt;span class="p"&gt;,&lt;/span&gt;thres&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;])&lt;/span&gt;
        acc&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        ppv&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        npv&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        sen&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        spe&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        yod&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        recall&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        precision&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        F1&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plotdata&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        auc&lt;span class="p"&gt;[&lt;/span&gt;l&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; roc_auc_score&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;,&lt;/span&gt; score&lt;span class="p"&gt;)&lt;/span&gt;
    prev &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;#roc_vals=np.zeros((length(spe),8))&lt;/span&gt;
    roc_vals&lt;span class="o"&gt;=&lt;/span&gt;pd.DataFrame&lt;span class="p"&gt;(&lt;/span&gt;index&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kp"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;101&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; columns&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;thres&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;acc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ppv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;npv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;specificity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;sensitivity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;yod_index&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;recall&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;precision&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;F1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;auc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="c1"&gt;#roc_vals &amp;lt;- dacolnames(roc_vals) &amp;lt;- c(&amp;quot;thres&amp;quot;,&amp;quot;acc&amp;quot;,&amp;quot;ppv&amp;quot;,&amp;quot;npv&amp;quot;,&amp;quot;specificity&amp;quot;,&amp;quot;sensitivity&amp;quot;,&amp;quot;yod_index&amp;quot;,&amp;quot;auc&amp;quot;)&lt;/span&gt;
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;thres&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;thres
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;acc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; acc
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ppv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; ppv
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;npv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; npv
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;specificity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;spe
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sensitivity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; sen
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;yod_index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; yod&lt;span class="p"&gt;;&lt;/span&gt;
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;recall&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; recall&lt;span class="p"&gt;;&lt;/span&gt;
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;precision&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; precision
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;F1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; F1&lt;span class="p"&gt;;&lt;/span&gt;
    roc_vals&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;auc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; auc&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kr"&gt;return&lt;/span&gt; roc_vals

def ROC_parameters&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;,&lt;/span&gt;score&lt;span class="p"&gt;,&lt;/span&gt;thr&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;#print obser,score,thr&lt;/span&gt;
    temp&lt;span class="o"&gt;=&lt;/span&gt;np.zeros&lt;span class="p"&gt;(&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;score&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="c1"&gt;#print thr;&lt;/span&gt;
    temp&lt;span class="p"&gt;[[&lt;/span&gt; i &lt;span class="kr"&gt;for&lt;/span&gt; i&lt;span class="p"&gt;,&lt;/span&gt; x &lt;span class="kr"&gt;in&lt;/span&gt; enumerate&lt;span class="p"&gt;(&lt;/span&gt;score&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;if&lt;/span&gt; x &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; thr &lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
    p_ind&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; i &lt;span class="kr"&gt;for&lt;/span&gt; i&lt;span class="p"&gt;,&lt;/span&gt; x &lt;span class="kr"&gt;in&lt;/span&gt; enumerate&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;if&lt;/span&gt; x &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
    n_ind &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; i &lt;span class="kr"&gt;for&lt;/span&gt; i&lt;span class="p"&gt;,&lt;/span&gt; x &lt;span class="kr"&gt;in&lt;/span&gt; enumerate&lt;span class="p"&gt;(&lt;/span&gt;obser&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kr"&gt;if&lt;/span&gt; x &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
    TP &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;[&lt;/span&gt;p_ind&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    FP &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;[&lt;/span&gt;n_ind&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    TN &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;[&lt;/span&gt;n_ind&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    FN &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;[&lt;/span&gt;p_ind&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    acc &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)(&lt;/span&gt;TP&lt;span class="o"&gt;+&lt;/span&gt;TN&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;len&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;)&lt;/span&gt;
    recall&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
    precision&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
    &lt;span class="c1"&gt;#print TP,FP,TN,FN;&lt;/span&gt;
    &lt;span class="kr"&gt;if&lt;/span&gt; TP&lt;span class="o"&gt;+&lt;/span&gt;FP&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        ppv &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)(&lt;/span&gt;TP&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;TP&lt;span class="o"&gt;+&lt;/span&gt;FP&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kp"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        ppv&lt;span class="o"&gt;=&lt;/span&gt;np.NaN
    &lt;span class="kr"&gt;if&lt;/span&gt; TN&lt;span class="o"&gt;+&lt;/span&gt;FN&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        npv &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)(&lt;/span&gt;TN&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;TN&lt;span class="o"&gt;+&lt;/span&gt;FN&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kp"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        npv&lt;span class="o"&gt;=&lt;/span&gt;np.NaN
    &lt;span class="kr"&gt;if&lt;/span&gt; TP&lt;span class="o"&gt;+&lt;/span&gt;FN&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        sen &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)(&lt;/span&gt;TP&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;TP&lt;span class="o"&gt;+&lt;/span&gt;FN&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kp"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        sen&lt;span class="o"&gt;=&lt;/span&gt;np.NaN
    &lt;span class="kr"&gt;if&lt;/span&gt; TN&lt;span class="o"&gt;+&lt;/span&gt;FP&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        spe &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)(&lt;/span&gt;TN&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;TN&lt;span class="o"&gt;+&lt;/span&gt;FP&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kp"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        spe&lt;span class="o"&gt;=&lt;/span&gt;np.NaN
    &lt;span class="kr"&gt;if&lt;/span&gt; TP&lt;span class="o"&gt;+&lt;/span&gt;FN&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        recall &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)(&lt;/span&gt;TP&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;TP&lt;span class="o"&gt;+&lt;/span&gt;FN&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kp"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        recall&lt;span class="o"&gt;=&lt;/span&gt;np.NaN    
    &lt;span class="kr"&gt;if&lt;/span&gt; TP&lt;span class="o"&gt;+&lt;/span&gt;FP&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        precision &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)(&lt;/span&gt;TP&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;TP&lt;span class="o"&gt;+&lt;/span&gt;FP&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kp"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        precision&lt;span class="o"&gt;=&lt;/span&gt;np.NaN
    &lt;span class="kr"&gt;if&lt;/span&gt; recall&lt;span class="o"&gt;+&lt;/span&gt;precision&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;         
        F1 &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)((&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;recall&lt;span class="o"&gt;*&lt;/span&gt;precision&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;recall&lt;span class="o"&gt;+&lt;/span&gt;precision&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="kp"&gt;else&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        F1&lt;span class="o"&gt;=&lt;/span&gt;np.NaN
    yod &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;float&lt;span class="p"&gt;)(&lt;/span&gt;sen&lt;span class="o"&gt;+&lt;/span&gt;spe&lt;span class="m"&gt;-1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    ls&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kt"&gt;list&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;acc&lt;span class="p"&gt;)&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;ppv&lt;span class="p"&gt;)&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;npv&lt;span class="p"&gt;)&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;sen&lt;span class="p"&gt;)&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;spe&lt;span class="p"&gt;)&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;yod&lt;span class="p"&gt;)&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;recall&lt;span class="p"&gt;)&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;precision&lt;span class="p"&gt;)&lt;/span&gt;
    ls.append&lt;span class="p"&gt;(&lt;/span&gt;F1&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kr"&gt;return&lt;/span&gt; &lt;span class="kp"&gt;ls&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;calculating performance measurements and confidence intervals using Boostraping&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def calculate_metric_boostrap(outcome, score):
    d = []
    for p in range(0,len(score)):
        d.append((score[p]))
    score=pd.Series(d)
    n_bootstraps = 0
    rng_seed = 42  # control reproducibility
    scores_table = {} 
    rng = np.random.RandomState(rng_seed)
    for i in range(n_bootstraps):
    # bootstrap by sampling with replacement on the prediction indices
        indices = rng.random_integers(0, len(outcome) - 1, len(outcome))
        if len(np.unique(outcome[indices])) &amp;lt; 2:
        # We need at least one positive and one negative sample for ROC AUC
        # to be defined: reject the sample
            continue
        scores_table[i]= calculate_metric(outcome[indices], score[indices])

    panel = pd.Panel(scores_table)
    df=panel.mean(axis=0)
    return df,panel

def confidence_interval(panel):
    vector = []
    confidence_lower=panel[1].copy()
    confidence_upper=panel[1].copy()
    nr=len(panel[1].axes[0])
    nc=len(panel[1].axes[1])
    for ix in  range(0,nr):
        for iy in range(0,nc):
            vector = []
            for k, df in panel.iteritems():
                vector.append(df.iloc[ix,iy])
            sorted_vector = np.array(vector)
            sorted_vector.sort()
            confidence_lower.iloc[ix,iy] = sorted_vector[int(0.05 * len(sorted_vector))]
            confidence_upper.iloc[ix,iy] = sorted_vector[int(0.95 * len(sorted_vector))]
    return confidence_lower, confidence_upper
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The chart bellow illustrates ROC and PR curves for 4 different estimators applied on the same data&lt;img alt=" ROC" src="/images/ROC-PV1.png"&gt;&lt;img alt=" PR" src="/images/ROC-PV2.png"&gt;
The performances of the algorithms appear to be comparable in ROC space, however, in PR space we can see that Estimator 4 has a clear advantage over Estimator 3.&lt;/p&gt;
&lt;h1&gt;Assessing the Model&lt;/h1&gt;
&lt;p&gt;The chart bellow outlines the performance evaluation pipeline (except prevalence) in an example&lt;img alt=" performance evaluation pipeline" src="/images/MLPipe.jpg"&gt;&lt;/p&gt;
&lt;h1&gt;The bias-variance trade-off (Bias-variance dilemma)&lt;/h1&gt;
&lt;p&gt;Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible. Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.
&lt;img alt=" bias-variance trade-off" src="/images/biasvariance.png"&gt;&lt;/p&gt;
&lt;h1&gt;Analyzing Model Variance and Bias&lt;/h1&gt;
&lt;p&gt;The two methods used here for analyzing how the model is performing with the data are Learning Curves and a Model Complexity plot.Learning curves give us an opportunity to diagnose bias and variance in supervised learning models. &lt;/p&gt;
&lt;h2&gt;Learning Curves&lt;/h2&gt;
&lt;p&gt;A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a &lt;strong&gt;variance error&lt;/strong&gt; or a &lt;strong&gt;bias error&lt;/strong&gt;. If both the validation score and the training score converge to a value that is &lt;strong&gt;too low&lt;/strong&gt; with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score.
We will probably have to &lt;strong&gt;use an estimator&lt;/strong&gt; or a &lt;strong&gt;parametrization of the current estimator&lt;/strong&gt; that can learn more &lt;strong&gt;complex concepts&lt;/strong&gt; (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number of training samples (&lt;strong&gt;i.e. has a high variance&lt;/strong&gt;), adding more training samples will most likely increase generalization. In the following plot you can see that the SVM could benefit from more training examples.&lt;/p&gt;
&lt;h1&gt;Example of Scoring Learners and Cohort&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Cohort Definition&lt;/th&gt;
&lt;th&gt;Cohort Size&lt;/th&gt;
&lt;th&gt;CVD Percent in Cohort&lt;/th&gt;
&lt;th&gt;Covariates in Learner/Model&lt;/th&gt;
&lt;th&gt;Method Type&lt;/th&gt;
&lt;th&gt;Method Sensitivity&lt;/th&gt;
&lt;th&gt;Method PPV&lt;/th&gt;
&lt;th&gt;Balanced Accuracy&lt;/th&gt;
&lt;th&gt;Method Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ALL OF THEM (don't emulate&lt;/td&gt;
&lt;td&gt;369000&lt;/td&gt;
&lt;td&gt;0.80%&lt;/td&gt;
&lt;td&gt;"bmi&lt;/td&gt;
&lt;td&gt;numAge&lt;/td&gt;
&lt;td&gt;tchol&lt;/td&gt;
&lt;td&gt;sbp&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;t2d"&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age &amp;gt; 55&lt;/td&gt;
&lt;td&gt;122792&lt;/td&gt;
&lt;td&gt;22.60%&lt;/td&gt;
&lt;td&gt;"numAge&lt;/td&gt;
&lt;td&gt;tchol&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;gender"&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;0.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age 20-40&lt;/td&gt;
&lt;td&gt;121130&lt;/td&gt;
&lt;td&gt;0.02%&lt;/td&gt;
&lt;td&gt;"tchol&lt;/td&gt;
&lt;td&gt;t2d&lt;/td&gt;
&lt;td&gt;smoking&lt;/td&gt;
&lt;td&gt;race"&lt;/td&gt;
&lt;td&gt;LDA&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;"htn == ""Y"""&lt;/td&gt;
&lt;td&gt;108510&lt;/td&gt;
&lt;td&gt;18.85%&lt;/td&gt;
&lt;td&gt;smoking&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;"""NA"""&lt;/td&gt;
&lt;td&gt;"""NA"""&lt;/td&gt;
&lt;td&gt;"""NA"" (is this weird?)"&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;"gender == ""F"" &amp;amp; numAge &amp;gt; 60"&lt;/td&gt;
&lt;td&gt;53929&lt;/td&gt;
&lt;td&gt;14.30%&lt;/td&gt;
&lt;td&gt;"tchol&lt;/td&gt;
&lt;td&gt;htn"&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;NaN&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age 30-45&lt;/td&gt;
&lt;td&gt;99930&lt;/td&gt;
&lt;td&gt;"numAge&lt;/td&gt;
&lt;td&gt;race&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;gender&lt;/td&gt;
&lt;td&gt;smoking"&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age &amp;lt;= 40&lt;/td&gt;
&lt;td&gt;93980 train; 93981 test&lt;/td&gt;
&lt;td&gt;1.64%&lt;/td&gt;
&lt;td&gt;"numAge&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;smoking&lt;/td&gt;
&lt;td&gt;treat&lt;/td&gt;
&lt;td&gt;t2d&lt;/td&gt;
&lt;td&gt;gender&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;"gender == ""M"" &amp;amp; numAge &amp;gt; 60"&lt;/td&gt;
&lt;td&gt;36853&lt;/td&gt;
&lt;td&gt;30.75%&lt;/td&gt;
&lt;td&gt;"tchol&lt;/td&gt;
&lt;td&gt;htn"&lt;/td&gt;
&lt;td&gt;Logit&lt;/td&gt;
&lt;td&gt;0.28&lt;/td&gt;
&lt;td&gt;0.51&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GENETICS&lt;/td&gt;
&lt;td&gt;66100&lt;/td&gt;
&lt;td&gt;2.40%&lt;/td&gt;
&lt;td&gt;"tchol&lt;/td&gt;
&lt;td&gt;rs8055236&lt;/td&gt;
&lt;td&gt;htn&lt;/td&gt;
&lt;td&gt;t2d&lt;/td&gt;
&lt;td&gt;smoking"&lt;/td&gt;
&lt;td&gt;lda&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;age &amp;lt; 55 &amp;amp; age &amp;gt; 35&lt;/td&gt;
&lt;td&gt;379272&lt;/td&gt;
&lt;td&gt;5.30%&lt;/td&gt;
&lt;td&gt;cvd ~ tchol + htn + t2d + bmi + rs8055236&lt;/td&gt;
&lt;td&gt;logit&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;nan&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;age&amp;gt;55&lt;/td&gt;
&lt;td&gt;logit&lt;/td&gt;
&lt;td&gt;0.22&lt;/td&gt;
&lt;td&gt;0.59&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GENETICS&lt;/td&gt;
&lt;td&gt;46217 train; 8157 test&lt;/td&gt;
&lt;td&gt;9.78%&lt;/td&gt;
&lt;td&gt;"cvd ~ numAge + htn + smoking&lt;/td&gt;
&lt;td&gt;+ treat + t2d + gender + bmi + tchol + sbp + rs10757278 + rs4665058 + rs8055236"&lt;/td&gt;
&lt;td&gt;SuperLearner&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;td&gt;0.369&lt;/td&gt;
&lt;td&gt;0.8394&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="ROC"></category><category term="AUROC"></category><category term="AUCPR"></category><category term="F1 Score"></category><category term="Recall"></category><category term="Precision"></category></entry><entry><title>The role of unit tests in test automation</title><link href="https://mohcinemadkour.github.io/posts/2017/08/The%20role%20of%20unit%20tests%20in%20test%20automation/" rel="alternate"></link><published>2017-08-03T16:00:00-04:00</published><updated>2017-08-03T16:00:00-04:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2017-08-03:/posts/2017/08/The role of unit tests in test automation/</id><summary type="html">&lt;p&gt;Unit testing is a software development and testing approach in which the smallest testable parts of an application, called units, are individually and independently tested to see if they are operating properly. Unit testing can be done manually but is usually automated. Unit testing is a part of the test-driven …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Unit testing is a software development and testing approach in which the smallest testable parts of an application, called units, are individually and independently tested to see if they are operating properly. Unit testing can be done manually but is usually automated. Unit testing is a part of the test-driven development(TDD) methodology that requires developers to first write failing unit tests. Then they write code in order to change the application until the test passes. Writing the failing test is important because it forces the developer to take into account all possible inputs, errors and outputs. &lt;/p&gt;
&lt;p&gt;The result of using TDD is that an agile team can accumulate a comprehensive suite of unit tests that can be run at any time to provide feedback that their software is still working.  If the new code breaks something and causes a test to fail,  TDD also makes it easier to pinpoint the problem, refactor the application and fix the bug.&lt;/p&gt;
&lt;h1&gt;The AAA pattern&lt;/h1&gt;
&lt;p&gt;The goal of unit testing is to isolate each part of a program and show that the individual parts work correctly.  This is in line with the YAGNI ("You ain't gonna need it") principle at the heart of the agile development practice of doing the simplest thing that can possibly work.  Using the YAGNI principle to build units of software, together with other practices such as continuous refactoring and continuous integration, make it easier to automate groups or suites of unit tests.  Unit test automation is a key component of a Continuous Delivery DevTestOps solution, that is, a continuously tested, two-way DevOps software delivery pipeline between an organization and its customers.&lt;/p&gt;
&lt;p&gt;Unit tests are designed for code that has no external dependencies, such as calls to the database or web services.  Because they focus on a specific behavior in a small section of a system under test (SUT), they're also relatively straight-forward to automate, especially if they are written in a standard format such as the AAA pattern.&lt;/p&gt;
&lt;p&gt;The AAA unit test pattern&lt;/p&gt;
&lt;p&gt;Image Source: Code Project
![Image Source: Code Project](images/AAA pattern.)&lt;/p&gt;
&lt;p&gt;The AAA (Arrange, Act, Assert ) pattern helps organize and clarify test code by breaking down a test case into the following functional sections:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;The Arrange section of a unit test initializes objects and sets the value of the data that is passed to the test case.
The Act section invokes the test case with the arranged parameters.
The Assert section verifies the test case behaves as expected.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Consider the following example test scenario:&lt;/p&gt;
&lt;p&gt;A unit test in AAA format tests a software unit that increments the number of products in an e-commerce shopping cart:&lt;/p&gt;
&lt;p&gt;Arrange&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Create a empty shopping cart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Act&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Add a product to the cart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Assert &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Number of products in cart increased by one
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's more about  the AAA pattern:&lt;/p&gt;
&lt;h2&gt;Arrange Section&lt;/h2&gt;
&lt;p&gt;In order to put yourself in a position where you can call a software unit and check that the result was correct, you first need to "prime the pump," or put the unit into a known beginning state.  When setting up the module to be tested, it may be necessary sometimes to surround that module with other collaborator modules.  For testing purposes, those collaborators could be test modules with actual or made-up data (also known mock objects, fakes, etc.). &lt;/p&gt;
&lt;p&gt;Mock objects are simulated objects created by a developer that mimic the behavior of real objects in controlled ways, similar to how crash test dummies are expected to simulate the dynamic behavior of humans in vehicle impacts.  A mock object, in the case of a database or e-commerce application, might be created as part of a unit test with a variety of fake data because real customer records may not exist yet or it would slow down testing if a complete customer database had to be accessed or initialized before running the test.&lt;/p&gt;
&lt;p&gt;Test-specific mock objects can used to verify application behavior&lt;/p&gt;
&lt;p&gt;Image source:  hackerchick
&lt;img alt="Image source:  hackerchick" src="images/mockobjects.png"&gt;&lt;/p&gt;
&lt;p&gt;Mock objects are used for much more than creating made-up test data in unit testing.  For example, using mock objects in place of real objects can make it easier to test a complex algorithm based on multiple objects being in particular states.  The use of mock objects is extensive in the literature on automated testing using xUnit testing frameworks. You can find an example here. &lt;/p&gt;
&lt;p&gt;In the shopping cart example, the Arrange part of the pattern involves creating a empty shopping cart  by initially setting the number of products in the cart to zero.  As we'll see later, there's a way to adapt the AAA syntax to handle more complicated scenarios.&lt;/p&gt;
&lt;h2&gt;Act Section:&lt;/h2&gt;
&lt;p&gt;This is the part of the test that exercises the unit of code under test by making a function or method call that returns a result or causes a reaction that can be observed.&lt;/p&gt;
&lt;p&gt;In the shopping cart example, the Act section takes place when Buy Item button on the shopping cart is pushed. &lt;/p&gt;
&lt;h2&gt;Assert Section:&lt;/h2&gt;
&lt;p&gt;The assertion section were you check to see that you have a result or reaction (include calls to other units of code) that matches your expectations. &lt;/p&gt;
&lt;p&gt;In the shopping cart example, the Assert section occurs when the number property is checked against your expectation (i.e. the number of products in the cart is increased by one every time the Buy Item button is pushed.)&lt;/p&gt;
&lt;p&gt;Following the AAA pattern consistently makes test code easier to read by clearly separating what is being tested from the setup and verification steps.  This helps when you need to reexamine sections of test code to see if they're still doing what it should be doing, such as following a previous set of successful test steps.&lt;/p&gt;
&lt;p&gt;In the bottom-up testing style of test-driven development, unit tests written in the AAA syntax will help you know exactly where to search to find a bug when a unit test fails.  Kent Beck, who popularized the TDD concept in his book Test Driven Development: By Example, states that TDD has two basic rules:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Never write a single line of code unless you have a failing automated test.
Eliminate duplication.  In software engineering, don&amp;#39;t repeat yourself (DRY) is a principle of agile software development, aimed at reducing repetition of information of all kinds, which is especially useful in multi-tier architectures
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;According to Beck, a good unit test in TDD should be able to do all of the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Run fast (they have short setups, run times, and break downs).
Run in isolation (you should be able to reorder them).
Use data that makes them easy to read and to understand.
Use real data (or copies of production data) when they need to.
Represent one step towards your overall goal.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This means unit tests need to be narrowly focused and shouldn't try to test too many different things at once.  An example of a unit test that tries to do too many things is shown in a test scenario involving a sweater purchase in an e-commerce shopping cart application.  The Arrange section in this example assumes that the pipe has been primed and you have sweaters in your inventory database, that another unit is able to show the inventory to your customer and still other units are be able to process the customer payment and remove items from inventory.&lt;/p&gt;
&lt;p&gt;Here's some pseudo code for a unit test in the AAA format for this kind of functionality:&lt;/p&gt;
&lt;p&gt;Arrange&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;setup Sweater Inventory (mostly likely with mock database objects)
set Sweater Inventory Count to 5      
when Sweater Inventory is requested to remove N items, then count = count - N
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Act&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;call the Unit Under Test to remove 3 items from inventory
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Assert&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;the number of sweaters in the Sweater Inventory is 2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As you can see, writing large unit tests in this way can quickly become complex and convoluted,  especially when you need to test end-to-end functionality for a complete 6-step e-commerce shopping-cart application, i.e.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Access Homepage –&amp;gt; 
Customer Search results –&amp;gt;
Product details  –&amp;gt;
Customer login (or Register New customer) –&amp;gt;
Payment details –&amp;gt;
Order confirmation
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A simpler and better to way to use the AAA unit test scaffolding, both for unit and higher-level tests, is by using Behavior-Driven Development (BDD), which BDD pioneer Dan North defines this way:&lt;/p&gt;
&lt;p&gt;"BDD is a second-generation, outside–in, pull-based, multiple-stakeholder, multiple-scale, high-automation, agile methodology. It describes a cycle of interactions with well-defined outputs, resulting in the delivery of working, tested software that matters."
![andolasoft](images/bdd n tdd.jpg)
BDD is an enhancement of TDD&lt;/p&gt;
&lt;p&gt;Image Source:  andolasoft&lt;/p&gt;
&lt;p&gt;The main advantage of BDD is that it encourages collaboration between developers, QA and non-technical or business participants on a software project.  It extends TDD by writing test cases in a natural language that non-programmers and domain experts can read. BDD features are usually defined in a GIVEN WHEN and THEN (GWT) format, which is a semi-structured way of writing down test cases.  A BDD feature  or user story needs to follow the following structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Describe who is the primary stakeholder of the feature
What effect the stakeholder wants the feature to have
What business value the stakeholder will derive from this effect
Acceptance criteria or scenarios
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A brief example of a BDD feature in this format looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Feature&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;   &lt;span class="n"&gt;Items&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;abandoned&lt;/span&gt; &lt;span class="n"&gt;shopping&lt;/span&gt; &lt;span class="n"&gt;carts&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="n"&gt;be&lt;/span&gt; &lt;span class="n"&gt;returned&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;inventory&lt;/span&gt;
&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="n"&gt;order&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt; &lt;span class="n"&gt;track&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;inventory&lt;/span&gt;
&lt;span class="n"&gt;As&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="n"&gt;store&lt;/span&gt; &lt;span class="n"&gt;owner&lt;/span&gt;
&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="n"&gt;want&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt; &lt;span class="n"&gt;items&lt;/span&gt; &lt;span class="n"&gt;back&lt;/span&gt; &lt;span class="n"&gt;into&lt;/span&gt; &lt;span class="n"&gt;inventory&lt;/span&gt; &lt;span class="n"&gt;when&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="n"&gt;shopping&lt;/span&gt; &lt;span class="n"&gt;cart&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;abandoned&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Scenario 1: On-line shopping cart items not purchased within 30 minutes go back into inventory&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Given that a customer puts a black sweater into his shopping cart
And I have three black sweaters in inventory.
When he does not complete the purchase with 30 minutes (i.e. abandons the shopping cart)
Then I should have four black sweaters in inventory.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In TDD, the developers write the tests while in BDD the automated specifications are created by users or testers (with developers writing the underlying code that implements the test.) 
![Test Automation Pyramid](/images/automation pyramid.png)
Test Automation Pyramid&lt;/p&gt;
&lt;p&gt;Image Source:  Effective Testing Practices in an Agile Environment&lt;/p&gt;
&lt;h1&gt;Outside-in vs. Inside-out Testing&lt;/h1&gt;
&lt;p&gt;Agile teams generally follow one of two approaches when it comes to testing their applications, either outside-In  or inside-out.  In the outside-in approach, teams start by focusing on the end user's perspective and attempt to describe high-level desired functionality and goals for the software under test in the form of user stories.  In every iteration or Sprint, user stories are refined until the agile team and the Product Owner/Customer Representative can agree on the acceptance criteria, which determine that a User Story works as planned.  Testing then goes 'inward' and code is written to test smaller and small components until you reach the unit-test level.&lt;/p&gt;
&lt;p&gt;In the inside-out or bottom-up approach, agile teams start with unit tests at the lowest level of the Test Automation Pyramid (see Figure 5 above).  As the code evolves due to refactoring, testing efforts evolve as well as the team moves upward to acceptance level testing, which tests business logic at the API or service level.  The top of the pyramid and the last thing tested is the user interface (UI).&lt;/p&gt;
&lt;p&gt;Inside-out and outside-in are different but complementary approaches to testing.  Software quality control relies on the related notions of verification and validation (V&amp;amp;V) that check to see that a software system meets specifications and that it fulfills its intended purpose.  The terms verification and validation are often used interchangeably but have different meanings: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Verification&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Checks&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;software&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;respect&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;specifications&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Is our team building the code right?&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Validation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Checks&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;software&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;respect&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;customer&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;expectations&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Are we building the right code?&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On modern agile DevOps projects, validation and verification steps overlap and take place continuously since agile team members must engage with customers and other stakeholders throughout the project-- to do things like prioritizing bug fixes and enhancements on the team's project backlog --  and not just after a separate test phase at the end of the project.  This requires effective communication at all levels of the business since team members need to be understand what features need to be built and who needs each feature.&lt;/p&gt;
&lt;p&gt;BDD and TDD use syntax to describe three test states that are roughly equivalent:&lt;/p&gt;
&lt;p&gt;Given = Arrange 
When = Act,
Then   = Assert&lt;/p&gt;
&lt;p&gt;In  TDD, "inside-out" development starts with the innermost components and proceeds towards the user interface building on the previously constructed components.  BDD makes it easier to for agile teams to design outside-in but then code inside-out.  Because of the increased communication and collaboration among developers, QA and non-technical or business participants on a software project that the BDD/GWT syntax promotes,  developers and testers are able to anticipate how to test the outer software when writing tests for the inner software.  This means that tests at the Acceptance and GUI level can better take advantage of already-built tests, reducing the need for the Mock Objects mentioned earlier. &lt;/p&gt;
&lt;p&gt;Using TDD and the AAA syntax encourages developers to write small, independent tests and to continually refactor their code.  BDD supports TDD by helping bridge the gap between unit tests and higher-level acceptance and integration tests.  BDD and the GWT syntax are useful in business environments, where work done by developers needs to be mapped to business value. Because the two approaches complement each other so well, you should combine them with test management tools to get the best results on your DevOps test automation projects.&lt;/p&gt;</content><category term="Unit Test"></category><category term="Software"></category></entry><entry><title>Clustering of vaccine temporal data in timeline</title><link href="https://mohcinemadkour.github.io/posts/2015/02/Temporal%20clinical%20events%20clustering/" rel="alternate"></link><published>2015-02-18T16:00:00-05:00</published><updated>2015-02-18T16:00:00-05:00</updated><author><name>Mohcine Madkour</name></author><id>tag:mohcinemadkour.github.io,2015-02-18:/posts/2015/02/Temporal clinical events clustering/</id><summary type="html">&lt;p&gt;Events in clinical narratives are naturally associated with medical trials, including surgery, vaccination, lab test, medication, medical procedure, and diagnosis. They are interrelated with many temporal relations. The grouping of medical events onto temporal clusters is a key to applications such as longitudinal studies, clinical question answering, and information retrieval …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Events in clinical narratives are naturally associated with medical trials, including surgery, vaccination, lab test, medication, medical procedure, and diagnosis. They are interrelated with many temporal relations. The grouping of medical events onto temporal clusters is a key to applications such as longitudinal studies, clinical question answering, and information retrieval. However, it is difficult to define clinical event quantitatively or consistently in coarse time-bins (e.g. before vaccination or after admission). In this article, I developed the K-means classifier to enable labeling a sequence of medical events with predefined time-bins. The features set is based solely on temporal distance similarity between boundaries of events. The result of the solution is integrated with the &lt;a href="https://timeline.knightlab.com/"&gt;KnightLab timeline JS tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this article I investigate the task of tagging a sequence of events using a clustering algorithm. For this purpose I assume that each medical note can be associated with a predefined set of coarse of times that I refer to as time bins. For our example of VAERS note, the potential time-bins are: “before vaccination”, “soon after vaccination”, and “way after vaccination”. The time-bin “before vaccination” is intended to capture past medical history of the patient including the medical state of the patient on time of vaccination; “soon after vaccination” captures medical events that occurred immediately after the vaccination; and “way after vaccination” captures medical events that occurred after an extended duration from the vaccination. The issue in clustering events in predefined time-bins is that the time duration of each timebin varies based on the patient. For instance, the coarse of time “soon
after vaccination” could be the first few hours after or a few days
after depending on the general conditions. For that I consider that
related events happen in relatively close proximity of time. I use a
non-hierarchical clustering to classify the set of events. I consider
the temporal distance between events as the measure of similarity
between events of same clusters and dissimilarity between events of
different clusters.&lt;/p&gt;
&lt;h2&gt;K-Means Clustering&lt;/h2&gt;
&lt;p&gt;K-means is one of the simplest algorithms for solving the
clustering problem. Clustering is an unsupervised learning
problem whereby I aim to group subsets of entities with one
another based on a temporal distance similarity. The idea is to define
k centroids for the k assumed clusters and to associate each point
belonging to a given data set to the nearest center. A point represents
the time instant of the event or the center of interval if its time
interval event. When no point is pending, the first step is completed
and an early group age is done. At this point I re-calculate k new
centroids as barycenter of the clusters resulting from the previous
step. After I have these k new centroids, I re-bind the same data
set points to their nearest new center. A loop has been generated. As
a result of this loop the k centers change their location step by step
until no more changes are done or in other words centres do not
move any more.&lt;/p&gt;
&lt;h2&gt;Timeline View&lt;/h2&gt;
&lt;p&gt;For data that relates to temporal events, the Timeline Widget adds an interesting dimension to your exhibit.&lt;/p&gt;
&lt;p&gt;The nobelists.js data file lists the years when the Nobelists won their prizes, so I can plot each one on a time line. To display timelines in Exhibit you need to include a separate utility, the Timeline widget. The Timeline widget is a bit bulky, so Exhibit doesn't include it by default. You have to include the time extension to Exhibit. Open the file nobelists.html, find the reference to exhibit-api.js and add the following script element after it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;data-ex-role=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;view&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    data-ex-view-class=&amp;quot;Timeline&amp;quot;  
    data-ex-start=&amp;quot;.time&amp;quot; 
    data-ex-end=&amp;quot;.time2&amp;quot; 
    data-ex-color-key=&amp;quot;.cluster&amp;quot; 
    data-ex-top-band-unit=&amp;quot;month&amp;quot; 
    data-ex-bottom-band-unit=&amp;quot;year&amp;quot; 
    data-ex-top-band-pixels-per-unit=&amp;quot;90&amp;quot; 
    data-ex-bottom-band-pixels-per-unit=&amp;quot;400&amp;quot;
        &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;data-ex-role=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lens&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;lt;span&lt;/span&gt; &lt;span class="na"&gt;data-ex-content=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.hour&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/span&amp;gt;&lt;/span&gt;: 
                &lt;span class="nt"&gt;&amp;lt;span&lt;/span&gt; &lt;span class="na"&gt;data-ex-content=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.label&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Visualization&lt;/h2&gt;
&lt;p&gt;I visualize the results using the Exhibit dashboard
solution. The timeline dashboard enables intuitive cluster analysis
by user interactions. Also our visualization allows summarizing by
the various types of events information. &lt;img alt="Here is a screeshot of the visualization" src="/images/timeline.png"&gt; Check out the &lt;a href="http://htmlpreview.github.io/?https://github.com/mohcinemadkour/Event-Timeline/blob/master/index.html"&gt;visualization of clustered events&lt;/a&gt;&lt;/p&gt;</content><category term="K-means"></category><category term="data visualization"></category><category term="VAERS Reports"></category></entry></feed>