<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="What makes reinforcement learning different from other machine learning paradigms ? Learning by Doing? There is no supervisor, only a reward signal, Feedback is delayed, not instantaneous. Time...">
        <meta name="keywords" content="Deep Reinforcement Learning">
        <link rel="icon" href="https://mohcinemadkour.github.io/favicon.ico">

        <title>Deep Reinforcement Learning - A Pelican Blog</title>

        <!-- Stylesheets -->
        <link href="https://mohcinemadkour.github.io/theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->



    </head>

    <body>

        <!-- Header -->
    <div class="header-container" style="background: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url('https://mohcinemadkour.github.io/images/data-lake-background.png'); background-position: center; background-size: cover;">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="https://mohcinemadkour.github.io/"><img class="mr20" src="https://mohcinemadkour.github.io/images/logo.svg" alt="logo">A Pelican Blog</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="https://mohcinemadkour.github.io/pdfs/mohcine_madkour_cv.pdf">CV-Resume</a>
                                <a href="https://mohcinemadkour.github.io/categories.html">Categories</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">Deep Reinforcement Learning</h1>
                      <p class="header-date">By <a href="https://mohcinemadkour.github.io/author/mohcine-madkour.html">Mohcine Madkour</a>, Thu 14 June 2018, in category <a href="https://mohcinemadkour.github.io/category/docker.html">Docker</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="https://mohcinemadkour.github.io/tag/deep-reinforcement-learning.html">Deep Reinforcement Learning</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <h1>What makes reinforcement learning different from other machine</h1>
<h2>learning paradigms ? Learning by Doing?</h2>
<p>There is no supervisor, only a reward signal, Feedback is delayed, not instantaneous. Time really matters (sequential, non i.i.d data). Agent’s actions affect the subsequent data it receives
![supervised vs rl](/images/supervised vs rl.png)</p>
<h3>Rewards</h3>
<p>A reward Rt is a scalar feedback signal, It indicates how well agent is doing at step t. The agent’s job is to maximise cumulative reward.Reinforcement learning is based on the reward hypothesis, which states that all goals can be described by the <strong>maximisation of expected cumulative reward</strong>. Examples of Rewards can be +ve reward for following desired trajectory and −ve reward for crashing, in <em>Fly stunt manoeuvres in a helicopter</em>. +/−ve reward for winning/losing a game in <em>Defeat the world champion at Backgammon</em>. +ve reward for each $ in bank in <em>Manage an investment portfolio. +ve reward for producing power</em>, and −ve reward for exceeding safety thresholds in Control a power station. +ve reward for forward motion, and −ve reward for falling over in <em>Make a humanoid robot walk</em>, and +/−ve reward for increasing/decreasing score in <em>Play many different Atari games better than humans</em>.</p>
<h3>Sequential Decision Making</h3>
<p>Goal: select actions to maximise total future reward. The actions may have long term consequences, and the Reward may be delayed. SOmetimes It may be better to sacrifice immediate reward to gain more long-term reward
Examples are numerous, A financial investment (may take months to mature),
Refuelling a helicopter (might prevent a crash in several hours), Blocking opponent moves (might help winning chances many moves from now)
Two fundamental problems in sequential decision making 
*Reinforcement Learning:
<strong>The environment is initially unknown. 
</strong>The agent interacts with the environment
<strong>The agent improves its policy
*Planning: 
</strong>A model of the environment is known. 
<strong>The agent performs computations with its model (without any external interaction). 
</strong>The agent improves its policy a.k.a. deliberation, reasoning, introspection, pondering, thought, search</p>
<h2>Markov decision process: Fully Observable Environments</h2>
<p><img alt="MDP" src="/images/MDP.png">
Fully Observable Environments: Agent state = environment state = information state</p>
<h3>partially observable Markov decision process: Partially Observable Environments</h3>
<p>Partial observability: agent indirectly observes environment. FOr example : A robot with camera vision isn’t told its absolute location, A trading agent only observes current prices, A poker playing agent only observes public cards. The agent state # environment state. The Agent must construct its own state representation :Complete history, Beliefs of environment state, Recurrent neural network</p>
<h1>Major Components of an RL Agent</h1>
<p>An RL agent may include one or more of these components:
<em>Policy: agent’s behaviour function
</em>Value function: how good is each state and/or action
*Model: agent’s representation of the environment</p>
<h2>Policy</h2>
<p>A policy is the agent’s behaviour. It is a map from state to action, e.g. We have two types : Deterministic policy: a = π(s) and Stochastic policy: π(a|s) = P[At = a|St = s]</p>
<h2>Value Function</h2>
<p>The value function is a prediction of future reward. It is used to evaluate the goodness/badness of states And therefore to select between actions, e.g.]:
vπ(s) = E π [Rt+1 + γRt+2 + γ2 Rt+3 + ... | S t = s]</p>
<h2>Model</h2>
<p>A model predicts what the environment will do next. P predicts the next state
R predicts the next (immediate) reward, e.g.
Pss'= P[S t+1 = s | S t = s, A t = a]
Ras = E [Rt+1 |St = s, At = a]</p>
<h2>Categorizing RL agents</h2>
<p><em>Value Based: No Policy (Implicit)+ Value Function
</em>Policy Based: Policy + No Value Function
<em>Actor Critic: Policy+ Value Function
</em>Model Free: Policy and/or Value Function+ No Model
*Model Based: Policy and/or Value Function+ Model
<img alt="RL Agents" src="/images/RLAgents.png"></p>
<h2>Exploration and Exploitation</h2>
<p>Reinforcement learning is like trial-and-error learning. The agent should discover a good policy from its experiences of the environment and Without losing too much reward along the way. The Exploration finds more information about the environment. The Exploitation exploits known information to maximise reward. It is usually important to explore as well as exploit.</p>
<h2>Prediction and Control</h2>
<p>Prediction: evaluate the future Given a policy
Control: optimise the future Find the best policy</p>
<h1>Types of reinforcement learning algorithms</h1>
<p>RL algorithms that satisfy the <em>Markov property</em> are called the <em>Markov Decision Processes (MDP)</em>. The Markov property assumes that the current state is independent of the path that leads to that particular state. Hence, in Markovian problems a memoryless property of a stochastic process is assumed. In
practice it means that the probability distribution of the future states depends only on the current state and not on the sequence of events that preceded. This is a useful property for stochastic processes as it allows for analysing the future by setting the present
<img alt="State Transition from state s to state s'" src="/images/fig1_rl.png">
An MDPs consist of state (s), action (a) sets and given any state and action to be taken, a transition probability function of each possible next state (s’) illustrated in figure 1. In addition, each taken action to arrive to the next state is rewarded giving each of all possible actions a reward value
depending on the type of action. Each visited state is accredited by a value given to it according to a <strong>value function V(s)</strong> which represents how good it is for an agent to be in a given state. The value of a state s under a policy π is then denoted as Vπ(s) which in theory denotes the expected return when starting in state s and following a sequence of states to be visited according to the order defined in π thereafter. When this theorem is applied to a model-free control problem, the <strong>state-value function</strong> may
not suffice as it does not show what action was taken for the state value to be acquired. Therefore, a similar function has been introduced representing an estimation of the value of each possible action in a state. This is described as the <strong>action-value function</strong> for policy π Qπ(s,a). Figure 2 illustrates an example of the relationship between the action-value function and the state-value function. In 2.a. the action-values are shown for each direction of the propagation, North, East, South, and West respectively. <strong>The state value function represents then the highest action-value possible in that state which is the action North in the example</strong>.<img alt="" src="/images/9a9b.png"></p>
<p>The optimal policy is denoted as the superscript asterisk to the action-value-function Q(s,a) and state value-function V(s). Formally, the optimal value function is then given by:
<img alt="Eq1" src="/images/eq1.png">
Where Q*(s,a) is given by:
<img alt="Eq2" src="/images/eq2.png">
Herein, T(s, a, s’) is the transition probability to the next state s’ given state s and action a. γ presents the discount factor which is usually smaller than 1 and is used to discount for earlier values in order to assign
higher values for sooner rewards. This is necessary to converge the algorithm.
Substituting equation 3 in 2 gives the Bellman equation:
<img alt="Eq3" src="/images/eq3.png">
These updates will be appended to the states that were visited resulting (after a significant number of iterations) in state values showing how good to be in that state. In order to be able to choose between the states to select a policy, <strong>as many states as possible need to be visited</strong> in order to converge to an accurate estimation of the state value. Acquiring the highest reward depends on these visited states and the reward accumulated. However, in order to discover more states and potentially higher rewards, the agent needs to take actions it has never taken before. This is referred to as the <strong>trade-off between exploitation and exploration</strong>. This trade-off could be achieved by setting a variable denoted as Epsilon (ε) which gives the extent of exploration versus exploitation. A fully exploiting policy is referred to as an
epsilon-greedy policy and holds a value of 0 for ε. Correspondingly, a fully exploring policy gives a value of 1 to ε and is referred to as an epsilon-soft policy. The learning can therefore be tuned between these two extremes in order to allow for convergence towards an optimal value by occasionally exploring new states and actions.</p>
<h1>Markovian environments and Non Markovian environments</h1>
<p><img alt="Markovian environments and Non Markovian environments" src="/images/fig1_rl.png"></p>
<h1>Classes of RL algorithms</h1>
<p>RL knows three fundamental classes of methods for solving these learning problems:
<em> Dynamic Programming (DP)
</em> Monte Carlo methods
* Temporal-difference learning
Dependent on the problem at stake, each of these methods could be more suitable than the other. DP methods are model-based and require therefore a complete and accurate model of the environment i.e. all the aforementioned functions of the environment need to be known to initiate learning. However,
<strong>the environment is not always defined prior to the learning process</strong> which poses a challenge to this method. This is where the two other model-free learning methods come in handy. The Monte Carlo algorithms only require an experience sample such as a data set in which the states, actions and rewards
of the (simulated) interaction with the environment. In comparison with DP methods, no model of the transition probability function is required and neither the dynamics of the environment. Monte Carlo algorithms solve the RL problem by averaging sample return of each episode. Only after the termination
of an episode, that the value estimation and policies are updated. Hence, it is based on averages of complete returns of the value functions of each state. This class of algorithms does not exploit Markov property described before and is therefore more efficient in <strong>non-Markovian environments</strong>. On the
other hand, Temporal-Difference methods do also not require a model of the environment but are like DP solving for incrementing step-by-step rather than episode-by-episode. Hence, TD methods exploit the Markovian property and perform usually better in Markovian environments.
The choice between these two classes of model-free RL algorithms very much depends on the type of data set available. For continuous processes in which there are no fixed episodic transitions, Monte Carlo methods may not be the optimal solution as they average the return only at the end of each episode. TD algorithms might then be a better solution as they assign a reward incrementally over each state. This allows them to converge faster towards an optimal policy for large data sets with a large number state spaces.</p>
<h1>Temporal-difference learning: On-policy and off-policy TD control</h1>
<p>TD algorithms comprise two important RL classes of algorithms divided in Off-Policy and On-Policy TD control algorithm classes. The difference between the two lays in the policy that is learned from the simulation or set of experiences (data). On-Policy TD control algorithms are often referred to as <strong>SARSA algorithms</strong> in which the letters refer to the sequence of State, Action, Reward associated with the state transition, next State, next Action. This sequence is followed in each time-step and is used to update the action-value of these two states according:
<img alt="Eq4" src="/images/eq4.png">
Here, α represents the step-size parameter which functions as the exponentially moving average parameter. It is especially useful for non-stationary environments for weighting recent rewards more heavily than long-past ones. This could also be illustrated by rearranging the above equation to:
<img alt="Eq5" src="/images/eq5.png">
If α is a number smaller than one for non-stationary environments which indicates that recent updates weight more than previous ones. This transition happens after every nonterminal state. The Q (s t+1 ,a t+1 ) components of every terminal state is defined as zero. Hence, every terminal state has an update value of 0.</p>
<p><strong>SARSA</strong> is called an on-policy algorithm because it updates the action-value-function according to the policy it is taking in every step. Therefore, it takes the epsilon-policy into account in order to arrive the
optimal policy for a certain problem. Off-policy algorithms approximate the best possible policy even</p>
<p><a href="Cliff.png">Cliff)</a></p>
<p>when that policy is not taken by the agent. Hence, Off-Policy algorithms base the update of the state action-value function on the assumption of optimal behaviour without taking into account the epsilon policy (the chance to take a negative action). Figure 10 shows a suitable example given by Sutton and Barto (1998) and which illustrates the policy outcome differences between the two types of TD algorithms. The cliff represents states with high negative reward. Since SARSA takes the epsilon policy into account, it learns that at some instances a non-optimal action will be taken which results in a high
negative reward. Hence, it will learn to take the safe path rather than the optimal path. <strong>Q-learning algorithms</strong> on the other hand, will take the optimal path by which the highest total reward could be achieved. This is because it does not take the epsilon probability into account of taking an extremely negative action. This class of algorithms is denoted by the following equation:
<img alt="Eq6" src="/images/eq6.png">
This difference will inevitably influence the suitability for the type of application. </p>
<h1>Markov Decision Processes</h1>
<p>Markov decision processes formally describe an environment for reinforcement learning Where the environment is fully observable, i.e. The current state completely characterises the process. Almost all RL problems can be formalised as MDPs, e.g.Optimal control primarily deals with continuous MDPs, Partially observable problems can be converted into MDPs, Bandits are MDPs with one state</p>
<h2>Markov Property</h2>
<p>"The future is independent of the past given the present”
Definition: A state St is Markov if and only if
P [S t+1 | S t ] = P [S t+1 | S 1 , ..., S t ]
The state captures all relevant information from the history
Once the state is known, the history may be thrown away. i.e. The state is a sufficient statistic of the future</p>
<h2>State Transition Matrix</h2>
<p>For a Markov state s and successor state s' , the state transition
probability is defined by P ss' = P[S t+1 = s'| S t = s]
State transition matrix P defines transition probabilities from all
states s to all successor states s 0 transition probabilities from all
states s to all successor states s' ![State Transition Matrix](/images/Screenshot from 2018-06-17 08-05-54.png)</p>
<h2>Markov Process</h2>
<p>A Markov process is a memoryless random process, i.e. a sequence
of random states S 1 , S 2 , ... with the Markov property.
Definition: A Markov Process (or Markov Chain) is a tuple <S, P>
S is a (finite) set of states, P is a state transition probability matrix,
P ss'= P [S t+1 = s'| St = s]</p>
<p>![Example](/images/Screenshot from 2018-06-17 08-16-27.png)</p>
<h2>Markov Reward Process</h2>
<p>A Markov reward process is a Markov chain with values.
Definition: A Markov Reward Process is a tuple <S, P, R, γ>
S is a finite set of states
P is a state transition probability matrix,
P ss'= P [S t+1 = s'| St = s]
R is a reward function, Rs = E [Rt+1 | S t = s]
γ is a discount factor, γ ∈ [0, 1]</p>
<h2>Return</h2>
<p>Definition: The return G t is the total discounted reward from time-step t.
<a href="images/Screenshot from 2018-06-17 08-21-18.png"></a>
The discount γ ∈ [0, 1] is the present value of future rewards
The value of receiving reward R after k + 1 time-steps is γkR.
This values immediate reward above delayed reward.
γ close to 0 leads to ”myopic” evaluation
γ close to 1 leads to ”far-sighted” evaluation</p>
<p>Most Markov reward and decision processes are discounted. Why?
<em>Mathematically convenient to discount rewards
</em>Avoids infinite returns in cyclic Markov processes
<em>Uncertainty about the future may not be fully represented
</em>If the reward is financial, immediate rewards may earn more
interest than delayed rewards
<em>Animal/human behaviour shows preference for immediate
reward
</em>It is sometimes possible to use undiscounted Markov reward
processes (i.e. γ = 1), e.g. if all sequences terminate.</p>
<h2>Value Function</h2>
<p>The value function v (s) gives the long-term value of state s
Definition : The state value function v (s) of an MRP is the expected return
starting from state s v (s) = E [G t | S t = s]
![](/images/Screenshot from 2018-06-17 08-37-21.png)</p>
<h1>Application</h1>
<h2>Control a power station</h2>
<h3>Reward</h3>
<p>+ve reward for producing power
−ve reward for exceeding safety thresholds</p>


    <div class="comments">
        <div id="disqus_thread"></div>
            <script type="text/javascript">
                var disqus_shortname = 'leafyleap-2';
                var disqus_identifier = 'Deep Reinforcement Learning.html';
                var disqus_url = 'https://mohcinemadkour.github.io/Deep Reinforcement Learning.html';
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            </script>
        <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>
        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="https://mohcinemadkour.github.io/archives.html">Archives</a></li>
                            <li><a href="https://mohcinemadkour.github.io/tags.html">Tags</a></li>
                            <li><a href="https://mohcinemadkour.github.io/authors.html">Authors</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Social</div>
                        <ul class="list-unstyled">
                            <li><a href="https://www.meetup.com/members/112481792//" target="_blank">meetup</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Links</div>
                        <ul class="list-unstyled">
                            <li><a href="https://www.linkedin.com/in/mohcine-madkour-83a642b2//" target="_blank">LinkedIn</a></li>
                            <li><a href="https://github.com/mohcinemadkour/" target="_blank">Github</a></li>
                        </ul>
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; leafyleap 2017</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>